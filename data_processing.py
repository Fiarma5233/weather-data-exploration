# # # # # # # # # # # # # # # # import pandas as pd
# # # # # # # # # # # # # # # # import numpy as np
# # # # # # # # # # # # # # # # import matplotlib.pyplot as plt
# # # # # # # # # # # # # # # # import seaborn as sns
# # # # # # # # # # # # # # # # from matplotlib.dates import DateFormatter, MonthLocator, WeekdayLocator, YearLocator, DayLocator
# # # # # # # # # # # # # # # # from pyproj import CRS, Transformer
# # # # # # # # # # # # # # # # import pytz
# # # # # # # # # # # # # # # # from astral.location import LocationInfo
# # # # # # # # # # # # # # # # from astral import sun
# # # # # # # # # # # # # # # # from sklearn.preprocessing import MinMaxScaler
# # # # # # # # # # # # # # # # import io
# # # # # # # # # # # # # # # # import base64
# # # # # # # # # # # # # # # # import warnings
# # # # # # # # # # # # # # # # import gdown # Importation de gdown
# # # # # # # # # # # # # # # # import os    # Importation de os pour les chemins de fichiers

# # # # # # # # # # # # # # # # # Supprimer les avertissements de SettingWithCopyWarning de Pandas,
# # # # # # # # # # # # # # # # # qui peuvent survenir avec df.loc et les chaînes d'opérations.
# # # # # # # # # # # # # # # # pd.options.mode.chained_assignment = None # default='warn'

# # # # # # # # # # # # # # # # # --- Fonctions de Traitement de Données ---

# # # # # # # # # # # # # # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # # # # # # # # # # # # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # # # # # # # # # # # # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # # # # # # # # # # # # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     df_copy = df.copy() # Travailler sur une copie

# # # # # # # # # # # # # # # #     # Cas spécifique pour VEA_SISSILI ou si seule la colonne 'Date' est présente
# # # # # # # # # # # # # # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not all(col in df_copy.columns for col in ['Year', 'Month', 'Day'])):
# # # # # # # # # # # # # # # #         try:
# # # # # # # # # # # # # # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # # # # # # # # # # # # # #         except Exception as e:
# # # # # # # # # # # # # # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime: {e}")
# # # # # # # # # # # # # # # #             df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # # #         # Logique pour les colonnes Year, Month, Day, Hour, Minute
# # # # # # # # # # # # # # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # # # # # # # # # # # # # #         # Convertir les colonnes en numérique, forçant les erreurs en NaN
# # # # # # # # # # # # # # # #         for col in date_cols:
# # # # # # # # # # # # # # # #             if col in df_copy.columns:
# # # # # # # # # # # # # # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # # # # # # # # # # # # # #         # Supprimer les lignes où des valeurs temporelles essentielles sont manquantes
# # # # # # # # # # # # # # # #         # (seulement les colonnes qui existent)
# # # # # # # # # # # # # # # #         existing_date_cols = [col for col in date_cols if col in df_copy.columns]
# # # # # # # # # # # # # # # #         if existing_date_cols:
# # # # # # # # # # # # # # # #             df_copy = df_copy.dropna(subset=existing_date_cols)

# # # # # # # # # # # # # # # #         # Créer la colonne Datetime si toutes les colonnes nécessaires sont présentes
# # # # # # # # # # # # # # # #         if all(col in df_copy.columns for col in date_cols):
# # # # # # # # # # # # # # # #             try:
# # # # # # # # # # # # # # # #                 df_copy['Datetime'] = pd.to_datetime(df_copy[date_cols])
# # # # # # # # # # # # # # # #             except Exception as e:
# # # # # # # # # # # # # # # #                 warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées: {e}")
# # # # # # # # # # # # # # # #                 df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # # #             warnings.warn("Colonnes temporelles (Year, Month, Day, Hour, Minute) ou 'Date' manquantes. La colonne 'Datetime' n'a pas pu être créée.")
# # # # # # # # # # # # # # # #             df_copy['Datetime'] = pd.NaT

# # # # # # # # # # # # # # # #     # Extraire/recréer les composantes de date/heure si Datetime a été créé
# # # # # # # # # # # # # # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # # # # # # # # # # # # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # # # # # # # # # # # # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # # # # # # # # # # # # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # # # # # # # # # # # # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # # # # # # # # # # # # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # # # # # # # # # # # # # #         df_copy['Date'] = df_copy['Datetime'].dt.date
# # # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création.")

# # # # # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # # # # def check_compatibility_for_merge(df_list: list[pd.DataFrame]) -> tuple[bool, str]:
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     Vérifie si une liste de DataFrames est compatible pour la fusion (concaténation).
# # # # # # # # # # # # # # # #     Les DataFrames sont considérés compatibles si :
# # # # # # # # # # # # # # # #     - Ils ont le même nombre de colonnes.
# # # # # # # # # # # # # # # #     - Ils ont les mêmes noms de colonnes.

# # # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # # #         df_list (list): Une liste de pandas DataFrames.

# # # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # # #         tuple: (bool, str) - True si compatible, False sinon, et un message d'erreur.
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     if not df_list:
# # # # # # # # # # # # # # # #         return False, "La liste de DataFrames est vide."
# # # # # # # # # # # # # # # #     if len(df_list) == 1:
# # # # # # # # # # # # # # # #         return True, "Un seul DataFrame, pas de fusion nécessaire."

# # # # # # # # # # # # # # # #     first_df = df_list[0]
# # # # # # # # # # # # # # # #     num_cols_first = first_df.shape[1]
# # # # # # # # # # # # # # # #     cols_first = set(first_df.columns)

# # # # # # # # # # # # # # # #     for i, df in enumerate(df_list[1:]):
# # # # # # # # # # # # # # # #         if df.shape[1] != num_cols_first:
# # # # # # # # # # # # # # # #             return False, f"Le DataFrame {i+2} a {df.shape[1]} colonnes, le premier en a {num_cols_first}. Fusion impossible."
# # # # # # # # # # # # # # # #         if set(df.columns) != cols_first:
# # # # # # # # # # # # # # # #             missing_in_current = list(cols_first - set(df.columns))
# # # # # # # # # # # # # # # #             extra_in_current = list(set(df.columns) - cols_first)
            
# # # # # # # # # # # # # # # #             error_msg = f"Les colonnes du DataFrame {i+2} ne correspondent pas à celles du premier."
# # # # # # # # # # # # # # # #             if missing_in_current:
# # # # # # # # # # # # # # # #                 error_msg += f" Manquantes dans le DataFrame {i+2}: {missing_in_current}."
# # # # # # # # # # # # # # # #             if extra_in_current:
# # # # # # # # # # # # # # # #                 error_msg += f" Supplémentaires dans le DataFrame {i+2}: {extra_in_current}."
# # # # # # # # # # # # # # # #             return False, error_msg
# # # # # # # # # # # # # # # #     return True, "Tous les DataFrames sont compatibles pour la fusion."

# # # # # # # # # # # # # # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     Crée une colonne 'Rain_mm' comme moyenne des mesures de deux pluviomètres.

# # # # # # # # # # # # # # # #     Paramètres:
# # # # # # # # # # # # # # # #     -----------
# # # # # # # # # # # # # # # #     df : pandas.DataFrame
# # # # # # # # # # # # # # # #         DataFrame contenant au minimum les colonnes 'Rain_01_mm' et 'Rain_02_mm'

# # # # # # # # # # # # # # # #     Retourne:
# # # # # # # # # # # # # # # #     --------
# # # # # # # # # # # # # # # #     pandas.DataFrame
# # # # # # # # # # # # # # # #         Le même DataFrame avec une nouvelle colonne 'Rain_mm' ajoutée
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     df_copy = df.copy()
# # # # # # # # # # # # # # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # # # # # # # # # # # # # #         df_copy['Rain_mm'] = df_copy[['Rain_01_mm', 'Rain_02_mm']].mean(axis=1)
# # # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # # #         df_copy['Rain_mm'] = np.nan
# # # # # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     Supprime les doublons complets et nettoie les lignes avec des valeurs temporelles manquantes.
# # # # # # # # # # # # # # # #     Gère les conflits où 'Datetime' est à la fois une colonne et un index.

# # # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée.

# # # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # # #         pd.DataFrame: DataFrame traité.
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # # # # # #     # Suppression des doublons complets
# # # # # # # # # # # # # # # #     dup_count = df_copy.duplicated().sum()
# # # # # # # # # # # # # # # #     if dup_count > 0:
# # # # # # # # # # # # # # # #         warnings.warn(f"Suppression de {dup_count} doublons complets.")
# # # # # # # # # # # # # # # #         df_copy = df_copy.drop_duplicates()

# # # # # # # # # # # # # # # #     # Nettoyage des colonnes temporelles si elles existent
# # # # # # # # # # # # # # # #     time_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
# # # # # # # # # # # # # # # #     existing_time_cols = [col for col in time_cols if col in df_copy.columns]
# # # # # # # # # # # # # # # #     if existing_time_cols:
# # # # # # # # # # # # # # # #         initial_rows = len(df_copy)
# # # # # # # # # # # # # # # #         df_copy = df_copy.dropna(subset=existing_time_cols)
# # # # # # # # # # # # # # # #         removed_time_rows = initial_rows - len(df_copy)
# # # # # # # # # # # # # # # #         if removed_time_rows > 0:
# # # # # # # # # # # # # # # #             warnings.warn(f"Suppression de {removed_time_rows} lignes avec valeurs temporelles manquantes.")

# # # # # # # # # # # # # # # #     # Gestion des conflits Datetime colonne/index
# # # # # # # # # # # # # # # #     if 'Datetime' in df_copy.columns and isinstance(df_copy.index, pd.DatetimeIndex) and df_copy.index.name == 'Datetime':
# # # # # # # # # # # # # # # #         pass
# # # # # # # # # # # # # # # #     elif 'Datetime' in df_copy.columns and 'Datetime' in df_copy.index.names:
# # # # # # # # # # # # # # # #         df_copy = df_copy.reset_index(drop=True)
    
# # # # # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # # # # # # # # # # # # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # # # # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # # # # # # # # # # # # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # # # # # # # # # # # # # #         raise ValueError(
# # # # # # # # # # # # # # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # # # # # # # # # # # # # #         )

# # # # # # # # # # # # # # # #     def convert_row(row):
# # # # # # # # # # # # # # # #         try:
# # # # # # # # # # # # # # # #             zone = int(row['zone'])
# # # # # # # # # # # # # # # #             hemisphere = str(row['hemisphere']).upper()
# # # # # # # # # # # # # # # #             is_northern = hemisphere == 'N'

# # # # # # # # # # # # # # # #             proj_utm = CRS.from_proj4(
# # # # # # # # # # # # # # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # # # # # # # # # # # # # #             )
# # # # # # # # # # # # # # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # # # # # # # # # # # # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # # # # # # # # # # # # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # # # # # # # # # # # # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # # # # # # # # # # # # # #         except Exception as e:
# # # # # # # # # # # # # # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # # # # # # # # # # # # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # # # # # # # # # # # # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # # # # # # # # # # # # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # # # # # # # # # # # # # #     return df_copy


# # # # # # # # # # # # # # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # # # # # # # # # # # # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # # # # # # # # # # # # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # # # # # # # # # # # # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # # # # # # # # # # # # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.

# # # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # # # # # # # # # # # # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # # # # # # # # # # # # # #     data_dir = 'data'
# # # # # # # # # # # # # # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # # # # # # # # # # # # # #     # 1. Téléchargement et chargement du bassin VEA SISSILI
# # # # # # # # # # # # # # # #     file_id_location = '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz'
# # # # # # # # # # # # # # # #     output_file_location = os.path.join(data_dir, "WASCAL Basins Climate Station Coordinates.xlsx")
# # # # # # # # # # # # # # # #     print(f"Téléchargement de Vea Sissili depuis Drive...")
# # # # # # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_location}', output_file_location, quiet=False)
# # # # # # # # # # # # # # # #     vea_sissili_bassin = pd.read_excel(output_file_location)

# # # # # # # # # # # # # # # #     # 2. Téléchargement et chargement du bassin DANO
# # # # # # # # # # # # # # # #     file_id_dano_basins = '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z'
# # # # # # # # # # # # # # # #     output_file_dano_basins = os.path.join(data_dir, "Dano Basins Climate Station Coordinates.xlsx")
# # # # # # # # # # # # # # # #     print(f"Téléchargement de Dano depuis Drive...")
# # # # # # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_dano_basins}', output_file_dano_basins, quiet=False)
# # # # # # # # # # # # # # # #     dano_bassin = pd.read_excel(output_file_dano_basins)

# # # # # # # # # # # # # # # #     # 3. Téléchargement et chargement du bassin DASSARI
# # # # # # # # # # # # # # # #     file_id_dassari = '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU'
# # # # # # # # # # # # # # # #     output_file_dassari = os.path.join(data_dir, "DASSARI Climate Station Coordinates.xlsx")
# # # # # # # # # # # # # # # #     print(f"Téléchargement de Dassari depuis Drive...")
# # # # # # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_dassari}', output_file_dassari, quiet=False)
# # # # # # # # # # # # # # # #     dassari_bassin = pd.read_excel(output_file_dassari)

# # # # # # # # # # # # # # # #     # 4. Prétraitement des DataFrames (votre code original)
# # # # # # # # # # # # # # # #     print("Début du prétraitement des données de stations...")
    
# # # # # # # # # # # # # # # #     # Vea Sissili
# # # # # # # # # # # # # # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # # # # # # # # # # # # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # # # # # # # # # # # # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # # # # # # # # # # # # # #     # Dassari
# # # # # # # # # # # # # # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # # # # # # # # # # # # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # # # # # # # # # # # # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # # # # # # # # # # # # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # # # # # # # # # # # # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # # # # # # # # # # # # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # # # # # # # # # # # # # #     dano_bassin['zone'] = 30
# # # # # # # # # # # # # # # #     dano_bassin['hemisphere'] = 'N'
# # # # # # # # # # # # # # # #     dassari_bassin['zone'] = 31
# # # # # # # # # # # # # # # #     dassari_bassin['hemisphere'] = 'N'

# # # # # # # # # # # # # # # #     # Application de la fonction de conversion UTM vers GPS
# # # # # # # # # # # # # # # #     # C'est la même fonction 'convert_utm_df_to_gps' qui est définie plus haut dans ce même fichier.
# # # # # # # # # # # # # # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # # # # # # # # # # # # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # # # # # # # # # # # # # #     # Ajout des fuseaux horaires
# # # # # # # # # # # # # # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # # # # # # # # # # # # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # # # # # # # # # # # # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # # # # # # # # # # # # # #     # Fusion de tous les bassins
# # # # # # # # # # # # # # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # # # # # # # # # # # # # #     # Renommer 'Name' en 'Station'
# # # # # # # # # # # # # # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # # # # # # # # # # # # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # # # # # # # # # # # # # #     initial_rows_count = len(bassins)
# # # # # # # # # # # # # # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # # # # # # # # # # # # # #     if len(bassins) < initial_rows_count:
# # # # # # # # # # # # # # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # # # # # # # # # # # # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # # # # # # # # # # # # # #     if 'Station' in bassins.columns:
# # # # # # # # # # # # # # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # # # # # # # # # # # # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # # # # # # # # # # # # # #     return bassins


# # # # # # # # # # # # # # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     Effectue toutes les interpolations météorologiques en une seule passe,
# # # # # # # # # # # # # # # #     en intégrant le calcul automatique du lever et du coucher du soleil
# # # # # # # # # # # # # # # #     pour une gestion plus précise de la radiation solaire.

# # # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # # #         df (pd.DataFrame): Le DataFrame contenant les données météorologiques.
# # # # # # # # # # # # # # # #                            Il doit contenir une colonne 'Station'.
# # # # # # # # # # # # # # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # # # # # # # # # # # # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # # # # # # # # # # # # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # # #         pd.DataFrame: Le DataFrame original avec les données interpolées et
# # # # # # # # # # # # # # # #                       la colonne 'Is_Daylight' calculée, ainsi que la durée du jour.
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     df_processed = df.copy()

# # # # # # # # # # # # # # # #     # --- Pré-vérification et préparation des données GPS ---
# # # # # # # # # # # # # # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # # # # # # # # # # # # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # # # # # # # # # # # # # #         raise ValueError(
# # # # # # # # # # # # # # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # # # # # # # # # # # # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # # # # # # # # # # # # # #         )

# # # # # # # # # # # # # # # #     if not df_gps['Station'].is_unique:
# # # # # # # # # # # # # # # #         warnings.warn("La colonne 'Station' dans df_gps contient des noms de station dupliqués. Suppression des doublons.")
# # # # # # # # # # # # # # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # # #         df_gps_unique = df_gps.copy()

# # # # # # # # # # # # # # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # # # # # # # # # # # # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # # # # # # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # # # # # # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # # # # # # # # # # # # # #     for col in numerical_cols:
# # # # # # # # # # # # # # # #         if col in df_processed.columns:
# # # # # # # # # # # # # # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # # # # # # # # # # # # # #     # Assurez-vous que 'Datetime' est bien une colonne et qu'elle n'est pas vide
# # # # # # # # # # # # # # # #     if 'Datetime' not in df_processed.columns or df_processed['Datetime'].isnull().all():
# # # # # # # # # # # # # # # #         raise ValueError("La colonne 'Datetime' est manquante ou toutes ses valeurs sont NaN. Impossible de procéder à l'interpolation.")

# # # # # # # # # # # # # # # #     initial_rows = len(df_processed)
# # # # # # # # # # # # # # # #     df_processed = df_processed.dropna(subset=['Datetime'])
# # # # # # # # # # # # # # # #     if initial_rows - len(df_processed) > 0:
# # # # # # # # # # # # # # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec valeurs Datetime manquantes ou invalides.")

# # # # # # # # # # # # # # # #     df_processed_parts = []

# # # # # # # # # # # # # # # #     for station_name, group in df_processed.groupby('Station'):
# # # # # # # # # # # # # # # #         group_copy = group.copy()

# # # # # # # # # # # # # # # #         # Extraire les infos GPS pour la station actuelle
# # # # # # # # # # # # # # # #         gps_data = gps_info_dict.get(station_name)

# # # # # # # # # # # # # # # #         # Assurez-vous que l'index est DatetimeIndex et trié
# # # # # # # # # # # # # # # #         if not isinstance(group_copy.index, pd.DatetimeIndex) or group_copy.index.name != 'Datetime':
# # # # # # # # # # # # # # # #             if 'Datetime' in group_copy.columns:
# # # # # # # # # # # # # # # #                 group_copy = group_copy.set_index('Datetime', drop=True)
# # # # # # # # # # # # # # # #             else:
# # # # # # # # # # # # # # # #                 warnings.warn(f"La colonne 'Datetime' est manquante pour la station {station_name}. Impossible de définir l'index temporel.")
# # # # # # # # # # # # # # # #                 group_copy['Is_Daylight'] = False # Default to no daylight if no time data
# # # # # # # # # # # # # # # #                 group_copy['Daylight_Duration'] = "00:00:00"
# # # # # # # # # # # # # # # #                 df_processed_parts.append(group_copy)
# # # # # # # # # # # # # # # #                 continue # Passer à la station suivante

# # # # # # # # # # # # # # # #         group_copy = group_copy.sort_index()
# # # # # # # # # # # # # # # #         group_copy.index.name = 'Datetime' # Assurer que le nom de l'index est 'Datetime'


# # # # # # # # # # # # # # # #         # Calcul de lever/coucher du soleil ou fallback
# # # # # # # # # # # # # # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # # # # # # # # # # # # # #             lat = gps_data['Lat']
# # # # # # # # # # # # # # # #             long = gps_data['Long']
# # # # # # # # # # # # # # # #             timezone_str = gps_data['Timezone']

# # # # # # # # # # # # # # # #             try:
# # # # # # # # # # # # # # # #                 tz = pytz.timezone(timezone_str)
# # # # # # # # # # # # # # # #                 if group_copy.index.tz is None:
# # # # # # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_localize(tz, ambiguous='NaT', nonexistent='NaT')
# # # # # # # # # # # # # # # #                 else:
# # # # # # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_convert(tz)

# # # # # # # # # # # # # # # #                 daily_sun_info = {}
# # # # # # # # # # # # # # # #                 for date_only in group_copy.index.normalize().unique():
# # # # # # # # # # # # # # # #                     s = sun.sun(LocationInfo(station_name, "Site", timezone_str, lat, long).observer,
# # # # # # # # # # # # # # # #                                 date=date_only, tzinfo=tz) # Use tz directly
# # # # # # # # # # # # # # # #                     daily_sun_info[date_only.date()] = {
# # # # # # # # # # # # # # # #                         'sunrise': s['sunrise'],
# # # # # # # # # # # # # # # #                         'sunset': s['sunset']
# # # # # # # # # # # # # # # #                     }

# # # # # # # # # # # # # # # #                 group_copy.loc[:, 'sunrise_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunrise'))
# # # # # # # # # # # # # # # #                 group_copy.loc[:, 'sunset_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunset'))

# # # # # # # # # # # # # # # #                 # Assurer que les colonnes sunrise/sunset sont dans le même fuseau horaire que l'index pour la comparaison
# # # # # # # # # # # # # # # #                 if group_copy.index.tz is not None:
# # # # # # # # # # # # # # # #                      group_copy['sunrise_time_local'] = group_copy['sunrise_time_local'].dt.tz_convert(group_copy.index.tz)
# # # # # # # # # # # # # # # #                      group_copy['sunset_time_local'] = group_copy['sunset_time_local'].dt.tz_convert(group_copy.index.tz)

# # # # # # # # # # # # # # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index >= group_copy['sunrise_time_local']) & \
# # # # # # # # # # # # # # # #                                                   (group_copy.index < group_copy['sunset_time_local'])

# # # # # # # # # # # # # # # #                 daylight_timedelta = group_copy['sunset_time_local'] - group_copy['sunrise_time_local']
# # # # # # # # # # # # # # # #                 group_copy.loc[:, 'Daylight_Duration'] = daylight_timedelta.apply(
# # # # # # # # # # # # # # # #                     lambda td: (pd.to_datetime('00:00:00') + td).strftime('%H:%M:%S') if pd.notna(td) else np.nan
# # # # # # # # # # # # # # # #                 )
# # # # # # # # # # # # # # # #             except Exception as e:
# # # # # # # # # # # # # # # #                 warnings.warn(f"Erreur Astral pour {station_name}: {e}. Utilisation du fallback 7h-18h.")
# # # # # # # # # # # # # # # #                 if group_copy.index.tz is None:
# # # # # # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_localize('UTC') # Fallback pour localiser l'index
# # # # # # # # # # # # # # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # # # # # # # # # # # # # #                 group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # # #             warnings.warn(f"Coordonnées/Fuseau horaire manquants/invalides pour {station_name}. Utilisation du fallback 7h-18h.")
# # # # # # # # # # # # # # # #             if group_copy.index.tz is None:
# # # # # # # # # # # # # # # #                 group_copy.index = group_copy.index.tz_localize('UTC')
# # # # # # # # # # # # # # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # # # # # # # # # # # # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"

# # # # # # # # # # # # # # # #         df_processed_parts.append(group_copy)

# # # # # # # # # # # # # # # #     if not df_processed_parts:
# # # # # # # # # # # # # # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # # # # # # # # # # # # # #     df_final = pd.concat(df_processed_parts)
# # # # # # # # # # # # # # # #     df_final = df_final.sort_index()
# # # # # # # # # # # # # # # #     df_final.index.name = 'Datetime'

# # # # # # # # # # # # # # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date',
# # # # # # # # # # # # # # # #                                   'sunrise_time_local', 'sunset_time_local']
# # # # # # # # # # # # # # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # # # # # # # # # # # # # #     # Gestion intelligente de Rain_mm
# # # # # # # # # # # # # # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # # # # # # # # # # # # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # # # # # # # # # # # # # #             df_final = create_rain_mm(df_final)
# # # # # # # # # # # # # # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # # # # # # # # # # # # # #             df_final['Rain_mm'] = np.nan


# # # # # # # # # # # # # # # #     # Interpolation standard et bornage pour les variables numériques
# # # # # # # # # # # # # # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # # # # # # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # # # # # # # #                      'Wind_Dir_Deg']

# # # # # # # # # # # # # # # #     for var in standard_vars:
# # # # # # # # # # # # # # # #         if var in df_final.columns:
# # # # # # # # # # # # # # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # # # # # # # # # # # # # #             if var in limits:
# # # # # # # # # # # # # # # #                 min_val = limits[var]['min']
# # # # # # # # # # # # # # # #                 max_val = limits[var]['max']
# # # # # # # # # # # # # # # #                 initial_nan_count = df_final[var].isna().sum()
# # # # # # # # # # # # # # # #                 df_final.loc[(df_final[var] < min_val) | (df_final[var] > max_val), var] = np.nan
# # # # # # # # # # # # # # # #                 if df_final[var].isna().sum() > initial_nan_count:
# # # # # # # # # # # # # # # #                     warnings.warn(f"Remplacement de {df_final[var].isna().sum() - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # # # # # # # # # # # # # #             # Interpolation
# # # # # # # # # # # # # # # #             df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # # # # # # # # # # # # # #             df_final[var] = df_final[var].bfill().ffill() # Pour les NaNs aux extrémités

# # # # # # # # # # # # # # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # # # # # # # # # # # # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # # # # # # # # # # # # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # # # # # # # # # # # # # #         if 'Solar_R_W/m^2' in limits:
# # # # # # # # # # # # # # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # # # # # # # # # # # # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # # # # # # # # # # # # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # # # # # # # # # # # # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # # # # # # # # # # # # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # # # # # # # # # # # # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # # # # # # # # # # # # # #         if 'Is_Daylight' in df_final.columns:
# # # # # # # # # # # # # # # #             # Radiation positive hors du jour -> 0
# # # # # # # # # # # # # # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # # # # # # # # # # # # # #             # Zéros suspects en journée sans pluie -> NaN
# # # # # # # # # # # # # # # #             if 'Rain_mm' in df_final.columns:
# # # # # # # # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # # # # # # # # # # # # # #             else:
# # # # # # # # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # # # # # # # # # # # # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # # # # # # # # # # # # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # # # # # # # # # # # # # #             # Interpolation dans les périodes de jour
# # # # # # # # # # # # # # # #             is_day = df_final['Is_Daylight']
# # # # # # # # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # # # # # # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # # # # # # # # # # # # # #             # Les NaNs restants hors des heures de jour -> 0
# # # # # # # # # # # # # # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # # # # # # # # # # # # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # # # # # # # # # # # # # #             df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()

# # # # # # # # # # # # # # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # # # # # # # # # # # # # #     missing_after_interp = df_final.isna().sum()
# # # # # # # # # # # # # # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # # # # # # # # # # # # # #     if not columns_with_missing.empty:
# # # # # # # # # # # # # # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # # # # # # # # # # # # # #     return df_final

# # # # # # # # # # # # # # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     Traite les valeurs aberrantes (outliers) dans un DataFrame de données météorologiques
# # # # # # # # # # # # # # # #     en appliquant la méthode de l'écart interquartile (IQR) pour limiter les valeurs extrêmes.

# # # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame contenant les données météorologiques à traiter.
# # # # # # # # # # # # # # # #         limits (dict): Dictionnaire des variables à traiter (clés) avec leurs limites potentielles.
# # # # # # # # # # # # # # # #                        (Note: les limites min/max du dict ne sont pas utilisées pour l'IQR,
# # # # # # # # # # # # # # # #                        mais les clés du dict définissent les variables à analyser).

# # # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # # #         pd.DataFrame: Copie du DataFrame avec les outliers corrigés par la méthode IQR.
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # # # # # #     for var in limits.keys(): # On utilise les clés de limits pour savoir quelles variables traiter
# # # # # # # # # # # # # # # #         if var in df_copy.columns:
# # # # # # # # # # # # # # # #             # S'assurer que la colonne est numérique
# # # # # # # # # # # # # # # #             df_copy[var] = pd.to_numeric(df_copy[var], errors='coerce')

# # # # # # # # # # # # # # # #             # Calcul des quartiles
# # # # # # # # # # # # # # # #             Q1 = df_copy[var].quantile(0.25)
# # # # # # # # # # # # # # # #             Q3 = df_copy[var].quantile(0.75)
# # # # # # # # # # # # # # # #             IQR = Q3 - Q1

# # # # # # # # # # # # # # # #             # Bornes IQR
# # # # # # # # # # # # # # # #             borne_inf = Q1 - 1.5 * IQR
# # # # # # # # # # # # # # # #             borne_sup = Q3 + 1.5 * IQR

# # # # # # # # # # # # # # # #             # Application du bornage (clipping)
# # # # # # # # # # # # # # # #             initial_outliers_count = df_copy[(df_copy[var] < borne_inf) | (df_copy[var] > borne_sup)][var].count()
# # # # # # # # # # # # # # # #             df_copy[var] = df_copy[var].clip(lower=borne_inf, upper=borne_sup)
# # # # # # # # # # # # # # # #             if initial_outliers_count > 0:
# # # # # # # # # # # # # # # #                 warnings.warn(f"Outliers traités par IQR pour '{var}'. {initial_outliers_count} valeurs ajustées.")
# # # # # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables météorologiques.

# # # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec 'Datetime' comme index.

# # # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # # #         pd.DataFrame: DataFrame contenant les statistiques journalières.
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # # # # #         raise TypeError("L'index du DataFrame doit être de type DatetimeIndex.")

# # # # # # # # # # # # # # # #     # Définir les variables pour lesquelles nous voulons des statistiques
# # # # # # # # # # # # # # # #     # et leurs méthodes d'agrégation.
# # # # # # # # # # # # # # # #     # Rain_mm doit être sommée, les autres moyennées.
# # # # # # # # # # # # # # # #     # Wind_Dir_Deg est un cas particulier, souvent on calcule une direction moyenne vectorielle
# # # # # # # # # # # # # # # #     # mais pour des stats simples, une moyenne arithmétique peut être misleading.
# # # # # # # # # # # # # # # #     # Pour cet exemple, on le met en moyenne, mais gardez cela à l'esprit.
# # # # # # # # # # # # # # # #     agg_funcs = {
# # # # # # # # # # # # # # # #         'Air_Temp_Deg_C': 'mean',
# # # # # # # # # # # # # # # #         'Rel_H_%': 'mean',
# # # # # # # # # # # # # # # #         'BP_mbar_Avg': 'mean',
# # # # # # # # # # # # # # # #         'Rain_mm': 'sum', # Précipitation cumulée
# # # # # # # # # # # # # # # #         'Wind_Sp_m/sec': 'mean',
# # # # # # # # # # # # # # # #         'Solar_R_W/m^2': 'mean',
# # # # # # # # # # # # # # # #         'Wind_Dir_Deg': 'mean' # Moyenne simple, à interpréter avec prudence
# # # # # # # # # # # # # # # #     }

# # # # # # # # # # # # # # # #     # Filtrer les colonnes qui existent dans le DataFrame
# # # # # # # # # # # # # # # #     existing_cols_to_agg = {col: func for col, func in agg_funcs.items() if col in df.columns}

# # # # # # # # # # # # # # # #     if not existing_cols_to_agg:
# # # # # # # # # # # # # # # #         warnings.warn("Aucune colonne pertinente trouvée pour les statistiques journalières.")
# # # # # # # # # # # # # # # #         return pd.DataFrame()

# # # # # # # # # # # # # # # #     # Regrouper par jour (en utilisant l'index Datetime) et par station, puis agréger
# # # # # # # # # # # # # # # #     daily_summary = df.groupby([df.index.date, 'Station']).agg(
# # # # # # # # # # # # # # # #         **{f"{col}_{func}": (col, func) for col, func in existing_cols_to_agg.items()}
# # # # # # # # # # # # # # # #     )
# # # # # # # # # # # # # # # #     daily_summary.index.names = ['Date', 'Station']
# # # # # # # # # # # # # # # #     return daily_summary.reset_index()


# # # # # # # # # # # # # # # # # --- Fonctions de Visualisation ---

# # # # # # # # # # # # # # # # # Fonction utilitaire pour convertir une figure Matplotlib en base64
# # # # # # # # # # # # # # # # def fig_to_base64(fig: plt.Figure) -> str | None:
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     Convertit une figure Matplotlib en une chaîne Base64 encodée (format PNG).

# # # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # # #         fig (plt.Figure): La figure Matplotlib à convertir.

# # # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG, ou None si la figure est invalide.
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     if fig is None:
# # # # # # # # # # # # # # # #         return None
# # # # # # # # # # # # # # # #     buf = io.BytesIO()
# # # # # # # # # # # # # # # #     try:
# # # # # # # # # # # # # # # #         fig.savefig(buf, format='png', bbox_inches='tight')
# # # # # # # # # # # # # # # #         plt.close(fig) # Fermer la figure pour libérer la mémoire
# # # # # # # # # # # # # # # #         data = base64.b64encode(buf.getbuffer()).decode("ascii")
# # # # # # # # # # # # # # # #         return data
# # # # # # # # # # # # # # # #     except Exception as e:
# # # # # # # # # # # # # # # #         warnings.warn(f"Erreur lors de la conversion de la figure en base64: {e}")
# # # # # # # # # # # # # # # #         plt.close(fig) # Assurez-vous de fermer la figure même en cas d'erreur
# # # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station_name: str, variable_name: str, periode: str, custom_palette: list = None) -> str | None:
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     Génère un seul graphique de l'évolution d'une variable météorologique pour une station
# # # # # # # # # # # # # # # #     spécifique et une période donnée.

# # # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée contenant les données météorologiques.
# # # # # # # # # # # # # # # #                            Doit inclure 'Datetime' comme index et 'Station'.
# # # # # # # # # # # # # # # #         station_name (str): Nom de la station à visualiser.
# # # # # # # # # # # # # # # #         variable_name (str): Nom de la variable à visualiser.
# # # # # # # # # # # # # # # #         periode (str): Période d'agrégation ('Journalière', 'Hebdomadaire', 'Mensuelle', 'Annuelle').
# # # # # # # # # # # # # # # #         custom_palette (list, optional): Palette de couleurs personnalisée.

# # # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     METADONNEES_VARIABLES = {
# # # # # # # # # # # # # # # #         'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'agg_type': 'cumul'},
# # # # # # # # # # # # # # # #         'Air_Temp_Deg_C': {'Nom': "Température ", 'Unite': "°C", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # # # #         'Rel_H_%': {'Nom': "Humidité Relative", 'Unite': "%", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # # # #         'Solar_R_W/m^2': {'Nom': "Radiation Solaire", 'Unite': "W/m²", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # # # #         'Wind_Sp_m/sec': {'Nom': "Vitesse du Vent", 'Unite': "m/s", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # # # #         'Wind_Dir_Deg': {'Nom': "Direction du Vent", 'Unite': "°", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # # # #         'BP_mbar_Avg': {'Nom': "Pression Atmospherique moyenne", 'Unite': "mbar", 'agg_type': 'moyenne'}
# # # # # # # # # # # # # # # #     }

# # # # # # # # # # # # # # # #     if variable_name not in METADONNEES_VARIABLES or variable_name not in df.columns:
# # # # # # # # # # # # # # # #         warnings.warn(f"Variable '{variable_name}' non valide ou absente du DataFrame.")
# # # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # #     df_station = df[df['Station'] == station_name].copy()
# # # # # # # # # # # # # # # #     if df_station.empty:
# # # # # # # # # # # # # # # #         warnings.warn(f"Aucune donnée pour la station '{station_name}'.")
# # # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # #     df_station[variable_name] = pd.to_numeric(df_station[variable_name], errors='coerce')

# # # # # # # # # # # # # # # #     annees_uniques = df_station.index.year.nunique()
# # # # # # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # # # # # #         }
# # # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # # # # # #         }

# # # # # # # # # # # # # # # #     if periode not in freq_configs:
# # # # # # # # # # # # # # # #         warnings.warn(f"Période '{periode}' non valide.")
# # # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # #     freq, date_fmt, rot, locator, label_x = freq_configs[periode]
# # # # # # # # # # # # # # # #     meta = METADONNEES_VARIABLES[variable_name]
# # # # # # # # # # # # # # # #     nom_complet = f"{meta['Nom']} ({meta['Unite']})"
# # # # # # # # # # # # # # # #     agg_func = 'sum' if meta['agg_type'] == 'cumul' else 'mean'

# # # # # # # # # # # # # # # #     # Déterminer la couleur de la station
# # # # # # # # # # # # # # # #     stations_list = sorted(df['Station'].unique())
# # # # # # # # # # # # # # # #     try:
# # # # # # # # # # # # # # # #         station_idx = stations_list.index(station_name)
# # # # # # # # # # # # # # # #     except ValueError:
# # # # # # # # # # # # # # # #         warnings.warn(f"Station '{station_name}' non trouvée dans la liste des stations uniques.")
# # # # # # # # # # # # # # # #         station_idx = 0 # Fallback
    
# # # # # # # # # # # # # # # #     palette = custom_palette if custom_palette and len(custom_palette) > station_idx else sns.color_palette("husl", len(stations_list))
# # # # # # # # # # # # # # # #     station_color = palette[station_idx] if len(palette) > station_idx else 'blue' # Fallback couleur

# # # # # # # # # # # # # # # #     fig, ax = plt.subplots(figsize=(12, 7))

# # # # # # # # # # # # # # # #     df_temp = df_station.copy()

# # # # # # # # # # # # # # # #     if variable_name == 'Solar_R_W/m^2':
# # # # # # # # # # # # # # # #         if 'Is_Daylight' in df_temp.columns:
# # # # # # # # # # # # # # # #             df_temp = df_temp[df_temp['Is_Daylight']].copy()
# # # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # # #             warnings.warn("Avertissement: Colonne 'Is_Daylight' manquante pour la radiation solaire. Filtrage horaire non appliqué.")
# # # # # # # # # # # # # # # #             df_temp = df_temp[(df_temp.index.hour >= 7) & (df_temp.index.hour <= 18)]
            
# # # # # # # # # # # # # # # #     elif variable_name == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in df_temp.columns:
# # # # # # # # # # # # # # # #         df_temp = df_temp[(df_temp['Wind_Sp_m/sec'] > 0) & (df_temp['Wind_Dir_Deg'].notna())]

# # # # # # # # # # # # # # # #     with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # # # # # #         df_agg = df_temp.groupby(pd.Grouper(freq=freq))[variable_name].agg(agg_func).reset_index()
# # # # # # # # # # # # # # # #     df_agg.rename(columns={'index': 'Datetime'}, inplace=True)

# # # # # # # # # # # # # # # #     if df_agg.empty:
# # # # # # # # # # # # # # # #         plt.close(fig)
# # # # # # # # # # # # # # # #         warnings.warn(f"Aucune donnée agrégée pour {variable_name} à la période {periode} pour {station_name}.")
# # # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # #     sns.lineplot(
# # # # # # # # # # # # # # # #         data=df_agg,
# # # # # # # # # # # # # # # #         x='Datetime',
# # # # # # # # # # # # # # # #         y=variable_name,
# # # # # # # # # # # # # # # #         color=station_color,
# # # # # # # # # # # # # # # #         linewidth=2,
# # # # # # # # # # # # # # # #         marker='o' if periode == 'Annuelle' else None,
# # # # # # # # # # # # # # # #         markersize=6 if periode == 'Annuelle' else 0,
# # # # # # # # # # # # # # # #         ax=ax
# # # # # # # # # # # # # # # #     )

# # # # # # # # # # # # # # # #     ax.set_title(f"Évolution {periode.lower()} de {nom_complet} pour {station_name}", pad=10, fontsize=14)
# # # # # # # # # # # # # # # #     ax.set_ylabel(nom_complet, fontsize=12)
# # # # # # # # # # # # # # # #     ax.set_xlabel(label_x, fontsize=10)

# # # # # # # # # # # # # # # #     ax.xaxis.set_major_formatter(date_fmt)
# # # # # # # # # # # # # # # #     if locator:
# # # # # # # # # # # # # # # #         ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # # # # # #     for label in ax.get_xticklabels():
# # # # # # # # # # # # # # # #         label.set_rotation(rot)
# # # # # # # # # # # # # # # #         label.set_ha('right' if rot > 45 else 'center')

# # # # # # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # # # # # # def generer_graphique_comparatif(df: pd.DataFrame, variable_name: str, periode: str, custom_palette: list = None) -> str | None:
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     Génère un seul graphique comparatif de l'évolution d'une variable météorologique
# # # # # # # # # # # # # # # #     entre toutes les stations pour une période donnée.

# # # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée contenant les données météorologiques.
# # # # # # # # # # # # # # # #                            Doit inclure 'Datetime' comme index et 'Station'.
# # # # # # # # # # # # # # # #         variable_name (str): Nom de la variable à visualiser.
# # # # # # # # # # # # # # # #         periode (str): Période d'agrégation ('Journalière', 'Hebdomadaire', 'Mensuelle', 'Annuelle').
# # # # # # # # # # # # # # # #         custom_palette (list, optional): Palette de couleurs personnalisée.

# # # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     METADONNEES_VARIABLES = {
# # # # # # # # # # # # # # # #         'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'is_rain': True},
# # # # # # # # # # # # # # # #         'Air_Temp_Deg_C': {'Nom': "Température", 'Unite': "°C", 'is_rain': False},
# # # # # # # # # # # # # # # #         'Rel_H_%': {'Nom': "Humidité", 'Unite': "%", 'is_rain': False},
# # # # # # # # # # # # # # # #         'Solar_R_W/m^2': {'Nom': "Radiation solaire", 'Unite': "W/m²", 'is_rain': False},
# # # # # # # # # # # # # # # #         'Wind_Sp_m/sec': {'Nom': "Vitesse du vent", 'Unite': "m/s", 'is_rain': False},
# # # # # # # # # # # # # # # #         'Wind_Dir_Deg': {'Nom': "Direction du vent", 'Unite': "°", 'is_rain': False},
# # # # # # # # # # # # # # # #         'BP_mbar_Avg': {'Nom': "Pression atmospherique moyenne", 'Unite': "mbar", 'is_rain': False}
# # # # # # # # # # # # # # # #     }

# # # # # # # # # # # # # # # #     if variable_name not in METADONNEES_VARIABLES or variable_name not in df.columns:
# # # # # # # # # # # # # # # #         warnings.warn(f"Variable '{variable_name}' non valide ou absente du DataFrame.")
# # # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # #     annees_uniques = df.index.year.nunique()
# # # # # # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # # # # # #         }
# # # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # # # # # #         }

# # # # # # # # # # # # # # # #     if periode not in freq_configs:
# # # # # # # # # # # # # # # #         warnings.warn(f"Période '{periode}' non valide.")
# # # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # #     freq, date_fmt, rot, locator, label_x = freq_configs[periode]
# # # # # # # # # # # # # # # #     meta = METADONNEES_VARIABLES[variable_name]
# # # # # # # # # # # # # # # #     nom_complet = f"{meta['Nom']} ({meta['Unite']})"
# # # # # # # # # # # # # # # #     agg_text = " (cumul)" if meta['is_rain'] else " (moyenne)"
# # # # # # # # # # # # # # # #     agg_func = 'sum' if meta['is_rain'] else 'mean'

# # # # # # # # # # # # # # # #     stations = sorted(df['Station'].unique())
# # # # # # # # # # # # # # # #     palette = custom_palette if custom_palette and len(custom_palette) >= len(stations) \
# # # # # # # # # # # # # # # #               else sns.color_palette("husl", len(stations))

# # # # # # # # # # # # # # # #     fig, ax = plt.subplots(figsize=(14, 8))

# # # # # # # # # # # # # # # #     temp_df = df.copy()

# # # # # # # # # # # # # # # #     if variable_name == 'Solar_R_W/m^2':
# # # # # # # # # # # # # # # #         if 'Is_Daylight' in temp_df.columns:
# # # # # # # # # # # # # # # #             temp_df = temp_df[temp_df['Is_Daylight']].copy()
# # # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # # #             warnings.warn("Avertissement: Colonne 'Is_Daylight' manquante. Filtrage horaire par défaut (7h-18h) pour radiation solaire.")
# # # # # # # # # # # # # # # #             temp_df = temp_df[(temp_df.index.hour >= 7) & (temp_df.index.hour <= 18)]
            
# # # # # # # # # # # # # # # #     elif variable_name == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in temp_df.columns:
# # # # # # # # # # # # # # # #         temp_df = temp_df[(temp_df['Wind_Sp_m/sec'] > 0) & (temp_df['Wind_Dir_Deg'].notna())]

# # # # # # # # # # # # # # # #     with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # # # # # #         df_agg = temp_df.groupby(['Station', pd.Grouper(freq=freq)])[variable_name].agg(agg_func).reset_index()

# # # # # # # # # # # # # # # #     if df_agg.empty:
# # # # # # # # # # # # # # # #         plt.close(fig)
# # # # # # # # # # # # # # # #         warnings.warn(f"Aucune donnée agrégée pour {variable_name} à la période {periode}.")
# # # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # #     sns.lineplot(
# # # # # # # # # # # # # # # #         data=df_agg,
# # # # # # # # # # # # # # # #         x='Datetime',
# # # # # # # # # # # # # # # #         y=variable_name,
# # # # # # # # # # # # # # # #         hue='Station',
# # # # # # # # # # # # # # # #         palette=palette,
# # # # # # # # # # # # # # # #         ax=ax,
# # # # # # # # # # # # # # # #         linewidth=2,
# # # # # # # # # # # # # # # #         marker='o' if periode == 'Annuelle' else None,
# # # # # # # # # # # # # # # #         markersize=6 if periode == 'Annuelle' else 0,
# # # # # # # # # # # # # # # #         legend='full'
# # # # # # # # # # # # # # # #     )

# # # # # # # # # # # # # # # #     ax.set_title(
# # # # # # # # # # # # # # # #         f"Comparaison de {meta['Nom']} {agg_text} ({meta['Unite']}) - Période: {periode}",
# # # # # # # # # # # # # # # #         fontsize=16,
# # # # # # # # # # # # # # # #         pad=15,
# # # # # # # # # # # # # # # #         weight='bold'
# # # # # # # # # # # # # # # #     )
# # # # # # # # # # # # # # # #     ax.set_xlabel(label_x, fontsize=12, labelpad=15)
# # # # # # # # # # # # # # # #     ax.set_ylabel(nom_complet, fontsize=12)

# # # # # # # # # # # # # # # #     ax.xaxis.set_major_formatter(date_fmt)
# # # # # # # # # # # # # # # #     if locator:
# # # # # # # # # # # # # # # #         ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # # # # # #     for label in ax.get_xticklabels():
# # # # # # # # # # # # # # # #         label.set_rotation(rot)
# # # # # # # # # # # # # # # #         label.set_ha('right' if rot < 90 else 'center')
# # # # # # # # # # # # # # # #         label.set_fontsize(10)

# # # # # # # # # # # # # # # #     ax.legend(
# # # # # # # # # # # # # # # #         title='Stations',
# # # # # # # # # # # # # # # #         loc='upper left',
# # # # # # # # # # # # # # # #         fontsize=10,
# # # # # # # # # # # # # # # #         title_fontsize=12,
# # # # # # # # # # # # # # # #         framealpha=0.9
# # # # # # # # # # # # # # # #     )
# # # # # # # # # # # # # # # #     ax.grid(True, alpha=0.2)

# # # # # # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # # # # # #     plt.subplots_adjust(
# # # # # # # # # # # # # # # #         top=0.93,
# # # # # # # # # # # # # # # #         hspace=0.3,
# # # # # # # # # # # # # # # #         wspace=0.15
# # # # # # # # # # # # # # # #     )
# # # # # # # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # # # # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station_name: str, variable_palette: list = None) -> str | None:
# # # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # # #     Génère un graphique comparatif normalisé de l'évolution de plusieurs variables météorologiques
# # # # # # # # # # # # # # # #     pour une seule station sélectionnée, à différentes échelles temporelles.

# # # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame contenant les données météorologiques. Doit avoir 'Datetime' comme index.
# # # # # # # # # # # # # # # #         station_name (str): Le nom de la station pour laquelle générer le graphique.
# # # # # # # # # # # # # # # #         variable_palette (list, optional): Liste personnalisée de couleurs pour chaque variable.

# # # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # # # # # #     """

# # # # # # # # # # # # # # # #     METADONNEES_VARIABLES = {
# # # # # # # # # # # # # # # #         'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'agg_type': 'cumul'},
# # # # # # # # # # # # # # # #         'Air_Temp_Deg_C': {'Nom': "Température ", 'Unite': "°C", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # # # #         'Rel_H_%': {'Nom': "Humidité Relative", 'Unite': "%", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # # # #         'Solar_R_W/m^2': {'Nom': "Radiation Solaire", 'Unite': "W/m²", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # # # #         'Wind_Sp_m/sec': {'Nom': "Vitesse du Vent", 'Unite': "m/s", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # # # #         'Wind_Dir_Deg': {'Nom': "Direction du Vent", 'Unite': "°", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # # # #         'BP_mbar_Avg': {'Nom': "Pression Atmospherique moyenne", 'Unite': "mbar", 'agg_type': 'moyenne'}
# # # # # # # # # # # # # # # #     }

# # # # # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # #     df_station = df[df['Station'] == station_name].copy()

# # # # # # # # # # # # # # # #     if df_station.empty:
# # # # # # # # # # # # # # # #         warnings.warn(f"Station {station_name} sans données - ignorée.")
# # # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # #     annees_uniques = df_station.index.year.nunique()
# # # # # # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # # # # # #         }
# # # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # # # # # #         }

# # # # # # # # # # # # # # # #     nrows, ncols = (2, 2) if len(freq_configs) >= 2 else (1, 1)

# # # # # # # # # # # # # # # #     variables_to_plot = [var for var in METADONNEES_VARIABLES.keys() if var in df_station.columns]

# # # # # # # # # # # # # # # #     if not variables_to_plot:
# # # # # # # # # # # # # # # #         warnings.warn("Aucune variable valide à tracer pour cette station.")
# # # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # #     num_vars_to_plot = len(variables_to_plot)
# # # # # # # # # # # # # # # #     if (variable_palette is not None and
# # # # # # # # # # # # # # # #         isinstance(variable_palette, list) and
# # # # # # # # # # # # # # # #         len(variable_palette) >= num_vars_to_plot):
# # # # # # # # # # # # # # # #         plot_palette = variable_palette
# # # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # # #         plot_palette = sns.color_palette("tab10", num_vars_to_plot)
# # # # # # # # # # # # # # # #         if variable_palette is not None:
# # # # # # # # # # # # # # # #             warnings.warn(f"Avertissement: Palette fournie insuffisante ou invalide - utilisation de la palette par défaut pour {station_name}.")

# # # # # # # # # # # # # # # #     fig, axs = plt.subplots(nrows, ncols, figsize=(28, 22))
# # # # # # # # # # # # # # # #     axs = axs.flatten()

# # # # # # # # # # # # # # # #     fig.suptitle(
# # # # # # # # # # # # # # # #         f'Évolution Normalisée des Variables Météorologiques - Station: {station_name}',
# # # # # # # # # # # # # # # #         fontsize=24,
# # # # # # # # # # # # # # # #         weight='bold',
# # # # # # # # # # # # # # # #         y=1.02
# # # # # # # # # # # # # # # #     )

# # # # # # # # # # # # # # # #     for i, (periode, (freq, date_format, rot, locator, label_x)) in enumerate(freq_configs.items()):
# # # # # # # # # # # # # # # #         if i >= len(axs):
# # # # # # # # # # # # # # # #             continue

# # # # # # # # # # # # # # # #         ax = axs[i]
# # # # # # # # # # # # # # # #         agg_dict = {}
# # # # # # # # # # # # # # # #         for var in variables_to_plot:
# # # # # # # # # # # # # # # #             if var in df_station.columns:
# # # # # # # # # # # # # # # #                 agg_type = METADONNEES_VARIABLES[var].get('agg_type', 'moyenne')
# # # # # # # # # # # # # # # #                 agg_dict[var] = 'sum' if agg_type == 'cumul' else 'mean'

# # # # # # # # # # # # # # # #         if not agg_dict:
# # # # # # # # # # # # # # # #             warnings.warn(f"Aucune donnée à agréger pour {station_name} - période {periode}.")
# # # # # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # # # #             continue

# # # # # # # # # # # # # # # #         with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # # # # # #             for col in agg_dict.keys():
# # # # # # # # # # # # # # # #                 if col in df_station.columns:
# # # # # # # # # # # # # # # #                     df_station[col] = pd.to_numeric(df_station[col], errors='coerce')

# # # # # # # # # # # # # # # #             cols_to_agg = list(agg_dict.keys())
# # # # # # # # # # # # # # # #             if not cols_to_agg:
# # # # # # # # # # # # # # # #                 warnings.warn(f"Aucune colonne numérique valide pour {station_name} - période {periode}.")
# # # # # # # # # # # # # # # #                 if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # # # #                 continue

# # # # # # # # # # # # # # # #             df_agg = pd.DataFrame(index=df_station.index)

# # # # # # # # # # # # # # # #             for var in cols_to_agg:
# # # # # # # # # # # # # # # #                 temp_data = df_station[var]
# # # # # # # # # # # # # # # #                 if var == 'Solar_R_W/m^2':
# # # # # # # # # # # # # # # #                     if 'Is_Daylight' in df_station.columns:
# # # # # # # # # # # # # # # #                         temp_data = df_station.loc[df_station['Is_Daylight'], var]
# # # # # # # # # # # # # # # #                     else:
# # # # # # # # # # # # # # # #                         temp_data = df_station.loc[(df_station.index.hour >= 7) & (df_station.index.hour <= 18), var]
# # # # # # # # # # # # # # # #                         warnings.warn(f"Avertissement: 'Is_Daylight' non trouvé. Radiation solaire filtrée par heures fixes (7h-18h).")
# # # # # # # # # # # # # # # #                 elif var == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in df_station.columns:
# # # # # # # # # # # # # # # #                     temp_data = df_station.loc[(df_station['Wind_Sp_m/sec'] > 0) & (df_station['Wind_Dir_Deg'].notna()), var]

# # # # # # # # # # # # # # # #                 if not temp_data.empty:
# # # # # # # # # # # # # # # #                     df_agg[var] = temp_data.resample(freq).agg(agg_dict[var])
# # # # # # # # # # # # # # # #                 else:
# # # # # # # # # # # # # # # #                     df_agg[var] = np.nan

# # # # # # # # # # # # # # # #             df_agg = df_agg.dropna(how='all').reset_index()


# # # # # # # # # # # # # # # #         scaler = MinMaxScaler()
# # # # # # # # # # # # # # # #         vars_to_scale = list(agg_dict.keys())
# # # # # # # # # # # # # # # #         numeric_cols = [col for col in vars_to_scale if col in df_agg.columns]

# # # # # # # # # # # # # # # #         if not numeric_cols or df_agg.empty:
# # # # # # # # # # # # # # # #             warnings.warn(f"Données manquantes après agrégation ou colonnes numériques absentes pour {station_name} - période {periode}.")
# # # # # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # # # #             continue

# # # # # # # # # # # # # # # #         df_agg_scaled = df_agg.copy()
# # # # # # # # # # # # # # # #         valid_cols_for_scaling = [col for col in numeric_cols if df_agg_scaled[col].notna().any()]
# # # # # # # # # # # # # # # #         if valid_cols_for_scaling:
# # # # # # # # # # # # # # # #             df_agg_scaled[valid_cols_for_scaling] = scaler.fit_transform(df_agg_scaled[valid_cols_for_scaling])
# # # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # # #             warnings.warn(f"Aucune colonne avec des valeurs valides à normaliser pour {station_name} - période {periode}.")
# # # # # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # # # #             continue


# # # # # # # # # # # # # # # #         df_melted = df_agg_scaled.melt(
# # # # # # # # # # # # # # # #             id_vars=['Datetime'],
# # # # # # # # # # # # # # # #             value_vars=numeric_cols,
# # # # # # # # # # # # # # # #             var_name='Variable',
# # # # # # # # # # # # # # # #             value_name='Valeur Normalisée'
# # # # # # # # # # # # # # # #         )

# # # # # # # # # # # # # # # #         if df_melted.empty or df_melted['Valeur Normalisée'].dropna().empty:
# # # # # # # # # # # # # # # #             warnings.warn(f"Données normalisées vides ou toutes NaN pour {station_name} - période {periode}.")
# # # # # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # # # #             continue

# # # # # # # # # # # # # # # #         variable_color_map = {var: plot_palette[j] for j, var in enumerate(variables_to_plot)}

# # # # # # # # # # # # # # # #         sns.lineplot(
# # # # # # # # # # # # # # # #             data=df_melted,
# # # # # # # # # # # # # # # #             x='Datetime',
# # # # # # # # # # # # # # # #             y='Valeur Normalisée',
# # # # # # # # # # # # # # # #             hue='Variable',
# # # # # # # # # # # # # # # #             palette=variable_color_map,
# # # # # # # # # # # # # # # #             ax=ax,
# # # # # # # # # # # # # # # #             linewidth=2,
# # # # # # # # # # # # # # # #             marker='o' if periode == 'Annuelle' or len(df_melted['Datetime'].unique()) < 50 else None,
# # # # # # # # # # # # # # # #             markersize=6,
# # # # # # # # # # # # # # # #             legend='full'
# # # # # # # # # # # # # # # #         )

# # # # # # # # # # # # # # # #         ax.set_title(f"Évolution {periode.lower()}", fontsize=18, weight='bold', pad=15)
# # # # # # # # # # # # # # # #         ax.set_xlabel(label_x, fontsize=14, labelpad=15)
# # # # # # # # # # # # # # # #         ax.set_ylabel("Valeur Normalisée (0-1)", fontsize=14)

# # # # # # # # # # # # # # # #         ax.xaxis.set_major_formatter(date_format)
# # # # # # # # # # # # # # # #         if locator is not None:
# # # # # # # # # # # # # # # #             ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # # # # # #         for label in ax.get_xticklabels():
# # # # # # # # # # # # # # # #             label.set_rotation(rot)
# # # # # # # # # # # # # # # #             label.set_ha('right' if rot > 0 and rot < 90 else 'center')
# # # # # # # # # # # # # # # #             label.set_fontsize(12)

# # # # # # # # # # # # # # # #         ax.tick_params(axis='x', which='both', bottom=True, labelbottom=True)
# # # # # # # # # # # # # # # #         ax.legend(
# # # # # # # # # # # # # # # #             title='Variables',
# # # # # # # # # # # # # # # #             loc='upper left',
# # # # # # # # # # # # # # # #             fontsize=10,
# # # # # # # # # # # # # # # #             title_fontsize=12,
# # # # # # # # # # # # # # # #             framealpha=0.9
# # # # # # # # # # # # # # # #         )
# # # # # # # # # # # # # # # #         ax.grid(True, alpha=0.2)

# # # # # # # # # # # # # # # #     for j in range(len(freq_configs), len(axs)):
# # # # # # # # # # # # # # # #         if j < len(axs) and axs[j] is not None:
# # # # # # # # # # # # # # # #             fig.delaxes(axs[j])

# # # # # # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # # # # # #     plt.subplots_adjust(top=0.94, hspace=0.4, wspace=0.15)
    
# # # # # # # # # # # # # # # #     return fig_to_base64(fig)



# # # # # # # # # # # # # # # import pandas as pd
# # # # # # # # # # # # # # # import numpy as np
# # # # # # # # # # # # # # # import matplotlib.pyplot as plt
# # # # # # # # # # # # # # # import seaborn as sns
# # # # # # # # # # # # # # # from matplotlib.dates import DateFormatter, MonthLocator, WeekdayLocator, YearLocator, DayLocator
# # # # # # # # # # # # # # # from pyproj import CRS, Transformer
# # # # # # # # # # # # # # # import pytz
# # # # # # # # # # # # # # # from astral.location import LocationInfo
# # # # # # # # # # # # # # # from astral import sun
# # # # # # # # # # # # # # # from sklearn.preprocessing import MinMaxScaler
# # # # # # # # # # # # # # # import io
# # # # # # # # # # # # # # # import base64
# # # # # # # # # # # # # # # import warnings
# # # # # # # # # # # # # # # import gdown
# # # # # # # # # # # # # # # import os

# # # # # # # # # # # # # # # # Importation des métadonnées des variables depuis le fichier de configuration
# # # # # # # # # # # # # # # from config import METADATA_VARIABLES

# # # # # # # # # # # # # # # # Supprimer les avertissements de SettingWithCopyWarning de Pandas,
# # # # # # # # # # # # # # # # qui peuvent survenir avec df.loc et les chaînes d'opérations.
# # # # # # # # # # # # # # # pd.options.mode.chained_assignment = None # default='warn'

# # # # # # # # # # # # # # # # --- Fonctions de Traitement de Données ---

# # # # # # # # # # # # # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # # # # # # # # # # # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # # # # # # # # # # # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # # # # # # # # # # # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     df_copy = df.copy() # Travailler sur une copie

# # # # # # # # # # # # # # #     # Cas spécifique pour VEA_SISSILI ou si seule la colonne 'Date' est présente
# # # # # # # # # # # # # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not all(col in df_copy.columns for col in ['Year', 'Month', 'Day'])):
# # # # # # # # # # # # # # #         try:
# # # # # # # # # # # # # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # # # # # # # # # # # # #         except Exception as e:
# # # # # # # # # # # # # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime: {e}")
# # # # # # # # # # # # # # #             df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # #         # Logique pour les colonnes Year, Month, Day, Hour, Minute
# # # # # # # # # # # # # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # # # # # # # # # # # # #         # Convertir les colonnes en numérique, forçant les erreurs en NaN
# # # # # # # # # # # # # # #         for col in date_cols:
# # # # # # # # # # # # # # #             if col in df_copy.columns:
# # # # # # # # # # # # # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # # # # # # # # # # # # #         # Supprimer les lignes où des valeurs temporelles essentielles sont manquantes
# # # # # # # # # # # # # # #         # (seulement les colonnes qui existent)
# # # # # # # # # # # # # # #         existing_date_cols = [col for col in date_cols if col in df_copy.columns]
# # # # # # # # # # # # # # #         if existing_date_cols:
# # # # # # # # # # # # # # #             df_copy = df_copy.dropna(subset=existing_date_cols)

# # # # # # # # # # # # # # #         # Créer la colonne Datetime si toutes les colonnes nécessaires sont présentes
# # # # # # # # # # # # # # #         if all(col in df_copy.columns for col in date_cols):
# # # # # # # # # # # # # # #             try:
# # # # # # # # # # # # # # #                 df_copy['Datetime'] = pd.to_datetime(df_copy[date_cols])
# # # # # # # # # # # # # # #             except Exception as e:
# # # # # # # # # # # # # # #                 warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées: {e}")
# # # # # # # # # # # # # # #                 df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # #             warnings.warn("Colonnes temporelles (Year, Month, Day, Hour, Minute) ou 'Date' manquantes. La colonne 'Datetime' n'a pas pu être créée.")
# # # # # # # # # # # # # # #             df_copy['Datetime'] = pd.NaT

# # # # # # # # # # # # # # #     # Extraire/recréer les composantes de date/heure si Datetime a été créé
# # # # # # # # # # # # # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # # # # # # # # # # # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # # # # # # # # # # # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # # # # # # # # # # # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # # # # # # # # # # # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # # # # # # # # # # # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # # # # # # # # # # # # #         df_copy['Date'] = df_copy['Datetime'].dt.date
# # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création.")

# # # # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # # # def check_compatibility_for_merge(df_list: list[pd.DataFrame]) -> tuple[bool, str]:
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     Vérifie si une liste de DataFrames est compatible pour la fusion (concaténation).
# # # # # # # # # # # # # # #     Les DataFrames sont considérés compatibles si :
# # # # # # # # # # # # # # #     - Ils ont le même nombre de colonnes.
# # # # # # # # # # # # # # #     - Ils ont les mêmes noms de colonnes.

# # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # #         df_list (list): Une liste de pandas DataFrames.

# # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # #         tuple: (bool, str) - True si compatible, False sinon, et un message d'erreur.
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     if not df_list:
# # # # # # # # # # # # # # #         return False, "La liste de DataFrames est vide."
# # # # # # # # # # # # # # #     if len(df_list) == 1:
# # # # # # # # # # # # # # #         return True, "Un seul DataFrame, pas de fusion nécessaire."

# # # # # # # # # # # # # # #     first_df = df_list[0]
# # # # # # # # # # # # # # #     num_cols_first = first_df.shape[1]
# # # # # # # # # # # # # # #     cols_first = set(first_df.columns)

# # # # # # # # # # # # # # #     for i, df in enumerate(df_list[1:]):
# # # # # # # # # # # # # # #         if df.shape[1] != num_cols_first:
# # # # # # # # # # # # # # #             return False, f"Le DataFrame {i+2} a {df.shape[1]} colonnes, le premier en a {num_cols_first}. Fusion impossible."
# # # # # # # # # # # # # # #         if set(df.columns) != cols_first:
# # # # # # # # # # # # # # #             missing_in_current = list(cols_first - set(df.columns))
# # # # # # # # # # # # # # #             extra_in_current = list(set(df.columns) - cols_first)
            
# # # # # # # # # # # # # # #             error_msg = f"Les colonnes du DataFrame {i+2} ne correspondent pas à celles du premier."
# # # # # # # # # # # # # # #             if missing_in_current:
# # # # # # # # # # # # # # #                 error_msg += f" Manquantes dans le DataFrame {i+2}: {missing_in_current}."
# # # # # # # # # # # # # # #             if extra_in_current:
# # # # # # # # # # # # # # #                 error_msg += f" Supplémentaires dans le DataFrame {i+2}: {extra_in_current}."
# # # # # # # # # # # # # # #             return False, error_msg
# # # # # # # # # # # # # # #     return True, "Tous les DataFrames sont compatibles pour la fusion."

# # # # # # # # # # # # # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     Crée une colonne 'Rain_mm' comme moyenne des mesures de deux pluviomètres.

# # # # # # # # # # # # # # #     Paramètres:
# # # # # # # # # # # # # # #     -----------
# # # # # # # # # # # # # # #     df : pandas.DataFrame
# # # # # # # # # # # # # # #         DataFrame contenant au minimum les colonnes 'Rain_01_mm' et 'Rain_02_mm'

# # # # # # # # # # # # # # #     Retourne:
# # # # # # # # # # # # # # #     --------
# # # # # # # # # # # # # # #     pandas.DataFrame
# # # # # # # # # # # # # # #         Le même DataFrame avec une nouvelle colonne 'Rain_mm' ajoutée
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     df_copy = df.copy()
# # # # # # # # # # # # # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # # # # # # # # # # # # #         df_copy['Rain_mm'] = df_copy[['Rain_01_mm', 'Rain_02_mm']].mean(axis=1)
# # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # #         df_copy['Rain_mm'] = np.nan
# # # # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     Supprime les doublons complets et nettoie les lignes avec des valeurs temporelles manquantes.
# # # # # # # # # # # # # # #     Gère les conflits où 'Datetime' est à la fois une colonne et un index.

# # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée.

# # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # #         pd.DataFrame: DataFrame traité.
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # # # # #     # Suppression des doublons complets
# # # # # # # # # # # # # # #     dup_count = df_copy.duplicated().sum()
# # # # # # # # # # # # # # #     if dup_count > 0:
# # # # # # # # # # # # # # #         warnings.warn(f"Suppression de {dup_count} doublons complets.")
# # # # # # # # # # # # # # #         df_copy = df_copy.drop_duplicates()

# # # # # # # # # # # # # # #     # Nettoyage des colonnes temporelles si elles existent
# # # # # # # # # # # # # # #     time_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
# # # # # # # # # # # # # # #     existing_time_cols = [col for col in time_cols if col in df_copy.columns]
# # # # # # # # # # # # # # #     if existing_time_cols:
# # # # # # # # # # # # # # #         initial_rows = len(df_copy)
# # # # # # # # # # # # # # #         df_copy = df_copy.dropna(subset=existing_time_cols)
# # # # # # # # # # # # # # #         removed_time_rows = initial_rows - len(df_copy)
# # # # # # # # # # # # # # #         if removed_time_rows > 0:
# # # # # # # # # # # # # # #             warnings.warn(f"Suppression de {removed_time_rows} lignes avec valeurs temporelles manquantes.")

# # # # # # # # # # # # # # #     # Gestion des conflits Datetime colonne/index
# # # # # # # # # # # # # # #     if 'Datetime' in df_copy.columns and isinstance(df_copy.index, pd.DatetimeIndex) and df_copy.index.name == 'Datetime':
# # # # # # # # # # # # # # #         pass
# # # # # # # # # # # # # # #     elif 'Datetime' in df_copy.columns and 'Datetime' in df_copy.index.names:
# # # # # # # # # # # # # # #         df_copy = df_copy.reset_index(drop=True)
    
# # # # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # # # # # # # # # # # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # # # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # # # # # # # # # # # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # # # # # # # # # # # # #         raise ValueError(
# # # # # # # # # # # # # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # # # # # # # # # # # # #         )

# # # # # # # # # # # # # # #     def convert_row(row):
# # # # # # # # # # # # # # #         try:
# # # # # # # # # # # # # # #             zone = int(row['zone'])
# # # # # # # # # # # # # # #             hemisphere = str(row['hemisphere']).upper()
# # # # # # # # # # # # # # #             is_northern = hemisphere == 'N'

# # # # # # # # # # # # # # #             proj_utm = CRS.from_proj4(
# # # # # # # # # # # # # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # # # # # # # # # # # # #             )
# # # # # # # # # # # # # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # # # # # # # # # # # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # # # # # # # # # # # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # # # # # # # # # # # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # # # # # # # # # # # # #         except Exception as e:
# # # # # # # # # # # # # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # # # # # # # # # # # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # # # # # # # # # # # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # # # # # # # # # # # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # # # # # # # # # # # # #     return df_copy


# # # # # # # # # # # # # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # # # # # # # # # # # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # # # # # # # # # # # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # # # # # # # # # # # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # # # # # # # # # # # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.

# # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # # # # # # # # # # # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # # # # # # # # # # # # #     data_dir = 'data'
# # # # # # # # # # # # # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # # # # # # # # # # # # #     # 1. Téléchargement et chargement du bassin VEA SISSILI
# # # # # # # # # # # # # # #     file_id_location = '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz'
# # # # # # # # # # # # # # #     output_file_location = os.path.join(data_dir, "WASCAL Basins Climate Station Coordinates.xlsx")
# # # # # # # # # # # # # # #     print(f"Téléchargement de Vea Sissili depuis Drive...")
# # # # # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_location}', output_file_location, quiet=False)
# # # # # # # # # # # # # # #     vea_sissili_bassin = pd.read_excel(output_file_location)

# # # # # # # # # # # # # # #     # 2. Téléchargement et chargement du bassin DANO
# # # # # # # # # # # # # # #     file_id_dano_basins = '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z'
# # # # # # # # # # # # # # #     output_file_dano_basins = os.path.join(data_dir, "Dano Basins Climate Station Coordinates.xlsx")
# # # # # # # # # # # # # # #     print(f"Téléchargement de Dano depuis Drive...")
# # # # # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_dano_basins}', output_file_dano_basins, quiet=False)
# # # # # # # # # # # # # # #     dano_bassin = pd.read_excel(output_file_dano_basins)

# # # # # # # # # # # # # # #     # 3. Téléchargement et chargement du bassin DASSARI
# # # # # # # # # # # # # # #     file_id_dassari = '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU'
# # # # # # # # # # # # # # #     output_file_dassari = os.path.join(data_dir, "DASSARI Climate Station Coordinates.xlsx")
# # # # # # # # # # # # # # #     print(f"Téléchargement de Dassari depuis Drive...")
# # # # # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_dassari}', output_file_dassari, quiet=False)
# # # # # # # # # # # # # # #     dassari_bassin = pd.read_excel(output_file_dassari)

# # # # # # # # # # # # # # #     # 4. Prétraitement des DataFrames (votre code original)
# # # # # # # # # # # # # # #     print("Début du prétraitement des données de stations...")
    
# # # # # # # # # # # # # # #     # Vea Sissili
# # # # # # # # # # # # # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # # # # # # # # # # # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # # # # # # # # # # # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # # # # # # # # # # # # #     # Dassari
# # # # # # # # # # # # # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # # # # # # # # # # # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # # # # # # # # # # # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # # # # # # # # # # # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # # # # # # # # # # # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # # # # # # # # # # # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # # # # # # # # # # # # #     dano_bassin['zone'] = 30
# # # # # # # # # # # # # # #     dano_bassin['hemisphere'] = 'N'
# # # # # # # # # # # # # # #     dassari_bassin['zone'] = 31
# # # # # # # # # # # # # # #     dassari_bassin['hemisphere'] = 'N'

# # # # # # # # # # # # # # #     # Application de la fonction de conversion UTM vers GPS
# # # # # # # # # # # # # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # # # # # # # # # # # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # # # # # # # # # # # # #     # Ajout des fuseaux horaires
# # # # # # # # # # # # # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # # # # # # # # # # # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # # # # # # # # # # # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # # # # # # # # # # # # #     # Fusion de tous les bassins
# # # # # # # # # # # # # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # # # # # # # # # # # # #     # Renommer 'Name' en 'Station'
# # # # # # # # # # # # # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # # # # # # # # # # # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # # # # # # # # # # # # #     initial_rows_count = len(bassins)
# # # # # # # # # # # # # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # # # # # # # # # # # # #     if len(bassins) < initial_rows_count:
# # # # # # # # # # # # # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # # # # # # # # # # # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # # # # # # # # # # # # #     if 'Station' in bassins.columns:
# # # # # # # # # # # # # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # # # # # # # # # # # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # # # # # # # # # # # # #     return bassins


# # # # # # # # # # # # # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     Effectue toutes les interpolations météorologiques en une seule passe,
# # # # # # # # # # # # # # #     en intégrant le calcul automatique du lever et du coucher du soleil
# # # # # # # # # # # # # # #     pour une gestion plus précise de la radiation solaire.

# # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # #         df (pd.DataFrame): Le DataFrame contenant les données météorologiques.
# # # # # # # # # # # # # # #                            Il doit contenir une colonne 'Station'.
# # # # # # # # # # # # # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # # # # # # # # # # # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # # # # # # # # # # # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # #         pd.DataFrame: Le DataFrame original avec les données interpolées et
# # # # # # # # # # # # # # #                       la colonne 'Is_Daylight' calculée, ainsi que la durée du jour.
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     df_processed = df.copy()

# # # # # # # # # # # # # # #     # --- Pré-vérification et préparation des données GPS ---
# # # # # # # # # # # # # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # # # # # # # # # # # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # # # # # # # # # # # # #         raise ValueError(
# # # # # # # # # # # # # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # # # # # # # # # # # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # # # # # # # # # # # # #         )

# # # # # # # # # # # # # # #     if not df_gps['Station'].is_unique:
# # # # # # # # # # # # # # #         warnings.warn("La colonne 'Station' dans df_gps contient des noms de station dupliqués. Suppression des doublons.")
# # # # # # # # # # # # # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # #         df_gps_unique = df_gps.copy()

# # # # # # # # # # # # # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # # # # # # # # # # # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # # # # # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # # # # # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # # # # # # # # # # # # #     for col in numerical_cols:
# # # # # # # # # # # # # # #         if col in df_processed.columns:
# # # # # # # # # # # # # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # # # # # # # # # # # # #     # Assurez-vous que 'Datetime' est bien une colonne et qu'elle n'est pas vide
# # # # # # # # # # # # # # #     if 'Datetime' not in df_processed.columns or df_processed['Datetime'].isnull().all():
# # # # # # # # # # # # # # #         raise ValueError("La colonne 'Datetime' est manquante ou toutes ses valeurs sont NaN. Impossible de procéder à l'interpolation.")

# # # # # # # # # # # # # # #     initial_rows = len(df_processed)
# # # # # # # # # # # # # # #     df_processed = df_processed.dropna(subset=['Datetime'])
# # # # # # # # # # # # # # #     if initial_rows - len(df_processed) > 0:
# # # # # # # # # # # # # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec valeurs Datetime manquantes ou invalides.")

# # # # # # # # # # # # # # #     df_processed_parts = []

# # # # # # # # # # # # # # #     for station_name, group in df_processed.groupby('Station'):
# # # # # # # # # # # # # # #         group_copy = group.copy()

# # # # # # # # # # # # # # #         # Extraire les infos GPS pour la station actuelle
# # # # # # # # # # # # # # #         gps_data = gps_info_dict.get(station_name)

# # # # # # # # # # # # # # #         # Assurez-vous que l'index est DatetimeIndex et trié
# # # # # # # # # # # # # # #         if not isinstance(group_copy.index, pd.DatetimeIndex) or group_copy.index.name != 'Datetime':
# # # # # # # # # # # # # # #             if 'Datetime' in group_copy.columns:
# # # # # # # # # # # # # # #                 group_copy = group_copy.set_index('Datetime', drop=True)
# # # # # # # # # # # # # # #             else:
# # # # # # # # # # # # # # #                 warnings.warn(f"La colonne 'Datetime' est manquante pour la station {station_name}. Impossible de définir l'index temporel.")
# # # # # # # # # # # # # # #                 group_copy['Is_Daylight'] = False # Default to no daylight if no time data
# # # # # # # # # # # # # # #                 group_copy['Daylight_Duration'] = "00:00:00"
# # # # # # # # # # # # # # #                 df_processed_parts.append(group_copy)
# # # # # # # # # # # # # # #                 continue # Passer à la station suivante

# # # # # # # # # # # # # # #         group_copy = group_copy.sort_index()
# # # # # # # # # # # # # # #         group_copy.index.name = 'Datetime' # Assurer que le nom de l'index est 'Datetime'


# # # # # # # # # # # # # # #         # Calcul de lever/coucher du soleil ou fallback
# # # # # # # # # # # # # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # # # # # # # # # # # # #             lat = gps_data['Lat']
# # # # # # # # # # # # # # #             long = gps_data['Long']
# # # # # # # # # # # # # # #             timezone_str = gps_data['Timezone']

# # # # # # # # # # # # # # #             try:
# # # # # # # # # # # # # # #                 tz = pytz.timezone(timezone_str)
# # # # # # # # # # # # # # #                 if group_copy.index.tz is None:
# # # # # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_localize(tz, ambiguous='NaT', nonexistent='NaT')
# # # # # # # # # # # # # # #                 else:
# # # # # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_convert(tz)

# # # # # # # # # # # # # # #                 daily_sun_info = {}
# # # # # # # # # # # # # # #                 for date_only in group_copy.index.normalize().unique():
# # # # # # # # # # # # # # #                     s = sun.sun(LocationInfo(station_name, "Site", timezone_str, lat, long).observer,
# # # # # # # # # # # # # # #                                 date=date_only, tzinfo=tz) # Use tz directly
# # # # # # # # # # # # # # #                     daily_sun_info[date_only.date()] = {
# # # # # # # # # # # # # # #                         'sunrise': s['sunrise'],
# # # # # # # # # # # # # # #                         'sunset': s['sunset']
# # # # # # # # # # # # # # #                     }

# # # # # # # # # # # # # # #                 group_copy.loc[:, 'sunrise_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunrise'))
# # # # # # # # # # # # # # #                 group_copy.loc[:, 'sunset_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunset'))

# # # # # # # # # # # # # # #                 # Assurer que les colonnes sunrise/sunset sont dans le même fuseau horaire que l'index pour la comparaison
# # # # # # # # # # # # # # #                 if group_copy.index.tz is not None:
# # # # # # # # # # # # # # #                      group_copy['sunrise_time_local'] = group_copy['sunrise_time_local'].dt.tz_convert(group_copy.index.tz)
# # # # # # # # # # # # # # #                      group_copy['sunset_time_local'] = group_copy['sunset_time_local'].dt.tz_convert(group_copy.index.tz)

# # # # # # # # # # # # # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index >= group_copy['sunrise_time_local']) & \
# # # # # # # # # # # # # # #                                                   (group_copy.index < group_copy['sunset_time_local'])

# # # # # # # # # # # # # # #                 daylight_timedelta = group_copy['sunset_time_local'] - group_copy['sunrise_time_local']
# # # # # # # # # # # # # # #                 group_copy.loc[:, 'Daylight_Duration'] = daylight_timedelta.apply(
# # # # # # # # # # # # # # #                     lambda td: (pd.to_datetime('00:00:00') + td).strftime('%H:%M:%S') if pd.notna(td) else np.nan
# # # # # # # # # # # # # # #                 )
# # # # # # # # # # # # # # #             except Exception as e:
# # # # # # # # # # # # # # #                 warnings.warn(f"Erreur Astral pour {station_name}: {e}. Utilisation du fallback 7h-18h.")
# # # # # # # # # # # # # # #                 if group_copy.index.tz is None:
# # # # # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_localize('UTC') # Fallback pour localiser l'index
# # # # # # # # # # # # # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # # # # # # # # # # # # #                 group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # #             warnings.warn(f"Coordonnées/Fuseau horaire manquants/invalides pour {station_name}. Utilisation du fallback 7h-18h.")
# # # # # # # # # # # # # # #             if group_copy.index.tz is None:
# # # # # # # # # # # # # # #                 group_copy.index = group_copy.index.tz_localize('UTC')
# # # # # # # # # # # # # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # # # # # # # # # # # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"

# # # # # # # # # # # # # # #         df_processed_parts.append(group_copy)

# # # # # # # # # # # # # # #     if not df_processed_parts:
# # # # # # # # # # # # # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # # # # # # # # # # # # #     df_final = pd.concat(df_processed_parts)
# # # # # # # # # # # # # # #     df_final = df_final.sort_index()
# # # # # # # # # # # # # # #     df_final.index.name = 'Datetime'

# # # # # # # # # # # # # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date',
# # # # # # # # # # # # # # #                                   'sunrise_time_local', 'sunset_time_local']
# # # # # # # # # # # # # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # # # # # # # # # # # # #     # Gestion intelligente de Rain_mm
# # # # # # # # # # # # # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # # # # # # # # # # # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # # # # # # # # # # # # #             df_final = create_rain_mm(df_final)
# # # # # # # # # # # # # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # # # # # # # # # # # # #             df_final['Rain_mm'] = np.nan


# # # # # # # # # # # # # # #     # Interpolation standard et bornage pour les variables numériques
# # # # # # # # # # # # # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # # # # # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # # # # # # #                      'Wind_Dir_Deg']

# # # # # # # # # # # # # # #     for var in standard_vars:
# # # # # # # # # # # # # # #         if var in df_final.columns:
# # # # # # # # # # # # # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # # # # # # # # # # # # #             if var in limits:
# # # # # # # # # # # # # # #                 min_val = limits[var]['min']
# # # # # # # # # # # # # # #                 max_val = limits[var]['max']
# # # # # # # # # # # # # # #                 initial_nan_count = df_final[var].isna().sum()
# # # # # # # # # # # # # # #                 df_final.loc[(df_final[var] < min_val) | (df_final[var] > max_val), var] = np.nan
# # # # # # # # # # # # # # #                 if df_final[var].isna().sum() > initial_nan_count:
# # # # # # # # # # # # # # #                     warnings.warn(f"Remplacement de {df_final[var].isna().sum() - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # # # # # # # # # # # # #             # Interpolation
# # # # # # # # # # # # # # #             df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # # # # # # # # # # # # #             df_final[var] = df_final[var].bfill().ffill() # Pour les NaNs aux extrémités

# # # # # # # # # # # # # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # # # # # # # # # # # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # # # # # # # # # # # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # # # # # # # # # # # # #         if 'Solar_R_W/m^2' in limits:
# # # # # # # # # # # # # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # # # # # # # # # # # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # # # # # # # # # # # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # # # # # # # # # # # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # # # # # # # # # # # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # # # # # # # # # # # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # # # # # # # # # # # # #         if 'Is_Daylight' in df_final.columns:
# # # # # # # # # # # # # # #             # Radiation positive hors du jour -> 0
# # # # # # # # # # # # # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # # # # # # # # # # # # #             # Zéros suspects en journée sans pluie -> NaN
# # # # # # # # # # # # # # #             if 'Rain_mm' in df_final.columns:
# # # # # # # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # # # # # # # # # # # # #             else:
# # # # # # # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # # # # # # # # # # # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # # # # # # # # # # # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # # # # # # # # # # # # #             # Interpolation dans les périodes de jour
# # # # # # # # # # # # # # #             is_day = df_final['Is_Daylight']
# # # # # # # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # # # # # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # # # # # # # # # # # # #             # Les NaNs restants hors des heures de jour -> 0
# # # # # # # # # # # # # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # # # # # # # # # # # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # # # # # # # # # # # # #             df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()

# # # # # # # # # # # # # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # # # # # # # # # # # # #     missing_after_interp = df_final.isna().sum()
# # # # # # # # # # # # # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # # # # # # # # # # # # #     if not columns_with_missing.empty:
# # # # # # # # # # # # # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # # # # # # # # # # # # #     return df_final

# # # # # # # # # # # # # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     Traite les valeurs aberrantes (outliers) dans un DataFrame de données météorologiques
# # # # # # # # # # # # # # #     en appliquant la méthode de l'écart interquartile (IQR) pour limiter les valeurs extrêmes.

# # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame contenant les données météorologiques à traiter.
# # # # # # # # # # # # # # #         limits (dict): Dictionnaire des variables à traiter (clés) avec leurs limites potentielles.
# # # # # # # # # # # # # # #                        (Note: les limites min/max du dict ne sont pas utilisées pour l'IQR,
# # # # # # # # # # # # # # #                        mais les clés du dict définissent les variables à analyser).

# # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # #         pd.DataFrame: Copie du DataFrame avec les outliers corrigés par la méthode IQR.
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # # # # #     for var in limits.keys(): # On utilise les clés de limits pour savoir quelles variables traiter
# # # # # # # # # # # # # # #         if var in df_copy.columns:
# # # # # # # # # # # # # # #             # S'assurer que la colonne est numérique
# # # # # # # # # # # # # # #             df_copy[var] = pd.to_numeric(df_copy[var], errors='coerce')

# # # # # # # # # # # # # # #             # Calcul des quartiles
# # # # # # # # # # # # # # #             Q1 = df_copy[var].quantile(0.25)
# # # # # # # # # # # # # # #             Q3 = df_copy[var].quantile(0.75)
# # # # # # # # # # # # # # #             IQR = Q3 - Q1

# # # # # # # # # # # # # # #             # Bornes IQR
# # # # # # # # # # # # # # #             borne_inf = Q1 - 1.5 * IQR
# # # # # # # # # # # # # # #             borne_sup = Q3 + 1.5 * IQR

# # # # # # # # # # # # # # #             # Application du bornage (clipping)
# # # # # # # # # # # # # # #             initial_outliers_count = df_copy[(df_copy[var] < borne_inf) | (df_copy[var] > borne_sup)][var].count()
# # # # # # # # # # # # # # #             df_copy[var] = df_copy[var].clip(lower=borne_inf, upper=borne_sup)
# # # # # # # # # # # # # # #             if initial_outliers_count > 0:
# # # # # # # # # # # # # # #                 warnings.warn(f"Outliers traités par IQR pour '{var}'. {initial_outliers_count} valeurs ajustées.")
# # # # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables météorologiques.

# # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec 'Datetime' comme index.

# # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # #         pd.DataFrame: DataFrame contenant les statistiques journalières.
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # # # #         raise TypeError("L'index du DataFrame doit être de type DatetimeIndex.")

# # # # # # # # # # # # # # #     # Définir les variables pour lesquelles nous voulons des statistiques
# # # # # # # # # # # # # # #     # et leurs méthodes d'agrégation.
# # # # # # # # # # # # # # #     # Rain_mm doit être sommée, les autres moyennées.
# # # # # # # # # # # # # # #     # Wind_Dir_Deg est un cas particulier, souvent on calcule une direction moyenne vectorielle
# # # # # # # # # # # # # # #     # mais pour des stats simples, une moyenne arithmétique peut être misleading.
# # # # # # # # # # # # # # #     # Pour cet exemple, on le met en moyenne, mais gardez cela à l'esprit.
# # # # # # # # # # # # # # #     agg_funcs = {
# # # # # # # # # # # # # # #         'Air_Temp_Deg_C': 'mean',
# # # # # # # # # # # # # # #         'Rel_H_%': 'mean',
# # # # # # # # # # # # # # #         'BP_mbar_Avg': 'mean',
# # # # # # # # # # # # # # #         'Rain_mm': 'sum', # Précipitation cumulée
# # # # # # # # # # # # # # #         'Wind_Sp_m/sec': 'mean',
# # # # # # # # # # # # # # #         'Solar_R_W/m^2': 'mean',
# # # # # # # # # # # # # # #         'Wind_Dir_Deg': 'mean' # Moyenne simple, à interpréter avec prudence
# # # # # # # # # # # # # # #     }

# # # # # # # # # # # # # # #     # Filtrer les colonnes qui existent dans le DataFrame
# # # # # # # # # # # # # # #     existing_cols_to_agg = {col: func for col, func in agg_funcs.items() if col in df.columns}

# # # # # # # # # # # # # # #     if not existing_cols_to_agg:
# # # # # # # # # # # # # # #         warnings.warn("Aucune colonne pertinente trouvée pour les statistiques journalières.")
# # # # # # # # # # # # # # #         return pd.DataFrame()

# # # # # # # # # # # # # # #     # Regrouper par jour (en utilisant l'index Datetime) et par station, puis agréger
# # # # # # # # # # # # # # #     daily_summary = df.groupby([df.index.date, 'Station']).agg(
# # # # # # # # # # # # # # #         **{f"{col}_{func}": (col, func) for col, func in existing_cols_to_agg.items()}
# # # # # # # # # # # # # # #     )
# # # # # # # # # # # # # # #     daily_summary.index.names = ['Date', 'Station']
# # # # # # # # # # # # # # #     return daily_summary.reset_index()


# # # # # # # # # # # # # # # # --- Fonctions de Visualisation ---

# # # # # # # # # # # # # # # # Fonction utilitaire pour convertir une figure Matplotlib en base64
# # # # # # # # # # # # # # # def fig_to_base64(fig: plt.Figure) -> str | None:
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     Convertit une figure Matplotlib en une chaîne Base64 encodée (format PNG).

# # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # #         fig (plt.Figure): La figure Matplotlib à convertir.

# # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG, ou None si la figure est invalide.
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     if fig is None:
# # # # # # # # # # # # # # #         return None
# # # # # # # # # # # # # # #     buf = io.BytesIO()
# # # # # # # # # # # # # # #     try:
# # # # # # # # # # # # # # #         fig.savefig(buf, format='png', bbox_inches='tight')
# # # # # # # # # # # # # # #         plt.close(fig) # Fermer la figure pour libérer la mémoire
# # # # # # # # # # # # # # #         data = base64.b64encode(buf.getbuffer()).decode("ascii")
# # # # # # # # # # # # # # #         return data
# # # # # # # # # # # # # # #     except Exception as e:
# # # # # # # # # # # # # # #         warnings.warn(f"Erreur lors de la conversion de la figure en base64: {e}")
# # # # # # # # # # # # # # #         plt.close(fig) # Assurez-vous de fermer la figure même en cas d'erreur
# # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station_name: str, variable_name: str, periode: str, custom_palette: list = None) -> str | None:
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     Génère un seul graphique de l'évolution d'une variable météorologique pour une station
# # # # # # # # # # # # # # #     spécifique et une période donnée.

# # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée contenant les données météorologiques.
# # # # # # # # # # # # # # #                            Doit inclure 'Datetime' comme index et 'Station'.
# # # # # # # # # # # # # # #         station_name (str): Nom de la station à visualiser.
# # # # # # # # # # # # # # #         variable_name (str): Nom de la variable à visualiser.
# # # # # # # # # # # # # # #         periode (str): Période d'agrégation ('Journalière', 'Hebdomadaire', 'Mensuelle', 'Annuelle').
# # # # # # # # # # # # # # #         custom_palette (list, optional): Palette de couleurs personnalisée.

# # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     if variable_name not in METADATA_VARIABLES or variable_name not in df.columns:
# # # # # # # # # # # # # # #         warnings.warn(f"Variable '{variable_name}' non valide ou absente du DataFrame.")
# # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # #     df_station = df[df['Station'] == station_name].copy()
# # # # # # # # # # # # # # #     if df_station.empty:
# # # # # # # # # # # # # # #         warnings.warn(f"Aucune donnée pour la station '{station_name}'.")
# # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # #     df_station[variable_name] = pd.to_numeric(df_station[variable_name], errors='coerce')

# # # # # # # # # # # # # # #     annees_uniques = df_station.index.year.nunique()
# # # # # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # # # # #         }
# # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # # # # #         }

# # # # # # # # # # # # # # #     if periode not in freq_configs:
# # # # # # # # # # # # # # #         warnings.warn(f"Période '{periode}' non valide.")
# # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # #     freq, date_fmt, rot, locator, label_x = freq_configs[periode]
# # # # # # # # # # # # # # #     meta = METADATA_VARIABLES[variable_name]
# # # # # # # # # # # # # # #     nom_complet = f"{meta['Nom']} ({meta['Unite']})"
# # # # # # # # # # # # # # #     agg_func = 'sum' if meta['agg_type'] == 'cumul' else 'mean'

# # # # # # # # # # # # # # #     # Déterminer la couleur de la station
# # # # # # # # # # # # # # #     stations_list = sorted(df['Station'].unique())
# # # # # # # # # # # # # # #     try:
# # # # # # # # # # # # # # #         station_idx = stations_list.index(station_name)
# # # # # # # # # # # # # # #     except ValueError:
# # # # # # # # # # # # # # #         warnings.warn(f"Station '{station_name}' non trouvée dans la liste des stations uniques.")
# # # # # # # # # # # # # # #         station_idx = 0 # Fallback
    
# # # # # # # # # # # # # # #     palette = custom_palette if custom_palette and len(custom_palette) > station_idx else sns.color_palette("husl", len(stations_list))
# # # # # # # # # # # # # # #     station_color = palette[station_idx] if len(palette) > station_idx else 'blue' # Fallback couleur

# # # # # # # # # # # # # # #     fig, ax = plt.subplots(figsize=(12, 7))

# # # # # # # # # # # # # # #     df_temp = df_station.copy()

# # # # # # # # # # # # # # #     if variable_name == 'Solar_R_W/m^2':
# # # # # # # # # # # # # # #         if 'Is_Daylight' in df_temp.columns:
# # # # # # # # # # # # # # #             df_temp = df_temp[df_temp['Is_Daylight']].copy()
# # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # #             warnings.warn("Avertissement: Colonne 'Is_Daylight' manquante pour la radiation solaire. Filtrage horaire non appliqué.")
# # # # # # # # # # # # # # #             df_temp = df_temp[(df_temp.index.hour >= 7) & (df_temp.index.hour <= 18)]
            
# # # # # # # # # # # # # # #     elif variable_name == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in df_temp.columns:
# # # # # # # # # # # # # # #         df_temp = df_temp[(df_temp['Wind_Sp_m/sec'] > 0) & (df_temp['Wind_Dir_Deg'].notna())]

# # # # # # # # # # # # # # #     with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # # # # #         df_agg = df_temp.groupby(pd.Grouper(freq=freq))[variable_name].agg(agg_func).reset_index()
# # # # # # # # # # # # # # #     df_agg.rename(columns={'index': 'Datetime'}, inplace=True)

# # # # # # # # # # # # # # #     if df_agg.empty:
# # # # # # # # # # # # # # #         plt.close(fig)
# # # # # # # # # # # # # # #         warnings.warn(f"Aucune donnée agrégée pour {variable_name} à la période {periode} pour {station_name}.")
# # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # #     sns.lineplot(
# # # # # # # # # # # # # # #         data=df_agg,
# # # # # # # # # # # # # # #         x='Datetime',
# # # # # # # # # # # # # # #         y=variable_name,
# # # # # # # # # # # # # # #         color=station_color,
# # # # # # # # # # # # # # #         linewidth=2,
# # # # # # # # # # # # # # #         marker='o' if periode == 'Annuelle' else None,
# # # # # # # # # # # # # # #         markersize=6 if periode == 'Annuelle' else 0,
# # # # # # # # # # # # # # #         ax=ax
# # # # # # # # # # # # # # #     )

# # # # # # # # # # # # # # #     ax.set_title(f"Évolution {periode.lower()} de {nom_complet} pour {station_name}", pad=10, fontsize=14)
# # # # # # # # # # # # # # #     ax.set_ylabel(nom_complet, fontsize=12)
# # # # # # # # # # # # # # #     ax.set_xlabel(label_x, fontsize=10)

# # # # # # # # # # # # # # #     ax.xaxis.set_major_formatter(date_fmt)
# # # # # # # # # # # # # # #     if locator:
# # # # # # # # # # # # # # #         ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # # # # #     for label in ax.get_xticklabels():
# # # # # # # # # # # # # # #         label.set_rotation(rot)
# # # # # # # # # # # # # # #         label.set_ha('right' if rot > 45 else 'center')

# # # # # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # # # # # def generer_graphique_comparatif(df: pd.DataFrame, variable_name: str, periode: str, custom_palette: list = None) -> str | None:
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     Génère un seul graphique comparatif de l'évolution d'une variable météorologique
# # # # # # # # # # # # # # #     entre toutes les stations pour une période donnée.

# # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée contenant les données météorologiques.
# # # # # # # # # # # # # # #                            Doit inclure 'Datetime' comme index et 'Station'.
# # # # # # # # # # # # # # #         variable_name (str): Nom de la variable à visualiser.
# # # # # # # # # # # # # # #         periode (str): Période d'agrégation ('Journalière', 'Hebdomadaire', 'Mensuelle', 'Annuelle').
# # # # # # # # # # # # # # #         custom_palette (list, optional): Palette de couleurs personnalisée.

# # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     if variable_name not in METADATA_VARIABLES or variable_name not in df.columns:
# # # # # # # # # # # # # # #         warnings.warn(f"Variable '{variable_name}' non valide ou absente du DataFrame.")
# # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # #     annees_uniques = df.index.year.nunique()
# # # # # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # # # # #         }
# # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # # # # #         }

# # # # # # # # # # # # # # #     if periode not in freq_configs:
# # # # # # # # # # # # # # #         warnings.warn(f"Période '{periode}' non valide.")
# # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # #     freq, date_fmt, rot, locator, label_x = freq_configs[periode]
# # # # # # # # # # # # # # #     meta = METADATA_VARIABLES[variable_name]
# # # # # # # # # # # # # # #     nom_complet = f"{meta['Nom']} ({meta['Unite']})"
# # # # # # # # # # # # # # #     agg_text = " (cumul)" if meta['is_rain'] else " (moyenne)"
# # # # # # # # # # # # # # #     agg_func = 'sum' if meta['is_rain'] else 'mean'

# # # # # # # # # # # # # # #     stations = sorted(df['Station'].unique())
# # # # # # # # # # # # # # #     palette = custom_palette if custom_palette and len(custom_palette) >= len(stations) \
# # # # # # # # # # # # # # #               else sns.color_palette("husl", len(stations))

# # # # # # # # # # # # # # #     fig, ax = plt.subplots(figsize=(14, 8))

# # # # # # # # # # # # # # #     temp_df = df.copy()

# # # # # # # # # # # # # # #     if variable_name == 'Solar_R_W/m^2':
# # # # # # # # # # # # # # #         if 'Is_Daylight' in temp_df.columns:
# # # # # # # # # # # # # # #             temp_df = temp_df[temp_df['Is_Daylight']].copy()
# # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # #             warnings.warn("Avertissement: Colonne 'Is_Daylight' manquante. Filtrage horaire par défaut (7h-18h) pour radiation solaire.")
# # # # # # # # # # # # # # #             temp_df = temp_df[(temp_df.index.hour >= 7) & (temp_df.index.hour <= 18)]
            
# # # # # # # # # # # # # # #     elif variable_name == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in temp_df.columns:
# # # # # # # # # # # # # # #         temp_df = temp_df[(temp_df['Wind_Sp_m/sec'] > 0) & (temp_df['Wind_Dir_Deg'].notna())]

# # # # # # # # # # # # # # #     with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # # # # #         df_agg = temp_df.groupby(['Station', pd.Grouper(freq=freq)])[variable_name].agg(agg_func).reset_index()

# # # # # # # # # # # # # # #     if df_agg.empty:
# # # # # # # # # # # # # # #         plt.close(fig)
# # # # # # # # # # # # # # #         warnings.warn(f"Aucune donnée agrégée pour {variable_name} à la période {periode}.")
# # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # #     sns.lineplot(
# # # # # # # # # # # # # # #         data=df_agg,
# # # # # # # # # # # # # # #         x='Datetime',
# # # # # # # # # # # # # # #         y=variable_name,
# # # # # # # # # # # # # # #         hue='Station',
# # # # # # # # # # # # # # #         palette=palette,
# # # # # # # # # # # # # # #         ax=ax,
# # # # # # # # # # # # # # #         linewidth=2,
# # # # # # # # # # # # # # #         marker='o' if periode == 'Annuelle' else None,
# # # # # # # # # # # # # # #         markersize=6 if periode == 'Annuelle' else 0,
# # # # # # # # # # # # # # #         legend='full'
# # # # # # # # # # # # # # #     )

# # # # # # # # # # # # # # #     ax.set_title(
# # # # # # # # # # # # # # #         f"Comparaison de {meta['Nom']} {agg_text} ({meta['Unite']}) - Période: {periode}",
# # # # # # # # # # # # # # #         fontsize=16,
# # # # # # # # # # # # # # #         pad=15,
# # # # # # # # # # # # # # #         weight='bold'
# # # # # # # # # # # # # # #     )
# # # # # # # # # # # # # # #     ax.set_xlabel(label_x, fontsize=12, labelpad=15)
# # # # # # # # # # # # # # #     ax.set_ylabel(nom_complet, fontsize=12)

# # # # # # # # # # # # # # #     ax.xaxis.set_major_formatter(date_fmt)
# # # # # # # # # # # # # # #     if locator:
# # # # # # # # # # # # # # #         ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # # # # #     for label in ax.get_xticklabels():
# # # # # # # # # # # # # # #         label.set_rotation(rot)
# # # # # # # # # # # # # # #         label.set_ha('right' if rot < 90 else 'center')
# # # # # # # # # # # # # # #         label.set_fontsize(10)

# # # # # # # # # # # # # # #     ax.legend(
# # # # # # # # # # # # # # #         title='Stations',
# # # # # # # # # # # # # # #         loc='upper left',
# # # # # # # # # # # # # # #         fontsize=10,
# # # # # # # # # # # # # # #         title_fontsize=12,
# # # # # # # # # # # # # # #         framealpha=0.9
# # # # # # # # # # # # # # #     )
# # # # # # # # # # # # # # #     ax.grid(True, alpha=0.2)

# # # # # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # # # # #     plt.subplots_adjust(
# # # # # # # # # # # # # # #         top=0.93,
# # # # # # # # # # # # # # #         hspace=0.3,
# # # # # # # # # # # # # # #         wspace=0.15
# # # # # # # # # # # # # # #     )
# # # # # # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # # # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station_name: str, variable_palette: list = None) -> str | None:
# # # # # # # # # # # # # # #     """
# # # # # # # # # # # # # # #     Génère un graphique comparatif normalisé de l'évolution de plusieurs variables météorologiques
# # # # # # # # # # # # # # #     pour une seule station sélectionnée, à différentes échelles temporelles.

# # # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame contenant les données météorologiques. Doit avoir 'Datetime' comme index.
# # # # # # # # # # # # # # #         station_name (str): Le nom de la station pour laquelle générer le graphique.
# # # # # # # # # # # # # # #         variable_palette (list, optional): Liste personnalisée de couleurs pour chaque variable.

# # # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # # # # #     """

# # # # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # #     df_station = df[df['Station'] == station_name].copy()

# # # # # # # # # # # # # # #     if df_station.empty:
# # # # # # # # # # # # # # #         warnings.warn(f"Station {station_name} sans données - ignorée.")
# # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # #     annees_uniques = df_station.index.year.nunique()
# # # # # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # # # # #         }
# # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # # # # #         }

# # # # # # # # # # # # # # #     nrows, ncols = (2, 2) if len(freq_configs) >= 2 else (1, 1)

# # # # # # # # # # # # # # #     variables_to_plot = [var for var in METADATA_VARIABLES.keys() if var in df_station.columns]

# # # # # # # # # # # # # # #     if not variables_to_plot:
# # # # # # # # # # # # # # #         warnings.warn("Aucune variable valide à tracer pour cette station.")
# # # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # #     num_vars_to_plot = len(variables_to_plot)
# # # # # # # # # # # # # # #     if (variable_palette is not None and
# # # # # # # # # # # # # # #         isinstance(variable_palette, list) and
# # # # # # # # # # # # # # #         len(variable_palette) >= num_vars_to_plot):
# # # # # # # # # # # # # # #         plot_palette = variable_palette
# # # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # # #         plot_palette = sns.color_palette("tab10", num_vars_to_plot)
# # # # # # # # # # # # # # #         if variable_palette is not None:
# # # # # # # # # # # # # # #             warnings.warn(f"Avertissement: Palette fournie insuffisante ou invalide - utilisation de la palette par défaut pour {station_name}.")

# # # # # # # # # # # # # # #     fig, axs = plt.subplots(nrows, ncols, figsize=(28, 22))
# # # # # # # # # # # # # # #     axs = axs.flatten()

# # # # # # # # # # # # # # #     fig.suptitle(
# # # # # # # # # # # # # # #         f'Évolution Normalisée des Variables Météorologiques - Station: {station_name}',
# # # # # # # # # # # # # # #         fontsize=24,
# # # # # # # # # # # # # # #         weight='bold',
# # # # # # # # # # # # # # #         y=1.02
# # # # # # # # # # # # # # #     )

# # # # # # # # # # # # # # #     for i, (periode, (freq, date_format, rot, locator, label_x)) in enumerate(freq_configs.items()):
# # # # # # # # # # # # # # #         if i >= len(axs):
# # # # # # # # # # # # # # #             continue

# # # # # # # # # # # # # # #         ax = axs[i]
# # # # # # # # # # # # # # #         agg_dict = {}
# # # # # # # # # # # # # # #         for var in variables_to_plot:
# # # # # # # # # # # # # # #             if var in df_station.columns:
# # # # # # # # # # # # # # #                 agg_type = METADATA_VARIABLES[var].get('agg_type', 'moyenne')
# # # # # # # # # # # # # # #                 agg_dict[var] = 'sum' if agg_type == 'cumul' else 'mean'

# # # # # # # # # # # # # # #         if not agg_dict:
# # # # # # # # # # # # # # #             warnings.warn(f"Aucune donnée à agréger pour {station_name} - période {periode}.")
# # # # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # # #             continue

# # # # # # # # # # # # # # #         with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # # # # #             for col in agg_dict.keys():
# # # # # # # # # # # # # # #                 if col in df_station.columns:
# # # # # # # # # # # # # # #                     df_station[col] = pd.to_numeric(df_station[col], errors='coerce')

# # # # # # # # # # # # # # #             cols_to_agg = list(agg_dict.keys())
# # # # # # # # # # # # # # #             if not cols_to_agg:
# # # # # # # # # # # # # # #                 warnings.warn(f"Aucune colonne numérique valide pour {station_name} - période {periode}.")
# # # # # # # # # # # # # # #                 if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # # #                 continue

# # # # # # # # # # # # # # #             df_agg = pd.DataFrame(index=df_station.index)

# # # # # # # # # # # # # # #             for var in cols_to_agg:
# # # # # # # # # # # # # # #                 temp_data = df_station[var]
# # # # # # # # # # # # # # #                 if var == 'Solar_R_W/m^2':
# # # # # # # # # # # # # # #                     if 'Is_Daylight' in df_station.columns:
# # # # # # # # # # # # # # #                         temp_data = df_station.loc[df_station['Is_Daylight'], var]
# # # # # # # # # # # # # # #                     else:
# # # # # # # # # # # # # # #                         temp_data = df_station.loc[(df_station.index.hour >= 7) & (df_station.index.hour <= 18), var]
# # # # # # # # # # # # # # #                         warnings.warn(f"Avertissement: 'Is_Daylight' non trouvé. Radiation solaire filtrée par heures fixes (7h-18h).")
# # # # # # # # # # # # # # #                 elif var == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in df_station.columns:
# # # # # # # # # # # # # # #                     temp_data = df_station.loc[(df_station['Wind_Sp_m/sec'] > 0) & (df_station['Wind_Dir_Deg'].notna()), var]

# # # # # # # # # # # # # # #                 if not temp_data.empty:
# # # # # # # # # # # # # # #                     df_agg[var] = temp_data.resample(freq).agg(agg_dict[var])
# # # # # # # # # # # # # # #                 else:
# # # # # # # # # # # # # # #                     df_agg[var] = np.nan

# # # # # # # # # # # # # # #             df_agg = df_agg.dropna(how='all').reset_index()


# # # # # # # # # # # # # # #         scaler = MinMaxScaler()
# # # # # # # # # # # # # # #         vars_to_scale = list(agg_dict.keys())
# # # # # # # # # # # # # # #         numeric_cols = [col for col in vars_to_scale if col in df_agg.columns]

# # # # # # # # # # # # # # #         if not numeric_cols or df_agg.empty:
# # # # # # # # # # # # # # #             warnings.warn(f"Données manquantes après agrégation ou colonnes numériques absentes pour {station_name} - période {periode}.")
# # # # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # # #             continue

# # # # # # # # # # # # # # #         df_agg_scaled = df_agg.copy()
# # # # # # # # # # # # # # #         valid_cols_for_scaling = [col for col in numeric_cols if df_agg_scaled[col].notna().any()]
# # # # # # # # # # # # # # #         if valid_cols_for_scaling:
# # # # # # # # # # # # # # #             df_agg_scaled[valid_cols_for_scaling] = scaler.fit_transform(df_agg_scaled[valid_cols_for_scaling])
# # # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # # #             warnings.warn(f"Aucune colonne avec des valeurs valides à normaliser pour {station_name} - période {periode}.")
# # # # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # # #             continue


# # # # # # # # # # # # # # #         df_melted = df_agg_scaled.melt(
# # # # # # # # # # # # # # #             id_vars=['Datetime'],
# # # # # # # # # # # # # # #             value_vars=numeric_cols,
# # # # # # # # # # # # # # #             var_name='Variable',
# # # # # # # # # # # # # # #             value_name='Valeur Normalisée'
# # # # # # # # # # # # # # #         )

# # # # # # # # # # # # # # #         if df_melted.empty or df_melted['Valeur Normalisée'].dropna().empty:
# # # # # # # # # # # # # # #             warnings.warn(f"Données normalisées vides ou toutes NaN pour {station_name} - période {periode}.")
# # # # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # # #             continue

# # # # # # # # # # # # # # #         variable_color_map = {var: plot_palette[j] for j, var in enumerate(variables_to_plot)}

# # # # # # # # # # # # # # #         sns.lineplot(
# # # # # # # # # # # # # # #             data=df_melted,
# # # # # # # # # # # # # # #             x='Datetime',
# # # # # # # # # # # # # # #             y='Valeur Normalisée',
# # # # # # # # # # # # # # #             hue='Variable',
# # # # # # # # # # # # # # #             palette=variable_color_map,
# # # # # # # # # # # # # # #             ax=ax,
# # # # # # # # # # # # # # #             linewidth=2,
# # # # # # # # # # # # # # #             marker='o' if periode == 'Annuelle' or len(df_melted['Datetime'].unique()) < 50 else None,
# # # # # # # # # # # # # # #             markersize=6,
# # # # # # # # # # # # # # #             legend='full'
# # # # # # # # # # # # # # #         )

# # # # # # # # # # # # # # #         ax.set_title(f"Évolution {periode.lower()}", fontsize=18, weight='bold', pad=15)
# # # # # # # # # # # # # # #         ax.set_xlabel(label_x, fontsize=14, labelpad=15)
# # # # # # # # # # # # # # #         ax.set_ylabel("Valeur Normalisée (0-1)", fontsize=14)

# # # # # # # # # # # # # # #         ax.xaxis.set_major_formatter(date_format)
# # # # # # # # # # # # # # #         if locator is not None:
# # # # # # # # # # # # # # #             ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # # # # #         for label in ax.get_xticklabels():
# # # # # # # # # # # # # # #             label.set_rotation(rot)
# # # # # # # # # # # # # # #             label.set_ha('right' if rot > 0 and rot < 90 else 'center')
# # # # # # # # # # # # # # #             label.set_fontsize(12)

# # # # # # # # # # # # # # #         ax.tick_params(axis='x', which='both', bottom=True, labelbottom=True)
# # # # # # # # # # # # # # #         ax.legend(
# # # # # # # # # # # # # # #             title='Variables',
# # # # # # # # # # # # # # #             loc='upper left',
# # # # # # # # # # # # # # #             fontsize=10,
# # # # # # # # # # # # # # #             title_fontsize=12,
# # # # # # # # # # # # # # #             framealpha=0.9
# # # # # # # # # # # # # # #         )
# # # # # # # # # # # # # # #         ax.grid(True, alpha=0.2)

# # # # # # # # # # # # # # #     for j in range(len(freq_configs), len(axs)):
# # # # # # # # # # # # # # #         if j < len(axs) and axs[j] is not None:
# # # # # # # # # # # # # # #             fig.delaxes(axs[j])

# # # # # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # # # # #     plt.subplots_adjust(top=0.94, hspace=0.4, wspace=0.15)
    
# # # # # # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # # # # import pandas as pd
# # # # # # # # # # # # # # import numpy as np
# # # # # # # # # # # # # # import matplotlib.pyplot as plt
# # # # # # # # # # # # # # import seaborn as sns
# # # # # # # # # # # # # # from matplotlib.dates import DateFormatter, MonthLocator, WeekdayLocator, YearLocator, DayLocator
# # # # # # # # # # # # # # from pyproj import CRS, Transformer
# # # # # # # # # # # # # # import pytz
# # # # # # # # # # # # # # from astral.location import LocationInfo
# # # # # # # # # # # # # # from astral import sun
# # # # # # # # # # # # # # from sklearn.preprocessing import MinMaxScaler
# # # # # # # # # # # # # # import io
# # # # # # # # # # # # # # import base64
# # # # # # # # # # # # # # import warnings
# # # # # # # # # # # # # # import gdown
# # # # # # # # # # # # # # import os

# # # # # # # # # # # # # # # Importation des métadonnées des variables depuis le fichier de configuration n'est plus directe ici
# # # # # # # # # # # # # # # Elles seront passées en argument aux fonctions de visualisation.
# # # # # # # # # # # # # # # from config import METADATA_VARIABLES # Cette ligne est supprimée pour éviter l'importation circulaire

# # # # # # # # # # # # # # # Supprimer les avertissements de SettingWithCopyWarning de Pandas,
# # # # # # # # # # # # # # # qui peuvent survenir avec df.loc et les chaînes d'opérations.
# # # # # # # # # # # # # # pd.options.mode.chained_assignment = None # default='warn'

# # # # # # # # # # # # # # # --- Fonctions de Traitement de Données ---

# # # # # # # # # # # # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # # # # # # # # # # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # # # # # # # # # # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # # # # # # # # # # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     df_copy = df.copy() # Travailler sur une copie

# # # # # # # # # # # # # #     # Cas spécifique pour VEA_SISSILI ou si seule la colonne 'Date' est présente
# # # # # # # # # # # # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not all(col in df_copy.columns for col in ['Year', 'Month', 'Day'])):
# # # # # # # # # # # # # #         try:
# # # # # # # # # # # # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # # # # # # # # # # # #         except Exception as e:
# # # # # # # # # # # # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime: {e}")
# # # # # # # # # # # # # #             df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # #         # Logique pour les colonnes Year, Month, Day, Hour, Minute
# # # # # # # # # # # # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # # # # # # # # # # # #         # Convertir les colonnes en numérique, forçant les erreurs en NaN
# # # # # # # # # # # # # #         for col in date_cols:
# # # # # # # # # # # # # #             if col in df_copy.columns:
# # # # # # # # # # # # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # # # # # # # # # # # #         # Supprimer les lignes où des valeurs temporelles essentielles sont manquantes
# # # # # # # # # # # # # #         # (seulement les colonnes qui existent)
# # # # # # # # # # # # # #         existing_date_cols = [col for col in date_cols if col in df_copy.columns]
# # # # # # # # # # # # # #         if existing_date_cols:
# # # # # # # # # # # # # #             df_copy = df_copy.dropna(subset=existing_date_cols)

# # # # # # # # # # # # # #         # Créer la colonne Datetime si toutes les colonnes nécessaires sont présentes
# # # # # # # # # # # # # #         if all(col in df_copy.columns for col in date_cols):
# # # # # # # # # # # # # #             try:
# # # # # # # # # # # # # #                 df_copy['Datetime'] = pd.to_datetime(df_copy[date_cols])
# # # # # # # # # # # # # #             except Exception as e:
# # # # # # # # # # # # # #                 warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées: {e}")
# # # # # # # # # # # # # #                 df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # #             warnings.warn("Colonnes temporelles (Year, Month, Day, Hour, Minute) ou 'Date' manquantes. La colonne 'Datetime' n'a pas pu être créée.")
# # # # # # # # # # # # # #             df_copy['Datetime'] = pd.NaT

# # # # # # # # # # # # # #     # Extraire/recréer les composantes de date/heure si Datetime a été créé
# # # # # # # # # # # # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # # # # # # # # # # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # # # # # # # # # # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # # # # # # # # # # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # # # # # # # # # # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # # # # # # # # # # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # # # # # # # # # # # #         df_copy['Date'] = df_copy['Datetime'].dt.date
# # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création.")

# # # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # # def check_compatibility_for_merge(df_list: list[pd.DataFrame]) -> tuple[bool, str]:
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     Vérifie si une liste de DataFrames est compatible pour la fusion (concaténation).
# # # # # # # # # # # # # #     Les DataFrames sont considérés compatibles si :
# # # # # # # # # # # # # #     - Ils ont le même nombre de colonnes.
# # # # # # # # # # # # # #     - Ils ont les mêmes noms de colonnes.

# # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # #         df_list (list): Une liste de pandas DataFrames.

# # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # #         tuple: (bool, str) - True si compatible, False sinon, et un message d'erreur.
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     if not df_list:
# # # # # # # # # # # # # #         return False, "La liste de DataFrames est vide."
# # # # # # # # # # # # # #     if len(df_list) == 1:
# # # # # # # # # # # # # #         return True, "Un seul DataFrame, pas de fusion nécessaire."

# # # # # # # # # # # # # #     first_df = df_list[0]
# # # # # # # # # # # # # #     num_cols_first = first_df.shape[1]
# # # # # # # # # # # # # #     cols_first = set(first_df.columns)

# # # # # # # # # # # # # #     for i, df in enumerate(df_list[1:]):
# # # # # # # # # # # # # #         if df.shape[1] != num_cols_first:
# # # # # # # # # # # # # #             return False, f"Le DataFrame {i+2} a {df.shape[1]} colonnes, le premier en a {num_cols_first}. Fusion impossible."
# # # # # # # # # # # # # #         if set(df.columns) != cols_first:
# # # # # # # # # # # # # #             missing_in_current = list(cols_first - set(df.columns))
# # # # # # # # # # # # # #             extra_in_current = list(set(df.columns) - cols_first)
            
# # # # # # # # # # # # # #             error_msg = f"Les colonnes du DataFrame {i+2} ne correspondent pas à celles du premier."
# # # # # # # # # # # # # #             if missing_in_current:
# # # # # # # # # # # # # #                 error_msg += f" Manquantes dans le DataFrame {i+2}: {missing_in_current}."
# # # # # # # # # # # # # #             if extra_in_current:
# # # # # # # # # # # # # #                 error_msg += f" Supplémentaires dans le DataFrame {i+2}: {extra_in_current}."
# # # # # # # # # # # # # #             return False, error_msg
# # # # # # # # # # # # # #     return True, "Tous les DataFrames sont compatibles pour la fusion."

# # # # # # # # # # # # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     Crée une colonne 'Rain_mm' comme moyenne des mesures de deux pluviomètres.

# # # # # # # # # # # # # #     Paramètres:
# # # # # # # # # # # # # #     -----------
# # # # # # # # # # # # # #     df : pandas.DataFrame
# # # # # # # # # # # # # #         DataFrame contenant au minimum les colonnes 'Rain_01_mm' et 'Rain_02_mm'

# # # # # # # # # # # # # #     Retourne:
# # # # # # # # # # # # # #     --------
# # # # # # # # # # # # # #     pandas.DataFrame
# # # # # # # # # # # # # #         Le même DataFrame avec une nouvelle colonne 'Rain_mm' ajoutée
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     df_copy = df.copy()
# # # # # # # # # # # # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # # # # # # # # # # # #         df_copy['Rain_mm'] = df_copy[['Rain_01_mm', 'Rain_02_mm']].mean(axis=1)
# # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # #         df_copy['Rain_mm'] = np.nan
# # # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     Supprime les doublons complets et nettoie les lignes avec des valeurs temporelles manquantes.
# # # # # # # # # # # # # #     Gère les conflits où 'Datetime' est à la fois une colonne et un index.

# # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée.

# # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # #         pd.DataFrame: DataFrame traité.
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # # # #     # Suppression des doublons complets
# # # # # # # # # # # # # #     dup_count = df_copy.duplicated().sum()
# # # # # # # # # # # # # #     if dup_count > 0:
# # # # # # # # # # # # # #         warnings.warn(f"Suppression de {dup_count} doublons complets.")
# # # # # # # # # # # # # #         df_copy = df_copy.drop_duplicates()

# # # # # # # # # # # # # #     # Nettoyage des colonnes temporelles si elles existent
# # # # # # # # # # # # # #     time_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
# # # # # # # # # # # # # #     existing_time_cols = [col for col in time_cols if col in df_copy.columns]
# # # # # # # # # # # # # #     if existing_time_cols:
# # # # # # # # # # # # # #         initial_rows = len(df_copy)
# # # # # # # # # # # # # #         df_copy = df_copy.dropna(subset=existing_time_cols)
# # # # # # # # # # # # # #         removed_time_rows = initial_rows - len(df_copy)
# # # # # # # # # # # # # #         if removed_time_rows > 0:
# # # # # # # # # # # # # #             warnings.warn(f"Suppression de {removed_time_rows} lignes avec valeurs temporelles manquantes.")

# # # # # # # # # # # # # #     # Gestion des conflits Datetime colonne/index
# # # # # # # # # # # # # #     if 'Datetime' in df_copy.columns and isinstance(df_copy.index, pd.DatetimeIndex) and df_copy.index.name == 'Datetime':
# # # # # # # # # # # # # #         pass
# # # # # # # # # # # # # #     elif 'Datetime' in df_copy.columns and 'Datetime' in df_copy.index.names:
# # # # # # # # # # # # # #         df_copy = df_copy.reset_index(drop=True)
    
# # # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # # # # # # # # # # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # # # # # # # # # # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # # # # # # # # # # # #         raise ValueError(
# # # # # # # # # # # # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # # # # # # # # # # # #         )

# # # # # # # # # # # # # #     def convert_row(row):
# # # # # # # # # # # # # #         try:
# # # # # # # # # # # # # #             zone = int(row['zone'])
# # # # # # # # # # # # # #             hemisphere = str(row['hemisphere']).upper()
# # # # # # # # # # # # # #             is_northern = hemisphere == 'N'

# # # # # # # # # # # # # #             proj_utm = CRS.from_proj4(
# # # # # # # # # # # # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # # # # # # # # # # # #             )
# # # # # # # # # # # # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # # # # # # # # # # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # # # # # # # # # # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # # # # # # # # # # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # # # # # # # # # # # #         except Exception as e:
# # # # # # # # # # # # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # # # # # # # # # # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # # # # # # # # # # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # # # # # # # # # # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # # # # # # # # # # # #     return df_copy


# # # # # # # # # # # # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # # # # # # # # # # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # # # # # # # # # # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # # # # # # # # # # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # # # # # # # # # # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.

# # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # # # # # # # # # # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # # # # # # # # # # # #     data_dir = 'data'
# # # # # # # # # # # # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # # # # # # # # # # # #     # 1. Téléchargement et chargement du bassin VEA SISSILI
# # # # # # # # # # # # # #     file_id_location = '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz'
# # # # # # # # # # # # # #     output_file_location = os.path.join(data_dir, "WASCAL Basins Climate Station Coordinates.xlsx")
# # # # # # # # # # # # # #     print(f"Téléchargement de Vea Sissili depuis Drive...")
# # # # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_location}', output_file_location, quiet=False)
# # # # # # # # # # # # # #     vea_sissili_bassin = pd.read_excel(output_file_location)

# # # # # # # # # # # # # #     # 2. Téléchargement et chargement du bassin DANO
# # # # # # # # # # # # # #     file_id_dano_basins = '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z'
# # # # # # # # # # # # # #     output_file_dano_basins = os.path.join(data_dir, "Dano Basins Climate Station Coordinates.xlsx")
# # # # # # # # # # # # # #     print(f"Téléchargement de Dano depuis Drive...")
# # # # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_dano_basins}', output_file_dano_basins, quiet=False)
# # # # # # # # # # # # # #     dano_bassin = pd.read_excel(output_file_dano_basins)

# # # # # # # # # # # # # #     # 3. Téléchargement et chargement du bassin DASSARI
# # # # # # # # # # # # # #     file_id_dassari = '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU'
# # # # # # # # # # # # # #     output_file_dassari = os.path.join(data_dir, "DASSARI Climate Station Coordinates.xlsx")
# # # # # # # # # # # # # #     print(f"Téléchargement de Dassari depuis Drive...")
# # # # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_dassari}', output_file_dassari, quiet=False)
# # # # # # # # # # # # # #     dassari_bassin = pd.read_excel(output_file_dassari)

# # # # # # # # # # # # # #     # 4. Prétraitement des DataFrames (votre code original)
# # # # # # # # # # # # # #     print("Début du prétraitement des données de stations...")
    
# # # # # # # # # # # # # #     # Vea Sissili
# # # # # # # # # # # # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # # # # # # # # # # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # # # # # # # # # # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # # # # # # # # # # # #     # Dassari
# # # # # # # # # # # # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # # # # # # # # # # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # # # # # # # # # # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # # # # # # # # # # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # # # # # # # # # # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # # # # # # # # # # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # # # # # # # # # # # #     dano_bassin['zone'] = 30
# # # # # # # # # # # # # #     dano_bassin['hemisphere'] = 'N'
# # # # # # # # # # # # # #     dassari_bassin['zone'] = 31
# # # # # # # # # # # # # #     dassari_bassin['hemisphere'] = 'N'

# # # # # # # # # # # # # #     # Application de la fonction de conversion UTM vers GPS
# # # # # # # # # # # # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # # # # # # # # # # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # # # # # # # # # # # #     # Ajout des fuseaux horaires
# # # # # # # # # # # # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # # # # # # # # # # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # # # # # # # # # # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # # # # # # # # # # # #     # Fusion de tous les bassins
# # # # # # # # # # # # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # # # # # # # # # # # #     # Renommer 'Name' en 'Station'
# # # # # # # # # # # # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # # # # # # # # # # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # # # # # # # # # # # #     initial_rows_count = len(bassins)
# # # # # # # # # # # # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # # # # # # # # # # # #     if len(bassins) < initial_rows_count:
# # # # # # # # # # # # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # # # # # # # # # # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # # # # # # # # # # # #     if 'Station' in bassins.columns:
# # # # # # # # # # # # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # # # # # # # # # # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # # # # # # # # # # # #     return bassins


# # # # # # # # # # # # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     Effectue toutes les interpolations météorologiques en une seule passe,
# # # # # # # # # # # # # #     en intégrant le calcul automatique du lever et du coucher du soleil
# # # # # # # # # # # # # #     pour une gestion plus précise de la radiation solaire.

# # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # #         df (pd.DataFrame): Le DataFrame contenant les données météorologiques.
# # # # # # # # # # # # # #                            Il doit contenir une colonne 'Station'.
# # # # # # # # # # # # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # # # # # # # # # # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # # # # # # # # # # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # #         pd.DataFrame: Le DataFrame original avec les données interpolées et
# # # # # # # # # # # # # #                       la colonne 'Is_Daylight' calculée, ainsi que la durée du jour.
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     df_processed = df.copy()

# # # # # # # # # # # # # #     # --- Pré-vérification et préparation des données GPS ---
# # # # # # # # # # # # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # # # # # # # # # # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # # # # # # # # # # # #         raise ValueError(
# # # # # # # # # # # # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # # # # # # # # # # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # # # # # # # # # # # #         )

# # # # # # # # # # # # # #     if not df_gps['Station'].is_unique:
# # # # # # # # # # # # # #         warnings.warn("La colonne 'Station' dans df_gps contient des noms de station dupliqués. Suppression des doublons.")
# # # # # # # # # # # # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # #         df_gps_unique = df_gps.copy()

# # # # # # # # # # # # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # # # # # # # # # # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # # # # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # # # # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # # # # # # # # # # # #     for col in numerical_cols:
# # # # # # # # # # # # # #         if col in df_processed.columns:
# # # # # # # # # # # # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # # # # # # # # # # # #     # Assurez-vous que 'Datetime' est bien une colonne et qu'elle n'est pas vide
# # # # # # # # # # # # # #     if 'Datetime' not in df_processed.columns or df_processed['Datetime'].isnull().all():
# # # # # # # # # # # # # #         raise ValueError("La colonne 'Datetime' est manquante ou toutes ses valeurs sont NaN. Impossible de procéder à l'interpolation.")

# # # # # # # # # # # # # #     initial_rows = len(df_processed)
# # # # # # # # # # # # # #     df_processed = df_processed.dropna(subset=['Datetime'])
# # # # # # # # # # # # # #     if initial_rows - len(df_processed) > 0:
# # # # # # # # # # # # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec valeurs Datetime manquantes ou invalides.")

# # # # # # # # # # # # # #     df_processed_parts = []

# # # # # # # # # # # # # #     for station_name, group in df_processed.groupby('Station'):
# # # # # # # # # # # # # #         group_copy = group.copy()

# # # # # # # # # # # # # #         # Extraire les infos GPS pour la station actuelle
# # # # # # # # # # # # # #         gps_data = gps_info_dict.get(station_name)

# # # # # # # # # # # # # #         # Assurez-vous que l'index est DatetimeIndex et trié
# # # # # # # # # # # # # #         if not isinstance(group_copy.index, pd.DatetimeIndex) or group_copy.index.name != 'Datetime':
# # # # # # # # # # # # # #             if 'Datetime' in group_copy.columns:
# # # # # # # # # # # # # #                 group_copy = group_copy.set_index('Datetime', drop=True)
# # # # # # # # # # # # # #             else:
# # # # # # # # # # # # # #                 warnings.warn(f"La colonne 'Datetime' est manquante pour la station {station_name}. Impossible de définir l'index temporel.")
# # # # # # # # # # # # # #                 group_copy['Is_Daylight'] = False # Default to no daylight if no time data
# # # # # # # # # # # # # #                 group_copy['Daylight_Duration'] = "00:00:00"
# # # # # # # # # # # # # #                 df_processed_parts.append(group_copy)
# # # # # # # # # # # # # #                 continue # Passer à la station suivante

# # # # # # # # # # # # # #         group_copy = group_copy.sort_index()
# # # # # # # # # # # # # #         group_copy.index.name = 'Datetime' # Assurer que le nom de l'index est 'Datetime'


# # # # # # # # # # # # # #         # Calcul de lever/coucher du soleil ou fallback
# # # # # # # # # # # # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # # # # # # # # # # # #             lat = gps_data['Lat']
# # # # # # # # # # # # # #             long = gps_data['Long']
# # # # # # # # # # # # # #             timezone_str = gps_data['Timezone']

# # # # # # # # # # # # # #             try:
# # # # # # # # # # # # # #                 tz = pytz.timezone(timezone_str)
# # # # # # # # # # # # # #                 if group_copy.index.tz is None:
# # # # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_localize(tz, ambiguous='NaT', nonexistent='NaT')
# # # # # # # # # # # # # #                 else:
# # # # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_convert(tz)

# # # # # # # # # # # # # #                 daily_sun_info = {}
# # # # # # # # # # # # # #                 for date_only in group_copy.index.normalize().unique():
# # # # # # # # # # # # # #                     s = sun.sun(LocationInfo(station_name, "Site", timezone_str, lat, long).observer,
# # # # # # # # # # # # # #                                 date=date_only, tzinfo=tz) # Use tz directly
# # # # # # # # # # # # # #                     daily_sun_info[date_only.date()] = {
# # # # # # # # # # # # # #                         'sunrise': s['sunrise'],
# # # # # # # # # # # # # #                         'sunset': s['sunset']
# # # # # # # # # # # # # #                     }

# # # # # # # # # # # # # #                 group_copy.loc[:, 'sunrise_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunrise'))
# # # # # # # # # # # # # #                 group_copy.loc[:, 'sunset_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunset'))

# # # # # # # # # # # # # #                 # Assurer que les colonnes sunrise/sunset sont dans le même fuseau horaire que l'index pour la comparaison
# # # # # # # # # # # # # #                 if group_copy.index.tz is not None:
# # # # # # # # # # # # # #                      group_copy['sunrise_time_local'] = group_copy['sunrise_time_local'].dt.tz_convert(group_copy.index.tz)
# # # # # # # # # # # # # #                      group_copy['sunset_time_local'] = group_copy['sunset_time_local'].dt.tz_convert(group_copy.index.tz)

# # # # # # # # # # # # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index >= group_copy['sunrise_time_local']) & \
# # # # # # # # # # # # # #                                                   (group_copy.index < group_copy['sunset_time_local'])

# # # # # # # # # # # # # #                 daylight_timedelta = group_copy['sunset_time_local'] - group_copy['sunrise_time_local']
# # # # # # # # # # # # # #                 group_copy.loc[:, 'Daylight_Duration'] = daylight_timedelta.apply(
# # # # # # # # # # # # # #                     lambda td: (pd.to_datetime('00:00:00') + td).strftime('%H:%M:%S') if pd.notna(td) else np.nan
# # # # # # # # # # # # # #                 )
# # # # # # # # # # # # # #             except Exception as e:
# # # # # # # # # # # # # #                 warnings.warn(f"Erreur Astral pour {station_name}: {e}. Utilisation du fallback 7h-18h.")
# # # # # # # # # # # # # #                 if group_copy.index.tz is None:
# # # # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_localize('UTC') # Fallback pour localiser l'index
# # # # # # # # # # # # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # # # # # # # # # # # #                 group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # #             warnings.warn(f"Coordonnées/Fuseau horaire manquants/invalides pour {station_name}. Utilisation du fallback 7h-18h.")
# # # # # # # # # # # # # #             if group_copy.index.tz is None:
# # # # # # # # # # # # # #                 group_copy.index = group_copy.index.tz_localize('UTC')
# # # # # # # # # # # # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # # # # # # # # # # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"

# # # # # # # # # # # # # #         df_processed_parts.append(group_copy)

# # # # # # # # # # # # # #     if not df_processed_parts:
# # # # # # # # # # # # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # # # # # # # # # # # #     df_final = pd.concat(df_processed_parts)
# # # # # # # # # # # # # #     df_final = df_final.sort_index()
# # # # # # # # # # # # # #     df_final.index.name = 'Datetime'

# # # # # # # # # # # # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date',
# # # # # # # # # # # # # #                                   'sunrise_time_local', 'sunset_time_local']
# # # # # # # # # # # # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # # # # # # # # # # # #     # Gestion intelligente de Rain_mm
# # # # # # # # # # # # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # # # # # # # # # # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # # # # # # # # # # # #             df_final = create_rain_mm(df_final)
# # # # # # # # # # # # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # # # # # # # # # # # #             df_final['Rain_mm'] = np.nan


# # # # # # # # # # # # # #     # Interpolation standard et bornage pour les variables numériques
# # # # # # # # # # # # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # # # # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # # # # # #                      'Wind_Dir_Deg']

# # # # # # # # # # # # # #     for var in standard_vars:
# # # # # # # # # # # # # #         if var in df_final.columns:
# # # # # # # # # # # # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # # # # # # # # # # # #             if var in limits:
# # # # # # # # # # # # # #                 min_val = limits[var]['min']
# # # # # # # # # # # # # #                 max_val = limits[var]['max']
# # # # # # # # # # # # # #                 initial_nan_count = df_final[var].isna().sum()
# # # # # # # # # # # # # #                 df_final.loc[(df_final[var] < min_val) | (df_final[var] > max_val), var] = np.nan
# # # # # # # # # # # # # #                 if df_final[var].isna().sum() > initial_nan_count:
# # # # # # # # # # # # # #                     warnings.warn(f"Remplacement de {df_final[var].isna().sum() - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # # # # # # # # # # # #             # Interpolation
# # # # # # # # # # # # # #             df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # # # # # # # # # # # #             df_final[var] = df_final[var].bfill().ffill() # Pour les NaNs aux extrémités

# # # # # # # # # # # # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # # # # # # # # # # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # # # # # # # # # # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # # # # # # # # # # # #         if 'Solar_R_W/m^2' in limits:
# # # # # # # # # # # # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # # # # # # # # # # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # # # # # # # # # # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # # # # # # # # # # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # # # # # # # # # # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # # # # # # # # # # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # # # # # # # # # # # #         if 'Is_Daylight' in df_final.columns:
# # # # # # # # # # # # # #             # Radiation positive hors du jour -> 0
# # # # # # # # # # # # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # # # # # # # # # # # #             # Zéros suspects en journée sans pluie -> NaN
# # # # # # # # # # # # # #             if 'Rain_mm' in df_final.columns:
# # # # # # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # # # # # # # # # # # #             else:
# # # # # # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # # # # # # # # # # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # # # # # # # # # # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # # # # # # # # # # # #             # Interpolation dans les périodes de jour
# # # # # # # # # # # # # #             is_day = df_final['Is_Daylight']
# # # # # # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # # # # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # # # # # # # # # # # #             # Les NaNs restants hors des heures de jour -> 0
# # # # # # # # # # # # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # # # # # # # # # # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # # # # # # # # # # # #             df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()

# # # # # # # # # # # # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # # # # # # # # # # # #     missing_after_interp = df_final.isna().sum()
# # # # # # # # # # # # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # # # # # # # # # # # #     if not columns_with_missing.empty:
# # # # # # # # # # # # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # # # # # # # # # # # #     return df_final

# # # # # # # # # # # # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     Traite les valeurs aberrantes (outliers) dans un DataFrame de données météorologiques
# # # # # # # # # # # # # #     en appliquant la méthode de l'écart interquartile (IQR) pour limiter les valeurs extrêmes.

# # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame contenant les données météorologiques à traiter.
# # # # # # # # # # # # # #         limits (dict): Dictionnaire des variables à traiter (clés) avec leurs limites potentielles.
# # # # # # # # # # # # # #                        (Note: les limites min/max du dict ne sont pas utilisées pour l'IQR,
# # # # # # # # # # # # # #                        mais les clés du dict définissent les variables à analyser).

# # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # #         pd.DataFrame: Copie du DataFrame avec les outliers corrigés par la méthode IQR.
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # # # #     for var in limits.keys(): # On utilise les clés de limits pour savoir quelles variables traiter
# # # # # # # # # # # # # #         if var in df_copy.columns:
# # # # # # # # # # # # # #             # S'assurer que la colonne est numérique
# # # # # # # # # # # # # #             df_copy[var] = pd.to_numeric(df_copy[var], errors='coerce')

# # # # # # # # # # # # # #             # Calcul des quartiles
# # # # # # # # # # # # # #             Q1 = df_copy[var].quantile(0.25)
# # # # # # # # # # # # # #             Q3 = df_copy[var].quantile(0.75)
# # # # # # # # # # # # # #             IQR = Q3 - Q1

# # # # # # # # # # # # # #             # Bornes IQR
# # # # # # # # # # # # # #             borne_inf = Q1 - 1.5 * IQR
# # # # # # # # # # # # # #             borne_sup = Q3 + 1.5 * IQR

# # # # # # # # # # # # # #             # Application du bornage (clipping)
# # # # # # # # # # # # # #             initial_outliers_count = df_copy[(df_copy[var] < borne_inf) | (df_copy[var] > borne_sup)][var].count()
# # # # # # # # # # # # # #             df_copy[var] = df_copy[var].clip(lower=borne_inf, upper=borne_sup)
# # # # # # # # # # # # # #             if initial_outliers_count > 0:
# # # # # # # # # # # # # #                 warnings.warn(f"Outliers traités par IQR pour '{var}'. {initial_outliers_count} valeurs ajustées.")
# # # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables météorologiques.

# # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec 'Datetime' comme index.

# # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # #         pd.DataFrame: DataFrame contenant les statistiques journalières.
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # # #         raise TypeError("L'index du DataFrame doit être de type DatetimeIndex.")

# # # # # # # # # # # # # #     # Définir les variables pour lesquelles nous voulons des statistiques
# # # # # # # # # # # # # #     # et leurs méthodes d'agrégation.
# # # # # # # # # # # # # #     # Rain_mm doit être sommée, les autres moyennées.
# # # # # # # # # # # # # #     # Wind_Dir_Deg est un cas particulier, souvent on calcule une direction moyenne vectorielle
# # # # # # # # # # # # # #     # mais pour des stats simples, une moyenne arithmétique peut être misleading.
# # # # # # # # # # # # # #     # Pour cet exemple, on le met en moyenne, mais gardez cela à l'esprit.
# # # # # # # # # # # # # #     agg_funcs = {
# # # # # # # # # # # # # #         'Air_Temp_Deg_C': 'mean',
# # # # # # # # # # # # # #         'Rel_H_%': 'mean',
# # # # # # # # # # # # # #         'BP_mbar_Avg': 'mean',
# # # # # # # # # # # # # #         'Rain_mm': 'sum', # Précipitation cumulée
# # # # # # # # # # # # # #         'Wind_Sp_m/sec': 'mean',
# # # # # # # # # # # # # #         'Solar_R_W/m^2': 'mean',
# # # # # # # # # # # # # #         'Wind_Dir_Deg': 'mean' # Moyenne simple, à interpréter avec prudence
# # # # # # # # # # # # # #     }

# # # # # # # # # # # # # #     # Filtrer les colonnes qui existent dans le DataFrame
# # # # # # # # # # # # # #     existing_cols_to_agg = {col: func for col, func in agg_funcs.items() if col in df.columns}

# # # # # # # # # # # # # #     if not existing_cols_to_agg:
# # # # # # # # # # # # # #         warnings.warn("Aucune colonne pertinente trouvée pour les statistiques journalières.")
# # # # # # # # # # # # # #         return pd.DataFrame()

# # # # # # # # # # # # # #     # Regrouper par jour (en utilisant l'index Datetime) et par station, puis agréger
# # # # # # # # # # # # # #     daily_summary = df.groupby([df.index.date, 'Station']).agg(
# # # # # # # # # # # # # #         **{f"{col}_{func}": (col, func) for col, func in existing_cols_to_agg.items()}
# # # # # # # # # # # # # #     )
# # # # # # # # # # # # # #     daily_summary.index.names = ['Date', 'Station']
# # # # # # # # # # # # # #     return daily_summary.reset_index()


# # # # # # # # # # # # # # # --- Fonctions de Visualisation ---

# # # # # # # # # # # # # # # Fonction utilitaire pour convertir une figure Matplotlib en base64
# # # # # # # # # # # # # # def fig_to_base64(fig: plt.Figure) -> str | None:
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     Convertit une figure Matplotlib en une chaîne Base64 encodée (format PNG).

# # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # #         fig (plt.Figure): La figure Matplotlib à convertir.

# # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG, ou None si la figure est invalide.
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     if fig is None:
# # # # # # # # # # # # # #         return None
# # # # # # # # # # # # # #     buf = io.BytesIO()
# # # # # # # # # # # # # #     try:
# # # # # # # # # # # # # #         fig.savefig(buf, format='png', bbox_inches='tight')
# # # # # # # # # # # # # #         plt.close(fig) # Fermer la figure pour libérer la mémoire
# # # # # # # # # # # # # #         data = base64.b64encode(buf.getbuffer()).decode("ascii")
# # # # # # # # # # # # # #         return data
# # # # # # # # # # # # # #     except Exception as e:
# # # # # # # # # # # # # #         warnings.warn(f"Erreur lors de la conversion de la figure en base64: {e}")
# # # # # # # # # # # # # #         plt.close(fig) # Assurez-vous de fermer la figure même en cas d'erreur
# # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station_name: str, variable_name: str, periode: str, custom_palette: list = None, metadata_variables: dict = None) -> str | None:
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     Génère un seul graphique de l'évolution d'une variable météorologique pour une station
# # # # # # # # # # # # # #     spécifique et une période donnée.

# # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée contenant les données météorologiques.
# # # # # # # # # # # # # #                            Doit inclure 'Datetime' comme index et 'Station'.
# # # # # # # # # # # # # #         station_name (str): Nom de la station à visualiser.
# # # # # # # # # # # # # #         variable_name (str): Nom de la variable à visualiser.
# # # # # # # # # # # # # #         periode (str): Période d'agrégation ('Journalière', 'Hebdomadaire', 'Mensuelle', 'Annuelle').
# # # # # # # # # # # # # #         custom_palette (list, optional): Palette de couleurs personnalisée.
# # # # # # # # # # # # # #         metadata_variables (dict, optional): Dictionnaire des métadonnées des variables.
# # # # # # # # # # # # # #                                             Si None, un avertissement sera émis.

# # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     if metadata_variables is None:
# # # # # # # # # # # # # #         warnings.warn("metadata_variables n'a pas été fourni à generer_graphique_par_variable_et_periode. Certaines informations pourraient être manquantes.")
# # # # # # # # # # # # # #         # Fallback si METADATA_VARIABLES n'est pas passé (moins robuste)
# # # # # # # # # # # # # #         metadata_variables = {
# # # # # # # # # # # # # #             'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'agg_type': 'cumul'},
# # # # # # # # # # # # # #             'Air_Temp_Deg_C': {'Nom': "Température ", 'Unite': "°C", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # #             'Rel_H_%': {'Nom': "Humidité Relative", 'Unite': "%", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # #             'Solar_R_W/m^2': {'Nom': "Radiation Solaire", 'Unite': "W/m²", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # #             'Wind_Sp_m/sec': {'Nom': "Vitesse du Vent", 'Unite': "m/s", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # #             'Wind_Dir_Deg': {'Nom': "Direction du Vent", 'Unite': "°", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # #             'BP_mbar_Avg': {'Nom': "Pression Atmospherique moyenne", 'Unite': "mbar", 'agg_type': 'moyenne'}
# # # # # # # # # # # # # #         }


# # # # # # # # # # # # # #     if variable_name not in metadata_variables or variable_name not in df.columns:
# # # # # # # # # # # # # #         warnings.warn(f"Variable '{variable_name}' non valide ou absente du DataFrame.")
# # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # #     df_station = df[df['Station'] == station_name].copy()
# # # # # # # # # # # # # #     if df_station.empty:
# # # # # # # # # # # # # #         warnings.warn(f"Aucune donnée pour la station '{station_name}'.")
# # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # #     df_station[variable_name] = pd.to_numeric(df_station[variable_name], errors='coerce')

# # # # # # # # # # # # # #     annees_uniques = df_station.index.year.nunique()
# # # # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # # # #         }
# # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # # # #         }

# # # # # # # # # # # # # #     if periode not in freq_configs:
# # # # # # # # # # # # # #         warnings.warn(f"Période '{periode}' non valide.")
# # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # #     freq, date_fmt, rot, locator, label_x = freq_configs[periode]
# # # # # # # # # # # # # #     meta = metadata_variables[variable_name]
# # # # # # # # # # # # # #     nom_complet = f"{meta['Nom']} ({meta['Unite']})"
# # # # # # # # # # # # # #     agg_func = 'sum' if meta['agg_type'] == 'cumul' else 'mean'

# # # # # # # # # # # # # #     # Déterminer la couleur de la station
# # # # # # # # # # # # # #     stations_list = sorted(df['Station'].unique())
# # # # # # # # # # # # # #     try:
# # # # # # # # # # # # # #         station_idx = stations_list.index(station_name)
# # # # # # # # # # # # # #     except ValueError:
# # # # # # # # # # # # # #         warnings.warn(f"Station '{station_name}' non trouvée dans la liste des stations uniques.")
# # # # # # # # # # # # # #         station_idx = 0 # Fallback
    
# # # # # # # # # # # # # #     palette = custom_palette if custom_palette and len(custom_palette) > station_idx else sns.color_palette("husl", len(stations_list))
# # # # # # # # # # # # # #     station_color = palette[station_idx] if len(palette) > station_idx else 'blue' # Fallback couleur

# # # # # # # # # # # # # #     fig, ax = plt.subplots(figsize=(12, 7))

# # # # # # # # # # # # # #     df_temp = df_station.copy()

# # # # # # # # # # # # # #     if variable_name == 'Solar_R_W/m^2':
# # # # # # # # # # # # # #         if 'Is_Daylight' in df_temp.columns:
# # # # # # # # # # # # # #             df_temp = df_temp[df_temp['Is_Daylight']].copy()
# # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # #             warnings.warn("Avertissement: Colonne 'Is_Daylight' manquante pour la radiation solaire. Filtrage horaire non appliqué.")
# # # # # # # # # # # # # #             df_temp = df_temp[(df_temp.index.hour >= 7) & (df_temp.index.hour <= 18)]
            
# # # # # # # # # # # # # #     elif variable_name == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in df_temp.columns:
# # # # # # # # # # # # # #         df_temp = df_temp[(df_temp['Wind_Sp_m/sec'] > 0) & (df_temp['Wind_Dir_Deg'].notna())]

# # # # # # # # # # # # # #     with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # # # #         df_agg = df_temp.groupby(pd.Grouper(freq=freq))[variable_name].agg(agg_func).reset_index()
# # # # # # # # # # # # # #     df_agg.rename(columns={'index': 'Datetime'}, inplace=True)

# # # # # # # # # # # # # #     if df_agg.empty:
# # # # # # # # # # # # # #         plt.close(fig)
# # # # # # # # # # # # # #         warnings.warn(f"Aucune donnée agrégée pour {variable_name} à la période {periode} pour {station_name}.")
# # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # #     sns.lineplot(
# # # # # # # # # # # # # #         data=df_agg,
# # # # # # # # # # # # # #         x='Datetime',
# # # # # # # # # # # # # #         y=variable_name,
# # # # # # # # # # # # # #         color=station_color,
# # # # # # # # # # # # # #         linewidth=2,
# # # # # # # # # # # # # #         marker='o' if periode == 'Annuelle' else None,
# # # # # # # # # # # # # #         markersize=6 if periode == 'Annuelle' else 0,
# # # # # # # # # # # # # #         ax=ax
# # # # # # # # # # # # # #     )

# # # # # # # # # # # # # #     ax.set_title(f"Évolution {periode.lower()} de {nom_complet} pour {station_name}", pad=10, fontsize=14)
# # # # # # # # # # # # # #     ax.set_ylabel(nom_complet, fontsize=12)
# # # # # # # # # # # # # #     ax.set_xlabel(label_x, fontsize=10)

# # # # # # # # # # # # # #     ax.xaxis.set_major_formatter(date_fmt)
# # # # # # # # # # # # # #     if locator:
# # # # # # # # # # # # # #         ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # # # #     for label in ax.get_xticklabels():
# # # # # # # # # # # # # #         label.set_rotation(rot)
# # # # # # # # # # # # # #         label.set_ha('right' if rot > 45 else 'center')

# # # # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # # # # def generer_graphique_comparatif(df: pd.DataFrame, variable_name: str, periode: str, custom_palette: list = None, metadata_variables: dict = None) -> str | None:
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     Génère un seul graphique comparatif de l'évolution d'une variable météorologique
# # # # # # # # # # # # # #     entre toutes les stations pour une période donnée.

# # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée contenant les données météorologiques.
# # # # # # # # # # # # # #                            Doit inclure 'Datetime' comme index et 'Station'.
# # # # # # # # # # # # # #         variable_name (str): Nom de la variable à visualiser.
# # # # # # # # # # # # # #         periode (str): Période d'agrégation ('Journalière', 'Hebdomadaire', 'Mensuelle', 'Annuelle').
# # # # # # # # # # # # # #         custom_palette (list, optional): Palette de couleurs personnalisée.
# # # # # # # # # # # # # #         metadata_variables (dict, optional): Dictionnaire des métadonnées des variables.
# # # # # # # # # # # # # #                                             Si None, un avertissement sera émis.

# # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     if metadata_variables is None:
# # # # # # # # # # # # # #         warnings.warn("metadata_variables n'a pas été fourni à generer_graphique_comparatif. Certaines informations pourraient être manquantes.")
# # # # # # # # # # # # # #         # Fallback si METADATA_VARIABLES n'est pas passé (moins robuste)
# # # # # # # # # # # # # #         metadata_variables = {
# # # # # # # # # # # # # #             'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'is_rain': True},
# # # # # # # # # # # # # #             'Air_Temp_Deg_C': {'Nom': "Température", 'Unite': "°C", 'is_rain': False},
# # # # # # # # # # # # # #             'Rel_H_%': {'Nom': "Humidité", 'Unite': "%", 'is_rain': False},
# # # # # # # # # # # # # #             'Solar_R_W/m^2': {'Nom': "Radiation solaire", 'Unite': "W/m²", 'is_rain': False},
# # # # # # # # # # # # # #             'Wind_Sp_m/sec': {'Nom': "Vitesse du vent", 'Unite': "m/s", 'is_rain': False},
# # # # # # # # # # # # # #             'Wind_Dir_Deg': {'Nom': "Direction du vent", 'Unite': "°", 'is_rain': False},
# # # # # # # # # # # # # #             'BP_mbar_Avg': {'Nom': "Pression atmospherique moyenne", 'Unite': "mbar", 'is_rain': False}
# # # # # # # # # # # # # #         }


# # # # # # # # # # # # # #     if variable_name not in metadata_variables or variable_name not in df.columns:
# # # # # # # # # # # # # #         warnings.warn(f"Variable '{variable_name}' non valide ou absente du DataFrame.")
# # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # #     annees_uniques = df.index.year.nunique()
# # # # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # # # #         }
# # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # # # #         }

# # # # # # # # # # # # # #     if periode not in freq_configs:
# # # # # # # # # # # # # #         warnings.warn(f"Période '{periode}' non valide.")
# # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # #     freq, date_fmt, rot, locator, label_x = freq_configs[periode]
# # # # # # # # # # # # # #     meta = metadata_variables[variable_name]
# # # # # # # # # # # # # #     nom_complet = f"{meta['Nom']} ({meta['Unite']})"
# # # # # # # # # # # # # #     agg_text = " (cumul)" if meta['is_rain'] else " (moyenne)"
# # # # # # # # # # # # # #     agg_func = 'sum' if meta['is_rain'] else 'mean'

# # # # # # # # # # # # # #     stations = sorted(df['Station'].unique())
# # # # # # # # # # # # # #     palette = custom_palette if custom_palette and len(custom_palette) >= len(stations) \
# # # # # # # # # # # # # #               else sns.color_palette("husl", len(stations))

# # # # # # # # # # # # # #     fig, ax = plt.subplots(figsize=(14, 8))

# # # # # # # # # # # # # #     temp_df = df.copy()

# # # # # # # # # # # # # #     if variable_name == 'Solar_R_W/m^2':
# # # # # # # # # # # # # #         if 'Is_Daylight' in temp_df.columns:
# # # # # # # # # # # # # #             temp_df = temp_df[temp_df['Is_Daylight']].copy()
# # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # #             warnings.warn("Avertissement: Colonne 'Is_Daylight' manquante. Filtrage horaire par défaut (7h-18h) pour radiation solaire.")
# # # # # # # # # # # # # #             temp_df = temp_df[(temp_df.index.hour >= 7) & (temp_df.index.hour <= 18)]
            
# # # # # # # # # # # # # #     elif variable_name == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in temp_df.columns:
# # # # # # # # # # # # # #         temp_df = temp_df[(temp_df['Wind_Sp_m/sec'] > 0) & (temp_df['Wind_Dir_Deg'].notna())]

# # # # # # # # # # # # # #     with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # # # #         df_agg = temp_df.groupby(['Station', pd.Grouper(freq=freq)])[variable_name].agg(agg_func).reset_index()

# # # # # # # # # # # # # #     if df_agg.empty:
# # # # # # # # # # # # # #         plt.close(fig)
# # # # # # # # # # # # # #         warnings.warn(f"Aucune donnée agrégée pour {variable_name} à la période {periode}.")
# # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # #     sns.lineplot(
# # # # # # # # # # # # # #         data=df_agg,
# # # # # # # # # # # # # #         x='Datetime',
# # # # # # # # # # # # # #         y=variable_name,
# # # # # # # # # # # # # #         hue='Station',
# # # # # # # # # # # # # #         palette=palette,
# # # # # # # # # # # # # #         ax=ax,
# # # # # # # # # # # # # #         linewidth=2,
# # # # # # # # # # # # # #         marker='o' if periode == 'Annuelle' else None,
# # # # # # # # # # # # # #         markersize=6 if periode == 'Annuelle' else 0,
# # # # # # # # # # # # # #         legend='full'
# # # # # # # # # # # # # #     )

# # # # # # # # # # # # # #     ax.set_title(
# # # # # # # # # # # # # #         f"Comparaison de {meta['Nom']} {agg_text} ({meta['Unite']}) - Période: {periode}",
# # # # # # # # # # # # # #         fontsize=16,
# # # # # # # # # # # # # #         pad=15,
# # # # # # # # # # # # # #         weight='bold'
# # # # # # # # # # # # # #     )
# # # # # # # # # # # # # #     ax.set_xlabel(label_x, fontsize=12, labelpad=15)
# # # # # # # # # # # # # #     ax.set_ylabel(nom_complet, fontsize=12)

# # # # # # # # # # # # # #     ax.xaxis.set_major_formatter(date_fmt)
# # # # # # # # # # # # # #     if locator:
# # # # # # # # # # # # # #         ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # # # #     for label in ax.get_xticklabels():
# # # # # # # # # # # # # #         label.set_rotation(rot)
# # # # # # # # # # # # # #         label.set_ha('right' if rot < 90 else 'center')
# # # # # # # # # # # # # #         label.set_fontsize(10)

# # # # # # # # # # # # # #     ax.legend(
# # # # # # # # # # # # # #         title='Stations',
# # # # # # # # # # # # # #         loc='upper left',
# # # # # # # # # # # # # #         fontsize=10,
# # # # # # # # # # # # # #         title_fontsize=12,
# # # # # # # # # # # # # #         framealpha=0.9
# # # # # # # # # # # # # #     )
# # # # # # # # # # # # # #     ax.grid(True, alpha=0.2)

# # # # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # # # #     plt.subplots_adjust(
# # # # # # # # # # # # # #         top=0.93,
# # # # # # # # # # # # # #         hspace=0.3,
# # # # # # # # # # # # # #         wspace=0.15
# # # # # # # # # # # # # #     )
# # # # # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station_name: str, variable_palette: list = None, metadata_variables: dict = None) -> str | None:
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     Génère un graphique comparatif normalisé de l'évolution de plusieurs variables météorologiques
# # # # # # # # # # # # # #     pour une seule station sélectionnée, à différentes échelles temporelles.

# # # # # # # # # # # # # #     Args:
# # # # # # # # # # # # # #         df (pd.DataFrame): DataFrame contenant les données météorologiques. Doit avoir 'Datetime' comme index.
# # # # # # # # # # # # # #         station_name (str): Le nom de la station pour laquelle générer le graphique.
# # # # # # # # # # # # # #         variable_palette (list, optional): Liste personnalisée de couleurs pour chaque variable.
# # # # # # # # # # # # # #         metadata_variables (dict, optional): Dictionnaire des métadonnées des variables.
# # # # # # # # # # # # # #                                             Si None, un avertissement sera émis.

# # # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # # # #     """
# # # # # # # # # # # # # #     if metadata_variables is None:
# # # # # # # # # # # # # #         warnings.warn("metadata_variables n'a pas été fourni à generate_multi_variable_station_plot. Certaines informations pourraient être manquantes.")
# # # # # # # # # # # # # #         # Fallback si METADATA_VARIABLES n'est pas passé (moins robuste)
# # # # # # # # # # # # # #         metadata_variables = {
# # # # # # # # # # # # # #             'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'agg_type': 'cumul'},
# # # # # # # # # # # # # #             'Air_Temp_Deg_C': {'Nom': "Température ", 'Unite': "°C", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # #             'Rel_H_%': {'Nom': "Humidité Relative", 'Unite': "%", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # #             'Solar_R_W/m^2': {'Nom': "Radiation Solaire", 'Unite': "W/m²", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # #             'Wind_Sp_m/sec': {'Nom': "Vitesse du Vent", 'Unite': "m/s", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # #             'Wind_Dir_Deg': {'Nom': "Direction du Vent", 'Unite': "°", 'agg_type': 'moyenne'},
# # # # # # # # # # # # # #             'BP_mbar_Avg': {'Nom': "Pression Atmospherique moyenne", 'Unite': "mbar", 'agg_type': 'moyenne'}
# # # # # # # # # # # # # #         }


# # # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # #     df_station = df[df['Station'] == station_name].copy()

# # # # # # # # # # # # # #     if df_station.empty:
# # # # # # # # # # # # # #         warnings.warn(f"Station {station_name} sans données - ignorée.")
# # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # #     annees_uniques = df_station.index.year.nunique()
# # # # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # # # #         }
# # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # # # #         }

# # # # # # # # # # # # # #     nrows, ncols = (2, 2) if len(freq_configs) >= 2 else (1, 1)

# # # # # # # # # # # # # #     variables_to_plot = [var for var in metadata_variables.keys() if var in df_station.columns]

# # # # # # # # # # # # # #     if not variables_to_plot:
# # # # # # # # # # # # # #         warnings.warn("Aucune variable valide à tracer pour cette station.")
# # # # # # # # # # # # # #         return None

# # # # # # # # # # # # # #     num_vars_to_plot = len(variables_to_plot)
# # # # # # # # # # # # # #     if (variable_palette is not None and
# # # # # # # # # # # # # #         isinstance(variable_palette, list) and
# # # # # # # # # # # # # #         len(variable_palette) >= num_vars_to_plot):
# # # # # # # # # # # # # #         plot_palette = variable_palette
# # # # # # # # # # # # # #     else:
# # # # # # # # # # # # # #         plot_palette = sns.color_palette("tab10", num_vars_to_plot)
# # # # # # # # # # # # # #         if variable_palette is not None:
# # # # # # # # # # # # # #             warnings.warn(f"Avertissement: Palette fournie insuffisante ou invalide - utilisation de la palette par défaut pour {station_name}.")

# # # # # # # # # # # # # #     fig, axs = plt.subplots(nrows, ncols, figsize=(28, 22))
# # # # # # # # # # # # # #     axs = axs.flatten()

# # # # # # # # # # # # # #     fig.suptitle(
# # # # # # # # # # # # # #         f'Évolution Normalisée des Variables Météorologiques - Station: {station_name}',
# # # # # # # # # # # # # #         fontsize=24,
# # # # # # # # # # # # # #         weight='bold',
# # # # # # # # # # # # # #         y=1.02
# # # # # # # # # # # # # #     )

# # # # # # # # # # # # # #     for i, (periode, (freq, date_format, rot, locator, label_x)) in enumerate(freq_configs.items()):
# # # # # # # # # # # # # #         if i >= len(axs):
# # # # # # # # # # # # # #             continue

# # # # # # # # # # # # # #         ax = axs[i]
# # # # # # # # # # # # # #         agg_dict = {}
# # # # # # # # # # # # # #         for var in variables_to_plot:
# # # # # # # # # # # # # #             if var in df_station.columns:
# # # # # # # # # # # # # #                 agg_type = metadata_variables[var].get('agg_type', 'moyenne')
# # # # # # # # # # # # # #                 agg_dict[var] = 'sum' if agg_type == 'cumul' else 'mean'

# # # # # # # # # # # # # #         if not agg_dict:
# # # # # # # # # # # # # #             warnings.warn(f"Aucune donnée à agréger pour {station_name} - période {periode}.")
# # # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # #             continue

# # # # # # # # # # # # # #         with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # # # #             for col in agg_dict.keys():
# # # # # # # # # # # # # #                 if col in df_station.columns:
# # # # # # # # # # # # # #                     df_station[col] = pd.to_numeric(df_station[col], errors='coerce')

# # # # # # # # # # # # # #             cols_to_agg = list(agg_dict.keys())
# # # # # # # # # # # # # #             if not cols_to_agg:
# # # # # # # # # # # # # #                 warnings.warn(f"Aucune colonne numérique valide pour {station_name} - période {periode}.")
# # # # # # # # # # # # # #                 if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # #                 continue

# # # # # # # # # # # # # #             df_agg = pd.DataFrame(index=df_station.index)

# # # # # # # # # # # # # #             for var in cols_to_agg:
# # # # # # # # # # # # # #                 temp_data = df_station[var]
# # # # # # # # # # # # # #                 if var == 'Solar_R_W/m^2':
# # # # # # # # # # # # # #                     if 'Is_Daylight' in df_station.columns:
# # # # # # # # # # # # # #                         temp_data = df_station.loc[df_station['Is_Daylight'], var]
# # # # # # # # # # # # # #                     else:
# # # # # # # # # # # # # #                         temp_data = df_station.loc[(df_station.index.hour >= 7) & (df_station.index.hour <= 18), var]
# # # # # # # # # # # # # #                         warnings.warn(f"Avertissement: 'Is_Daylight' non trouvé. Radiation solaire filtrée par heures fixes (7h-18h).")
# # # # # # # # # # # # # #                 elif var == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in df_station.columns:
# # # # # # # # # # # # # #                     temp_data = df_station.loc[(df_station['Wind_Sp_m/sec'] > 0) & (df_station['Wind_Dir_Deg'].notna()), var]

# # # # # # # # # # # # # #                 if not temp_data.empty:
# # # # # # # # # # # # # #                     df_agg[var] = temp_data.resample(freq).agg(agg_dict[var])
# # # # # # # # # # # # # #                 else:
# # # # # # # # # # # # # #                     df_agg[var] = np.nan

# # # # # # # # # # # # # #             df_agg = df_agg.dropna(how='all').reset_index()


# # # # # # # # # # # # # #         scaler = MinMaxScaler()
# # # # # # # # # # # # # #         vars_to_scale = list(agg_dict.keys())
# # # # # # # # # # # # # #         numeric_cols = [col for col in vars_to_scale if col in df_agg.columns]

# # # # # # # # # # # # # #         if not numeric_cols or df_agg.empty:
# # # # # # # # # # # # # #             warnings.warn(f"Données manquantes après agrégation ou colonnes numériques absentes pour {station_name} - période {periode}.")
# # # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # #             continue

# # # # # # # # # # # # # #         df_agg_scaled = df_agg.copy()
# # # # # # # # # # # # # #         valid_cols_for_scaling = [col for col in numeric_cols if df_agg_scaled[col].notna().any()]
# # # # # # # # # # # # # #         if valid_cols_for_scaling:
# # # # # # # # # # # # # #             df_agg_scaled[valid_cols_for_scaling] = scaler.fit_transform(df_agg_scaled[valid_cols_for_scaling])
# # # # # # # # # # # # # #         else:
# # # # # # # # # # # # # #             warnings.warn(f"Aucune colonne avec des valeurs valides à normaliser pour {station_name} - période {periode}.")
# # # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # #             continue


# # # # # # # # # # # # # #         df_melted = df_agg_scaled.melt(
# # # # # # # # # # # # # #             id_vars=['Datetime'],
# # # # # # # # # # # # # #             value_vars=numeric_cols,
# # # # # # # # # # # # # #             var_name='Variable',
# # # # # # # # # # # # # #             value_name='Valeur Normalisée'
# # # # # # # # # # # # # #         )

# # # # # # # # # # # # # #         if df_melted.empty or df_melted['Valeur Normalisée'].dropna().empty:
# # # # # # # # # # # # # #             warnings.warn(f"Données normalisées vides ou toutes NaN pour {station_name} - période {periode}.")
# # # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # # #             continue

# # # # # # # # # # # # # #         variable_color_map = {var: plot_palette[j] for j, var in enumerate(variables_to_plot)}

# # # # # # # # # # # # # #         sns.lineplot(
# # # # # # # # # # # # # #             data=df_melted,
# # # # # # # # # # # # # #             x='Datetime',
# # # # # # # # # # # # # #             y='Valeur Normalisée',
# # # # # # # # # # # # # #             hue='Variable',
# # # # # # # # # # # # # #             palette=variable_color_map,
# # # # # # # # # # # # # #             ax=ax,
# # # # # # # # # # # # # #             linewidth=2,
# # # # # # # # # # # # # #             marker='o' if periode == 'Annuelle' or len(df_melted['Datetime'].unique()) < 50 else None,
# # # # # # # # # # # # # #             markersize=6,
# # # # # # # # # # # # # #             legend='full'
# # # # # # # # # # # # # #         )

# # # # # # # # # # # # # #         ax.set_title(f"Évolution {periode.lower()}", fontsize=18, weight='bold', pad=15)
# # # # # # # # # # # # # #         ax.set_xlabel(label_x, fontsize=14, labelpad=15)
# # # # # # # # # # # # # #         ax.set_ylabel("Valeur Normalisée (0-1)", fontsize=14)

# # # # # # # # # # # # # #         ax.xaxis.set_major_formatter(date_format)
# # # # # # # # # # # # # #         if locator is not None:
# # # # # # # # # # # # # #             ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # # # #         for label in ax.get_xticklabels():
# # # # # # # # # # # # # #             label.set_rotation(rot)
# # # # # # # # # # # # # #             label.set_ha('right' if rot > 0 and rot < 90 else 'center')
# # # # # # # # # # # # # #             label.set_fontsize(12)

# # # # # # # # # # # # # #         ax.tick_params(axis='x', which='both', bottom=True, labelbottom=True)
# # # # # # # # # # # # # #         ax.legend(
# # # # # # # # # # # # # #             title='Variables',
# # # # # # # # # # # # # #             loc='upper left',
# # # # # # # # # # # # # #             fontsize=10,
# # # # # # # # # # # # # #             title_fontsize=12,
# # # # # # # # # # # # # #             framealpha=0.9
# # # # # # # # # # # # # #         )
# # # # # # # # # # # # # #         ax.grid(True, alpha=0.2)

# # # # # # # # # # # # # #     for j in range(len(freq_configs), len(axs)):
# # # # # # # # # # # # # #         if j < len(axs) and axs[j] is not None:
# # # # # # # # # # # # # #             fig.delaxes(axs[j])

# # # # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # # # #     plt.subplots_adjust(top=0.94, hspace=0.4, wspace=0.15)
    
# # # # # # # # # # # # # #     return fig_to_base64(fig)



# # # # # # # # # # # # # import pandas as pd
# # # # # # # # # # # # # import numpy as np
# # # # # # # # # # # # # import matplotlib.pyplot as plt
# # # # # # # # # # # # # import seaborn as sns
# # # # # # # # # # # # # from matplotlib.dates import DateFormatter, MonthLocator, WeekdayLocator, YearLocator, DayLocator
# # # # # # # # # # # # # from pyproj import CRS, Transformer
# # # # # # # # # # # # # import pytz
# # # # # # # # # # # # # from astral.location import LocationInfo
# # # # # # # # # # # # # from astral import sun
# # # # # # # # # # # # # from sklearn.preprocessing import MinMaxScaler
# # # # # # # # # # # # # import io
# # # # # # # # # # # # # import base64
# # # # # # # # # # # # # import warnings
# # # # # # # # # # # # # import gdown
# # # # # # # # # # # # # import os

# # # # # # # # # # # # # # Importation des métadonnées des variables depuis le fichier de configuration
# # # # # # # # # # # # # from config import METADATA_VARIABLES

# # # # # # # # # # # # # # Supprimer les avertissements de SettingWithCopyWarning de Pandas,
# # # # # # # # # # # # # # qui peuvent survenir avec df.loc et les chaînes d'opérations.
# # # # # # # # # # # # # pd.options.mode.chained_assignment = None # default='warn'

# # # # # # # # # # # # # # --- Fonctions de Traitement de Données ---

# # # # # # # # # # # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # # # # # # # # # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # # # # # # # # # # #     Args:
# # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # # # # # # # # # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # # # # # # # # # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     df_copy = df.copy() # Travailler sur une copie

# # # # # # # # # # # # #     # Cas spécifique pour VEA_SISSILI ou si seule la colonne 'Date' est présente
# # # # # # # # # # # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not all(col in df_copy.columns for col in ['Year', 'Month', 'Day'])):
# # # # # # # # # # # # #         try:
# # # # # # # # # # # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # # # # # # # # # # #         except Exception as e:
# # # # # # # # # # # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime: {e}")
# # # # # # # # # # # # #             df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # # # # # # # # # # #     else:
# # # # # # # # # # # # #         # Logique pour les colonnes Year, Month, Day, Hour, Minute
# # # # # # # # # # # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # # # # # # # # # # #         # Convertir les colonnes en numérique, forçant les erreurs en NaN
# # # # # # # # # # # # #         for col in date_cols:
# # # # # # # # # # # # #             if col in df_copy.columns:
# # # # # # # # # # # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # # # # # # # # # # #         # Supprimer les lignes où des valeurs temporelles essentielles sont manquantes
# # # # # # # # # # # # #         # (seulement les colonnes qui existent)
# # # # # # # # # # # # #         existing_date_cols = [col for col in date_cols if col in df_copy.columns]
# # # # # # # # # # # # #         if existing_date_cols:
# # # # # # # # # # # # #             df_copy = df_copy.dropna(subset=existing_date_cols)

# # # # # # # # # # # # #         # Créer la colonne Datetime si toutes les colonnes nécessaires sont présentes
# # # # # # # # # # # # #         if all(col in df_copy.columns for col in date_cols):
# # # # # # # # # # # # #             try:
# # # # # # # # # # # # #                 df_copy['Datetime'] = pd.to_datetime(df_copy[date_cols])
# # # # # # # # # # # # #             except Exception as e:
# # # # # # # # # # # # #                 warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées: {e}")
# # # # # # # # # # # # #                 df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # # # # # # # # # # #         else:
# # # # # # # # # # # # #             warnings.warn("Colonnes temporelles (Year, Month, Day, Hour, Minute) ou 'Date' manquantes. La colonne 'Datetime' n'a pas pu être créée.")
# # # # # # # # # # # # #             df_copy['Datetime'] = pd.NaT

# # # # # # # # # # # # #     # Extraire/recréer les composantes de date/heure si Datetime a été créé
# # # # # # # # # # # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # # # # # # # # # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # # # # # # # # # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # # # # # # # # # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # # # # # # # # # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # # # # # # # # # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # # # # # # # # # # #         df_copy['Date'] = df_copy['Datetime'].dt.date
# # # # # # # # # # # # #     else:
# # # # # # # # # # # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création.")

# # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # def check_compatibility_for_merge(df_list: list[pd.DataFrame]) -> tuple[bool, str]:
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     Vérifie si une liste de DataFrames est compatible pour la fusion (concaténation).
# # # # # # # # # # # # #     Les DataFrames sont considérés compatibles si :
# # # # # # # # # # # # #     - Ils ont le même nombre de colonnes.
# # # # # # # # # # # # #     - Ils ont les mêmes noms de colonnes.

# # # # # # # # # # # # #     Args:
# # # # # # # # # # # # #         df_list (list): Une liste de pandas DataFrames.

# # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # #         tuple: (bool, str) - True si compatible, False sinon, et un message d'erreur.
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     if not df_list:
# # # # # # # # # # # # #         return False, "La liste de DataFrames est vide."
# # # # # # # # # # # # #     if len(df_list) == 1:
# # # # # # # # # # # # #         return True, "Un seul DataFrame, pas de fusion nécessaire."

# # # # # # # # # # # # #     first_df = df_list[0]
# # # # # # # # # # # # #     num_cols_first = first_df.shape[1]
# # # # # # # # # # # # #     cols_first = set(first_df.columns)

# # # # # # # # # # # # #     for i, df in enumerate(df_list[1:]):
# # # # # # # # # # # # #         if df.shape[1] != num_cols_first:
# # # # # # # # # # # # #             return False, f"Le DataFrame {i+2} a {df.shape[1]} colonnes, le premier en a {num_cols_first}. Fusion impossible."
# # # # # # # # # # # # #         if set(df.columns) != cols_first:
# # # # # # # # # # # # #             missing_in_current = list(cols_first - set(df.columns))
# # # # # # # # # # # # #             extra_in_current = list(set(df.columns) - cols_first)
            
# # # # # # # # # # # # #             error_msg = f"Les colonnes du DataFrame {i+2} ne correspondent pas à celles du premier."
# # # # # # # # # # # # #             if missing_in_current:
# # # # # # # # # # # # #                 error_msg += f" Manquantes dans le DataFrame {i+2}: {missing_in_current}."
# # # # # # # # # # # # #             if extra_in_current:
# # # # # # # # # # # # #                 error_msg += f" Supplémentaires dans le DataFrame {i+2}: {extra_in_current}."
# # # # # # # # # # # # #             return False, error_msg
# # # # # # # # # # # # #     return True, "Tous les DataFrames sont compatibles pour la fusion."

# # # # # # # # # # # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     Crée une colonne 'Rain_mm' comme moyenne des mesures de deux pluviomètres.

# # # # # # # # # # # # #     Paramètres:
# # # # # # # # # # # # #     -----------
# # # # # # # # # # # # #     df : pandas.DataFrame
# # # # # # # # # # # # #         DataFrame contenant au minimum les colonnes 'Rain_01_mm' et 'Rain_02_mm'

# # # # # # # # # # # # #     Retourne:
# # # # # # # # # # # # #     --------
# # # # # # # # # # # # #     pandas.DataFrame
# # # # # # # # # # # # #         Le même DataFrame avec une nouvelle colonne 'Rain_mm' ajoutée
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     df_copy = df.copy()
# # # # # # # # # # # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # # # # # # # # # # #         df_copy['Rain_mm'] = df_copy[['Rain_01_mm', 'Rain_02_mm']].mean(axis=1)
# # # # # # # # # # # # #     else:
# # # # # # # # # # # # #         df_copy['Rain_mm'] = np.nan
# # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     Supprime les doublons complets et nettoie les lignes avec des valeurs temporelles manquantes.
# # # # # # # # # # # # #     Gère les conflits où 'Datetime' est à la fois une colonne et un index.

# # # # # # # # # # # # #     Args:
# # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée.

# # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # #         pd.DataFrame: DataFrame traité.
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # # #     # Suppression des doublons complets
# # # # # # # # # # # # #     dup_count = df_copy.duplicated().sum()
# # # # # # # # # # # # #     if dup_count > 0:
# # # # # # # # # # # # #         warnings.warn(f"Suppression de {dup_count} doublons complets.")
# # # # # # # # # # # # #         df_copy = df_copy.drop_duplicates()

# # # # # # # # # # # # #     # Nettoyage des colonnes temporelles si elles existent
# # # # # # # # # # # # #     time_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
# # # # # # # # # # # # #     existing_time_cols = [col for col in time_cols if col in df_copy.columns]
# # # # # # # # # # # # #     if existing_time_cols:
# # # # # # # # # # # # #         initial_rows = len(df_copy)
# # # # # # # # # # # # #         df_copy = df_copy.dropna(subset=existing_time_cols)
# # # # # # # # # # # # #         removed_time_rows = initial_rows - len(df_copy)
# # # # # # # # # # # # #         if removed_time_rows > 0:
# # # # # # # # # # # # #             warnings.warn(f"Suppression de {removed_time_rows} lignes avec valeurs temporelles manquantes.")

# # # # # # # # # # # # #     # Gestion des conflits Datetime colonne/index
# # # # # # # # # # # # #     if 'Datetime' in df_copy.columns and isinstance(df_copy.index, pd.DatetimeIndex) and df_copy.index.name == 'Datetime':
# # # # # # # # # # # # #         pass
# # # # # # # # # # # # #     elif 'Datetime' in df_copy.columns and 'Datetime' in df_copy.index.names:
# # # # # # # # # # # # #         df_copy = df_copy.reset_index(drop=True)
    
# # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # # # # # # # # # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # # # # # # # # # # #     Args:
# # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # # # # # # # # # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # # # # # # # # # # #         raise ValueError(
# # # # # # # # # # # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # # # # # # # # # # #         )

# # # # # # # # # # # # #     def convert_row(row):
# # # # # # # # # # # # #         try:
# # # # # # # # # # # # #             zone = int(row['zone'])
# # # # # # # # # # # # #             hemisphere = str(row['hemisphere']).upper()
# # # # # # # # # # # # #             is_northern = hemisphere == 'N'

# # # # # # # # # # # # #             proj_utm = CRS.from_proj4(
# # # # # # # # # # # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # # # # # # # # # # #             )
# # # # # # # # # # # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # # # # # # # # # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # # # # # # # # # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # # # # # # # # # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # # # # # # # # # # #         except Exception as e:
# # # # # # # # # # # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # # # # # # # # # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # # # # # # # # # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # # # # # # # # # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # # # # # # # # # # #     return df_copy


# # # # # # # # # # # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # # # # # # # # # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # # # # # # # # # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # # # # # # # # # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # # # # # # # # # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.

# # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # # # # # # # # # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # # # # # # # # # # #     data_dir = 'data'
# # # # # # # # # # # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # # # # # # # # # # #     # 1. Téléchargement et chargement du bassin VEA SISSILI
# # # # # # # # # # # # #     file_id_location = '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz'
# # # # # # # # # # # # #     output_file_location = os.path.join(data_dir, "WASCAL Basins Climate Station Coordinates.xlsx")
# # # # # # # # # # # # #     print(f"Téléchargement de Vea Sissili depuis Drive...")
# # # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_location}', output_file_location, quiet=False)
# # # # # # # # # # # # #     vea_sissili_bassin = pd.read_excel(output_file_location)

# # # # # # # # # # # # #     # 2. Téléchargement et chargement du bassin DANO
# # # # # # # # # # # # #     file_id_dano_basins = '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z'
# # # # # # # # # # # # #     output_file_dano_basins = os.path.join(data_dir, "Dano Basins Climate Station Coordinates.xlsx")
# # # # # # # # # # # # #     print(f"Téléchargement de Dano depuis Drive...")
# # # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_dano_basins}', output_file_dano_basins, quiet=False)
# # # # # # # # # # # # #     dano_bassin = pd.read_excel(output_file_dano_basins)

# # # # # # # # # # # # #     # 3. Téléchargement et chargement du bassin DASSARI
# # # # # # # # # # # # #     file_id_dassari = '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU'
# # # # # # # # # # # # #     output_file_dassari = os.path.join(data_dir, "DASSARI Climate Station Coordinates.xlsx")
# # # # # # # # # # # # #     print(f"Téléchargement de Dassari depuis Drive...")
# # # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_dassari}', output_file_dassari, quiet=False)
# # # # # # # # # # # # #     dassari_bassin = pd.read_excel(output_file_dassari)

# # # # # # # # # # # # #     # 4. Prétraitement des DataFrames (votre code original)
# # # # # # # # # # # # #     print("Début du prétraitement des données de stations...")
    
# # # # # # # # # # # # #     # Vea Sissili
# # # # # # # # # # # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # # # # # # # # # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # # # # # # # # # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # # # # # # # # # # #     # Dassari
# # # # # # # # # # # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # # # # # # # # # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # # # # # # # # # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # # # # # # # # # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # # # # # # # # # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # # # # # # # # # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # # # # # # # # # # #     dano_bassin['zone'] = 30
# # # # # # # # # # # # #     dano_bassin['hemisphere'] = 'N'
# # # # # # # # # # # # #     dassari_bassin['zone'] = 31
# # # # # # # # # # # # #     dassari_bassin['hemisphere'] = 'N'

# # # # # # # # # # # # #     # Application de la fonction de conversion UTM vers GPS
# # # # # # # # # # # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # # # # # # # # # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # # # # # # # # # # #     # Ajout des fuseaux horaires
# # # # # # # # # # # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # # # # # # # # # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # # # # # # # # # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # # # # # # # # # # #     # Fusion de tous les bassins
# # # # # # # # # # # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # # # # # # # # # # #     # Renommer 'Name' en 'Station'
# # # # # # # # # # # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # # # # # # # # # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # # # # # # # # # # #     initial_rows_count = len(bassins)
# # # # # # # # # # # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # # # # # # # # # # #     if len(bassins) < initial_rows_count:
# # # # # # # # # # # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # # # # # # # # # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # # # # # # # # # # #     if 'Station' in bassins.columns:
# # # # # # # # # # # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # # # # # # # # # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # # # # # # # # # # #     return bassins


# # # # # # # # # # # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     Effectue toutes les interpolations météorologiques en une seule passe,
# # # # # # # # # # # # #     en intégrant le calcul automatique du lever et du coucher du soleil
# # # # # # # # # # # # #     pour une gestion plus précise de la radiation solaire.

# # # # # # # # # # # # #     Args:
# # # # # # # # # # # # #         df (pd.DataFrame): Le DataFrame d'entrée. Son index DOIT ÊTRE un DatetimeIndex.
# # # # # # # # # # # # #                            Il doit également contenir une colonne 'Station'.
# # # # # # # # # # # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # # # # # # # # # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # # # # # # # # # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # #         pd.DataFrame: Le DataFrame original avec les données interpolées et
# # # # # # # # # # # # #                       la colonne 'Is_Daylight' calculée, ainsi que la durée du jour.
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     df_processed = df.copy()

# # # # # # # # # # # # #     # VÉRIFICATIONS CRITIQUES POUR L'INDEX TEMPOREL
# # # # # # # # # # # # #     # S'assurer que l'index est bien un DatetimeIndex
# # # # # # # # # # # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # # # # # # # # # # #         raise TypeError("L'index du DataFrame doit être de type DatetimeIndex pour l'interpolation.")

# # # # # # # # # # # # #     # Supprime les lignes où l'index Datetime est NaT (valeur manquante)
# # # # # # # # # # # # #     # C'est la source de l'erreur précédente car on cherchait une colonne 'Datetime'
# # # # # # # # # # # # #     # alors qu'elle était l'index.
# # # # # # # # # # # # #     initial_rows = len(df_processed)
# # # # # # # # # # # # #     df_processed = df_processed[df_processed.index.notna()]
# # # # # # # # # # # # #     if len(df_processed) == 0:
# # # # # # # # # # # # #         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
# # # # # # # # # # # # #     if initial_rows - len(df_processed) > 0:
# # # # # # # # # # # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")

# # # # # # # # # # # # #     # --- Pré-vérification et préparation des données GPS (inchangée) ---
# # # # # # # # # # # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # # # # # # # # # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # # # # # # # # # # #         raise ValueError(
# # # # # # # # # # # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # # # # # # # # # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # # # # # # # # # # #         )

# # # # # # # # # # # # #     if not df_gps['Station'].is_unique:
# # # # # # # # # # # # #         warnings.warn("La colonne 'Station' dans df_gps contient des noms de station dupliqués. Suppression des doublons.")
# # # # # # # # # # # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # # # # # # # # # # #     else:
# # # # # # # # # # # # #         df_gps_unique = df_gps.copy()

# # # # # # # # # # # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # # # # # # # # # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # # # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # # # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # # # # # # # # # # #     for col in numerical_cols:
# # # # # # # # # # # # #         if col in df_processed.columns:
# # # # # # # # # # # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # # # # # # # # # # #     df_processed_parts = []

# # # # # # # # # # # # #     for station_name, group in df_processed.groupby('Station'):
# # # # # # # # # # # # #         group_copy = group.copy()

# # # # # # # # # # # # #         # Extraire les infos GPS pour la station actuelle
# # # # # # # # # # # # #         gps_data = gps_info_dict.get(station_name)

# # # # # # # # # # # # #         # L'index est déjà un DatetimeIndex à ce stade grâce aux vérifications précédentes
# # # # # # # # # # # # #         # Mais le tri est toujours une bonne pratique
# # # # # # # # # # # # #         group_copy = group_copy.sort_index()


# # # # # # # # # # # # #         # Calcul de lever/coucher du soleil ou fallback
# # # # # # # # # # # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # # # # # # # # # # #             lat = gps_data['Lat']
# # # # # # # # # # # # #             long = gps_data['Long']
# # # # # # # # # # # # #             timezone_str = gps_data['Timezone']

# # # # # # # # # # # # #             try:
# # # # # # # # # # # # #                 tz = pytz.timezone(timezone_str)
# # # # # # # # # # # # #                 # S'assurer que l'index du groupe est dans le bon fuseau horaire
# # # # # # # # # # # # #                 if group_copy.index.tz is None:
# # # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_localize(tz, ambiguous='NaT', nonexistent='NaT')
# # # # # # # # # # # # #                 else:
# # # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_convert(tz)

# # # # # # # # # # # # #                 daily_sun_info = {}
# # # # # # # # # # # # #                 for date_only in group_copy.index.normalize().unique():
# # # # # # # # # # # # #                     s = sun.sun(LocationInfo(station_name, "Site", timezone_str, lat, long).observer,
# # # # # # # # # # # # #                                 date=date_only, tzinfo=tz)
# # # # # # # # # # # # #                     daily_sun_info[date_only.date()] = {
# # # # # # # # # # # # #                         'sunrise': s['sunrise'],
# # # # # # # # # # # # #                         'sunset': s['sunset']
# # # # # # # # # # # # #                     }

# # # # # # # # # # # # #                 group_copy.loc[:, 'sunrise_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunrise'))
# # # # # # # # # # # # #                 group_copy.loc[:, 'sunset_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunset'))

# # # # # # # # # # # # #                 # Assurer que les colonnes sunrise/sunset sont dans le même fuseau horaire que l'index pour la comparaison
# # # # # # # # # # # # #                 if group_copy.index.tz is not None:
# # # # # # # # # # # # #                      group_copy['sunrise_time_local'] = group_copy['sunrise_time_local'].dt.tz_convert(group_copy.index.tz)
# # # # # # # # # # # # #                      group_copy['sunset_time_local'] = group_copy['sunset_time_local'].dt.tz_convert(group_copy.index.tz)

# # # # # # # # # # # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index >= group_copy['sunrise_time_local']) & \
# # # # # # # # # # # # #                                                   (group_copy.index < group_copy['sunset_time_local'])

# # # # # # # # # # # # #                 daylight_timedelta = group_copy['sunset_time_local'] - group_copy['sunrise_time_local']
# # # # # # # # # # # # #                 group_copy.loc[:, 'Daylight_Duration'] = daylight_timedelta.apply(
# # # # # # # # # # # # #                     lambda td: (pd.to_datetime('00:00:00') + td).strftime('%H:%M:%S') if pd.notna(td) else np.nan
# # # # # # # # # # # # #                 )
# # # # # # # # # # # # #             except Exception as e:
# # # # # # # # # # # # #                 warnings.warn(f"Erreur Astral pour {station_name}: {e}. Utilisation du fallback 7h-18h.")
# # # # # # # # # # # # #                 if group_copy.index.tz is None:
# # # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_localize('UTC') # Fallback pour localiser l'index
# # # # # # # # # # # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # # # # # # # # # # #                 group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # # # # # # # # # # #         else:
# # # # # # # # # # # # #             warnings.warn(f"Coordonnées/Fuseau horaire manquants/invalides pour {station_name}. Utilisation du fallback 7h-18h.")
# # # # # # # # # # # # #             if group_copy.index.tz is None:
# # # # # # # # # # # # #                 group_copy.index = group_copy.index.tz_localize('UTC')
# # # # # # # # # # # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # # # # # # # # # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"

# # # # # # # # # # # # #         df_processed_parts.append(group_copy)

# # # # # # # # # # # # #     if not df_processed_parts:
# # # # # # # # # # # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # # # # # # # # # # #     df_final = pd.concat(df_processed_parts)
# # # # # # # # # # # # #     df_final = df_final.sort_index()
# # # # # # # # # # # # #     df_final.index.name = 'Datetime'

# # # # # # # # # # # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date',
# # # # # # # # # # # # #                                   'sunrise_time_local', 'sunset_time_local']
# # # # # # # # # # # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # # # # # # # # # # #     # Gestion intelligente de Rain_mm
# # # # # # # # # # # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # # # # # # # # # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # # # # # # # # # # #             df_final = create_rain_mm(df_final)
# # # # # # # # # # # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # # # # # # # # # # #         else:
# # # # # # # # # # # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # # # # # # # # # # #             df_final['Rain_mm'] = np.nan


# # # # # # # # # # # # #     # Interpolation standard et bornage pour les variables numériques
# # # # # # # # # # # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # # # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # # # # #                      'Wind_Dir_Deg']

# # # # # # # # # # # # #     for var in standard_vars:
# # # # # # # # # # # # #         if var in df_final.columns:
# # # # # # # # # # # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # # # # # # # # # # #             if var in limits:
# # # # # # # # # # # # #                 min_val = limits[var]['min']
# # # # # # # # # # # # #                 max_val = limits[var]['max']
# # # # # # # # # # # # #                 initial_nan_count = df_final[var].isna().sum()
# # # # # # # # # # # # #                 df_final.loc[(df_final[var] < min_val) | (df_final[var] > max_val), var] = np.nan
# # # # # # # # # # # # #                 if df_final[var].isna().sum() > initial_nan_count:
# # # # # # # # # # # # #                     warnings.warn(f"Remplacement de {df_final[var].isna().sum() - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # # # # # # # # # # #             # Interpolation
# # # # # # # # # # # # #             df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # # # # # # # # # # #             df_final[var] = df_final[var].bfill().ffill() # Pour les NaNs aux extrémités

# # # # # # # # # # # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # # # # # # # # # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # # # # # # # # # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # # # # # # # # # # #         if 'Solar_R_W/m^2' in limits:
# # # # # # # # # # # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # # # # # # # # # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # # # # # # # # # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # # # # # # # # # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # # # # # # # # # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # # # # # # # # # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # # # # # # # # # # #         if 'Is_Daylight' in df_final.columns:
# # # # # # # # # # # # #             # Radiation positive hors du jour -> 0
# # # # # # # # # # # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # # # # # # # # # # #             # Zéros suspects en journée sans pluie -> NaN
# # # # # # # # # # # # #             if 'Rain_mm' in df_final.columns:
# # # # # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # # # # # # # # # # #             else:
# # # # # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # # # # # # # # # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # # # # # # # # # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # # # # # # # # # # #             # Interpolation dans les périodes de jour
# # # # # # # # # # # # #             is_day = df_final['Is_Daylight']
# # # # # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # # # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # # # # # # # # # # #             # Les NaNs restants hors des heures de jour -> 0
# # # # # # # # # # # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # # # # # # # # # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # # # # # # # # # # #         else:
# # # # # # # # # # # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # # # # # # # # # # #             df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()

# # # # # # # # # # # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # # # # # # # # # # #     missing_after_interp = df_final.isna().sum()
# # # # # # # # # # # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # # # # # # # # # # #     if not columns_with_missing.empty:
# # # # # # # # # # # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # # # # # # # # # # #     else:
# # # # # # # # # # # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # # # # # # # # # # #     return df_final

# # # # # # # # # # # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     Traite les valeurs aberrantes (outliers) dans un DataFrame de données météorologiques
# # # # # # # # # # # # #     en appliquant la méthode de l'écart interquartile (IQR) pour limiter les valeurs extrêmes.

# # # # # # # # # # # # #     Args:
# # # # # # # # # # # # #         df (pd.DataFrame): DataFrame contenant les données météorologiques à traiter.
# # # # # # # # # # # # #         limits (dict): Dictionnaire des variables à traiter (clés) avec leurs limites potentielles.
# # # # # # # # # # # # #                        (Note: les limites min/max du dict ne sont pas utilisées pour l'IQR,
# # # # # # # # # # # # #                        mais les clés du dict définissent les variables à analyser).

# # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # #         pd.DataFrame: Copie du DataFrame avec les outliers corrigés par la méthode IQR.
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # # #     for var in limits.keys(): # On utilise les clés de limits pour savoir quelles variables traiter
# # # # # # # # # # # # #         if var in df_copy.columns:
# # # # # # # # # # # # #             # S'assurer que la colonne est numérique
# # # # # # # # # # # # #             df_copy[var] = pd.to_numeric(df_copy[var], errors='coerce')

# # # # # # # # # # # # #             # Calcul des quartiles
# # # # # # # # # # # # #             Q1 = df_copy[var].quantile(0.25)
# # # # # # # # # # # # #             Q3 = df_copy[var].quantile(0.75)
# # # # # # # # # # # # #             IQR = Q3 - Q1

# # # # # # # # # # # # #             # Bornes IQR
# # # # # # # # # # # # #             borne_inf = Q1 - 1.5 * IQR
# # # # # # # # # # # # #             borne_sup = Q3 + 1.5 * IQR

# # # # # # # # # # # # #             # Application du bornage (clipping)
# # # # # # # # # # # # #             initial_outliers_count = df_copy[(df_copy[var] < borne_inf) | (df_copy[var] > borne_sup)][var].count()
# # # # # # # # # # # # #             df_copy[var] = df_copy[var].clip(lower=borne_inf, upper=borne_sup)
# # # # # # # # # # # # #             if initial_outliers_count > 0:
# # # # # # # # # # # # #                 warnings.warn(f"Outliers traités par IQR pour '{var}'. {initial_outliers_count} valeurs ajustées.")
# # # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables météorologiques.

# # # # # # # # # # # # #     Args:
# # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec 'Datetime' comme index.

# # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # #         pd.DataFrame: DataFrame contenant les statistiques journalières.
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # #         raise TypeError("L'index du DataFrame doit être de type DatetimeIndex.")

# # # # # # # # # # # # #     # Définir les variables pour lesquelles nous voulons des statistiques
# # # # # # # # # # # # #     # et leurs méthodes d'agrégation.
# # # # # # # # # # # # #     # Rain_mm doit être sommée, les autres moyennées.
# # # # # # # # # # # # #     # Wind_Dir_Deg est un cas particulier, souvent on calcule une direction moyenne vectorielle
# # # # # # # # # # # # #     # mais pour des stats simples, une moyenne arithmétique peut être misleading.
# # # # # # # # # # # # #     # Pour cet exemple, on le met en moyenne, mais gardez cela à l'esprit.
# # # # # # # # # # # # #     agg_funcs = {
# # # # # # # # # # # # #         'Air_Temp_Deg_C': 'mean',
# # # # # # # # # # # # #         'Rel_H_%': 'mean',
# # # # # # # # # # # # #         'BP_mbar_Avg': 'mean',
# # # # # # # # # # # # #         'Rain_mm': 'sum', # Précipitation cumulée
# # # # # # # # # # # # #         'Wind_Sp_m/sec': 'mean',
# # # # # # # # # # # # #         'Solar_R_W/m^2': 'mean',
# # # # # # # # # # # # #         'Wind_Dir_Deg': 'mean' # Moyenne simple, à interpréter avec prudence
# # # # # # # # # # # # #     }

# # # # # # # # # # # # #     # Filtrer les colonnes qui existent dans le DataFrame
# # # # # # # # # # # # #     existing_cols_to_agg = {col: func for col, func in agg_funcs.items() if col in df.columns}

# # # # # # # # # # # # #     if not existing_cols_to_agg:
# # # # # # # # # # # # #         warnings.warn("Aucune colonne pertinente trouvée pour les statistiques journalières.")
# # # # # # # # # # # # #         return pd.DataFrame()

# # # # # # # # # # # # #     # Regrouper par jour (en utilisant l'index Datetime) et par station, puis agréger
# # # # # # # # # # # # #     daily_summary = df.groupby([df.index.date, 'Station']).agg(
# # # # # # # # # # # # #         **{f"{col}_{func}": (col, func) for col, func in existing_cols_to_agg.items()}
# # # # # # # # # # # # #     )
# # # # # # # # # # # # #     daily_summary.index.names = ['Date', 'Station']
# # # # # # # # # # # # #     return daily_summary.reset_index()


# # # # # # # # # # # # # # --- Fonctions de Visualisation ---

# # # # # # # # # # # # # # Fonction utilitaire pour convertir une figure Matplotlib en base64
# # # # # # # # # # # # # def fig_to_base64(fig: plt.Figure) -> str | None:
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     Convertit une figure Matplotlib en une chaîne Base64 encodée (format PNG).

# # # # # # # # # # # # #     Args:
# # # # # # # # # # # # #         fig (plt.Figure): La figure Matplotlib à convertir.

# # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG, ou None si la figure est invalide.
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     if fig is None:
# # # # # # # # # # # # #         return None
# # # # # # # # # # # # #     buf = io.BytesIO()
# # # # # # # # # # # # #     try:
# # # # # # # # # # # # #         fig.savefig(buf, format='png', bbox_inches='tight')
# # # # # # # # # # # # #         plt.close(fig) # Fermer la figure pour libérer la mémoire
# # # # # # # # # # # # #         data = base64.b64encode(buf.getbuffer()).decode("ascii")
# # # # # # # # # # # # #         return data
# # # # # # # # # # # # #     except Exception as e:
# # # # # # # # # # # # #         warnings.warn(f"Erreur lors de la conversion de la figure en base64: {e}")
# # # # # # # # # # # # #         plt.close(fig) # Assurez-vous de fermer la figure même en cas d'erreur
# # # # # # # # # # # # #         return None

# # # # # # # # # # # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station_name: str, variable_name: str, periode: str, custom_palette: list = None, metadata_variables: dict = None) -> str | None:
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     Génère un seul graphique de l'évolution d'une variable météorologique pour une station
# # # # # # # # # # # # #     spécifique et une période donnée.

# # # # # # # # # # # # #     Args:
# # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée contenant les données météorologiques.
# # # # # # # # # # # # #                            Doit inclure 'Datetime' comme index et 'Station'.
# # # # # # # # # # # # #         station_name (str): Nom de la station à visualiser.
# # # # # # # # # # # # #         variable_name (str): Nom de la variable à visualiser.
# # # # # # # # # # # # #         periode (str): Période d'agrégation ('Journalière', 'Hebdomadaire', 'Mensuelle', 'Annuelle').
# # # # # # # # # # # # #         custom_palette (list, optional): Palette de couleurs personnalisée.
# # # # # # # # # # # # #         metadata_variables (dict, optional): Dictionnaire des métadonnées des variables.
# # # # # # # # # # # # #                                             Si None, un avertissement sera émis.

# # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     if metadata_variables is None:
# # # # # # # # # # # # #         warnings.warn("metadata_variables n'a pas été fourni à generer_graphique_par_variable_et_periode. Certaines informations pourraient être manquantes.")
# # # # # # # # # # # # #         # Fallback si METADATA_VARIABLES n'est pas passé (moins robuste)
# # # # # # # # # # # # #         metadata_variables = {
# # # # # # # # # # # # #             'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'agg_type': 'cumul'},
# # # # # # # # # # # # #             'Air_Temp_Deg_C': {'Nom': "Température ", 'Unite': "°C", 'agg_type': 'moyenne'},
# # # # # # # # # # # # #             'Rel_H_%': {'Nom': "Humidité Relative", 'Unite': "%", 'agg_type': 'moyenne'},
# # # # # # # # # # # # #             'Solar_R_W/m^2': {'Nom': "Radiation Solaire", 'Unite': "W/m²", 'agg_type': 'moyenne'},
# # # # # # # # # # # # #             'Wind_Sp_m/sec': {'Nom': "Vitesse du Vent", 'Unite': "m/s", 'agg_type': 'moyenne'},
# # # # # # # # # # # # #             'Wind_Dir_Deg': {'Nom': "Direction du Vent", 'Unite': "°", 'agg_type': 'moyenne'},
# # # # # # # # # # # # #             'BP_mbar_Avg': {'Nom': "Pression Atmospherique moyenne", 'Unite': "mbar", 'agg_type': 'moyenne'}
# # # # # # # # # # # # #         }


# # # # # # # # # # # # #     if variable_name not in metadata_variables or variable_name not in df.columns:
# # # # # # # # # # # # #         warnings.warn(f"Variable '{variable_name}' non valide ou absente du DataFrame.")
# # # # # # # # # # # # #         return None

# # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # # #         return None

# # # # # # # # # # # # #     df_station = df[df['Station'] == station_name].copy()
# # # # # # # # # # # # #     if df_station.empty:
# # # # # # # # # # # # #         warnings.warn(f"Aucune donnée pour la station '{station_name}'.")
# # # # # # # # # # # # #         return None

# # # # # # # # # # # # #     df_station[variable_name] = pd.to_numeric(df_station[variable_name], errors='coerce')

# # # # # # # # # # # # #     annees_uniques = df_station.index.year.nunique()
# # # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # # #         }
# # # # # # # # # # # # #     else:
# # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # # #         }

# # # # # # # # # # # # #     if periode not in freq_configs:
# # # # # # # # # # # # #         warnings.warn(f"Période '{periode}' non valide.")
# # # # # # # # # # # # #         return None

# # # # # # # # # # # # #     freq, date_fmt, rot, locator, label_x = freq_configs[periode]
# # # # # # # # # # # # #     meta = metadata_variables[variable_name]
# # # # # # # # # # # # #     nom_complet = f"{meta['Nom']} ({meta['Unite']})"
# # # # # # # # # # # # #     agg_func = 'sum' if meta['agg_type'] == 'cumul' else 'mean'

# # # # # # # # # # # # #     # Déterminer la couleur de la station
# # # # # # # # # # # # #     stations_list = sorted(df['Station'].unique())
# # # # # # # # # # # # #     try:
# # # # # # # # # # # # #         station_idx = stations_list.index(station_name)
# # # # # # # # # # # # #     except ValueError:
# # # # # # # # # # # # #         warnings.warn(f"Station '{station_name}' non trouvée dans la liste des stations uniques.")
# # # # # # # # # # # # #         station_idx = 0 # Fallback
    
# # # # # # # # # # # # #     palette = custom_palette if custom_palette and len(custom_palette) > station_idx else sns.color_palette("husl", len(stations_list))
# # # # # # # # # # # # #     station_color = palette[station_idx] if len(palette) > station_idx else 'blue' # Fallback couleur

# # # # # # # # # # # # #     fig, ax = plt.subplots(figsize=(12, 7))

# # # # # # # # # # # # #     df_temp = df_station.copy()

# # # # # # # # # # # # #     if variable_name == 'Solar_R_W/m^2':
# # # # # # # # # # # # #         if 'Is_Daylight' in df_temp.columns:
# # # # # # # # # # # # #             df_temp = df_temp[df_temp['Is_Daylight']].copy()
# # # # # # # # # # # # #         else:
# # # # # # # # # # # # #             warnings.warn("Avertissement: Colonne 'Is_Daylight' manquante pour la radiation solaire. Filtrage horaire non appliqué.")
# # # # # # # # # # # # #             df_temp = df_temp[(df_temp.index.hour >= 7) & (df_temp.index.hour <= 18)]
            
# # # # # # # # # # # # #     elif variable_name == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in df_temp.columns:
# # # # # # # # # # # # #         df_temp = df_temp[(df_temp['Wind_Sp_m/sec'] > 0) & (df_temp['Wind_Dir_Deg'].notna())]

# # # # # # # # # # # # #     with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # # #         df_agg = df_temp.groupby(pd.Grouper(freq=freq))[variable_name].agg(agg_func).reset_index()
# # # # # # # # # # # # #     df_agg.rename(columns={'index': 'Datetime'}, inplace=True)

# # # # # # # # # # # # #     if df_agg.empty:
# # # # # # # # # # # # #         plt.close(fig)
# # # # # # # # # # # # #         warnings.warn(f"Aucune donnée agrégée pour {variable_name} à la période {periode} pour {station_name}.")
# # # # # # # # # # # # #         return None

# # # # # # # # # # # # #     sns.lineplot(
# # # # # # # # # # # # #         data=df_agg,
# # # # # # # # # # # # #         x='Datetime',
# # # # # # # # # # # # #         y=variable_name,
# # # # # # # # # # # # #         color=station_color,
# # # # # # # # # # # # #         linewidth=2,
# # # # # # # # # # # # #         marker='o' if periode == 'Annuelle' else None,
# # # # # # # # # # # # #         markersize=6 if periode == 'Annuelle' else 0,
# # # # # # # # # # # # #         ax=ax
# # # # # # # # # # # # #     )

# # # # # # # # # # # # #     ax.set_title(f"Évolution {periode.lower()} de {nom_complet} pour {station_name}", pad=10, fontsize=14)
# # # # # # # # # # # # #     ax.set_ylabel(nom_complet, fontsize=12)
# # # # # # # # # # # # #     ax.set_xlabel(label_x, fontsize=10)

# # # # # # # # # # # # #     ax.xaxis.set_major_formatter(date_fmt)
# # # # # # # # # # # # #     if locator:
# # # # # # # # # # # # #         ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # # #     for label in ax.get_xticklabels():
# # # # # # # # # # # # #         label.set_rotation(rot)
# # # # # # # # # # # # #         label.set_ha('right' if rot > 45 else 'center')

# # # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # # # def generer_graphique_comparatif(df: pd.DataFrame, variable_name: str, periode: str, custom_palette: list = None, metadata_variables: dict = None) -> str | None:
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     Génère un seul graphique comparatif de l'évolution d'une variable météorologique
# # # # # # # # # # # # #     entre toutes les stations pour une période donnée.

# # # # # # # # # # # # #     Args:
# # # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée contenant les données météorologiques.
# # # # # # # # # # # # #                            Doit inclure 'Datetime' comme index et 'Station'.
# # # # # # # # # # # # #         variable_name (str): Nom de la variable à visualiser.
# # # # # # # # # # # # #         periode (str): Période d'agrégation ('Journalière', 'Hebdomadaire', 'Mensuelle', 'Annuelle').
# # # # # # # # # # # # #         custom_palette (list, optional): Palette de couleurs personnalisée.
# # # # # # # # # # # # #         metadata_variables (dict, optional): Dictionnaire des métadonnées des variables.
# # # # # # # # # # # # #                                             Si None, un avertissement sera émis.

# # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     if metadata_variables is None:
# # # # # # # # # # # # #         warnings.warn("metadata_variables n'a pas été fourni à generer_graphique_comparatif. Certaines informations pourraient être manquantes.")
# # # # # # # # # # # # #         # Fallback si METADATA_VARIABLES n'est pas passé (moins robuste)
# # # # # # # # # # # # #         metadata_variables = {
# # # # # # # # # # # # #             'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'is_rain': True},
# # # # # # # # # # # # #             'Air_Temp_Deg_C': {'Nom': "Température", 'Unite': "°C", 'is_rain': False},
# # # # # # # # # # # # #             'Rel_H_%': {'Nom': "Humidité", 'Unite': "%", 'is_rain': False},
# # # # # # # # # # # # #             'Solar_R_W/m^2': {'Nom': "Radiation solaire", 'Unite': "W/m²", 'is_rain': False},
# # # # # # # # # # # # #             'Wind_Sp_m/sec': {'Nom': "Vitesse du vent", 'Unite': "m/s", 'is_rain': False},
# # # # # # # # # # # # #             'Wind_Dir_Deg': {'Nom': "Direction du vent", 'Unite': "°", 'is_rain': False},
# # # # # # # # # # # # #             'BP_mbar_Avg': {'Nom': "Pression atmospherique moyenne", 'Unite': "mbar", 'is_rain': False}
# # # # # # # # # # # # #         }


# # # # # # # # # # # # #     if variable_name not in metadata_variables or variable_name not in df.columns:
# # # # # # # # # # # # #         warnings.warn(f"Variable '{variable_name}' non valide ou absente du DataFrame.")
# # # # # # # # # # # # #         return None

# # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # # #         return None

# # # # # # # # # # # # #     annees_uniques = df.index.year.nunique()
# # # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # # #         }
# # # # # # # # # # # # #     else:
# # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # # #         }

# # # # # # # # # # # # #     if periode not in freq_configs:
# # # # # # # # # # # # #         warnings.warn(f"Période '{periode}' non valide.")
# # # # # # # # # # # # #         return None

# # # # # # # # # # # # #     freq, date_fmt, rot, locator, label_x = freq_configs[periode]
# # # # # # # # # # # # #     meta = metadata_variables[variable_name]
# # # # # # # # # # # # #     nom_complet = f"{meta['Nom']} ({meta['Unite']})"
# # # # # # # # # # # # #     agg_text = " (cumul)" if meta['is_rain'] else " (moyenne)"
# # # # # # # # # # # # #     agg_func = 'sum' if meta['is_rain'] else 'mean'

# # # # # # # # # # # # #     stations = sorted(df['Station'].unique())
# # # # # # # # # # # # #     palette = custom_palette if custom_palette and len(custom_palette) >= len(stations) \
# # # # # # # # # # # # #               else sns.color_palette("husl", len(stations))

# # # # # # # # # # # # #     fig, ax = plt.subplots(figsize=(14, 8))

# # # # # # # # # # # # #     temp_df = df.copy()

# # # # # # # # # # # # #     if variable_name == 'Solar_R_W/m^2':
# # # # # # # # # # # # #         if 'Is_Daylight' in temp_df.columns:
# # # # # # # # # # # # #             temp_df = temp_df[temp_df['Is_Daylight']].copy()
# # # # # # # # # # # # #         else:
# # # # # # # # # # # # #             warnings.warn("Avertissement: Colonne 'Is_Daylight' manquante. Filtrage horaire par défaut (7h-18h) pour radiation solaire.")
# # # # # # # # # # # # #             temp_df = temp_df[(temp_df.index.hour >= 7) & (temp_df.index.hour <= 18)]
            
# # # # # # # # # # # # #     elif variable_name == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in temp_df.columns:
# # # # # # # # # # # # #         temp_df = temp_df[(temp_df['Wind_Sp_m/sec'] > 0) & (temp_df['Wind_Dir_Deg'].notna())]

# # # # # # # # # # # # #     with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # # #         df_agg = temp_df.groupby(['Station', pd.Grouper(freq=freq)])[variable_name].agg(agg_func).reset_index()

# # # # # # # # # # # # #     if df_agg.empty:
# # # # # # # # # # # # #         plt.close(fig)
# # # # # # # # # # # # #         warnings.warn(f"Aucune donnée agrégée pour {variable_name} à la période {periode}.")
# # # # # # # # # # # # #         return None

# # # # # # # # # # # # #     sns.lineplot(
# # # # # # # # # # # # #         data=df_agg,
# # # # # # # # # # # # #         x='Datetime',
# # # # # # # # # # # # #         y=variable_name,
# # # # # # # # # # # # #         hue='Station',
# # # # # # # # # # # # #         palette=palette,
# # # # # # # # # # # # #         ax=ax,
# # # # # # # # # # # # #         linewidth=2,
# # # # # # # # # # # # #         marker='o' if periode == 'Annuelle' else None,
# # # # # # # # # # # # #         markersize=6 if periode == 'Annuelle' else 0,
# # # # # # # # # # # # #         legend='full'
# # # # # # # # # # # # #     )

# # # # # # # # # # # # #     ax.set_title(
# # # # # # # # # # # # #         f"Comparaison de {meta['Nom']} {agg_text} ({meta['Unite']}) - Période: {periode}",
# # # # # # # # # # # # #         fontsize=16,
# # # # # # # # # # # # #         pad=15,
# # # # # # # # # # # # #         weight='bold'
# # # # # # # # # # # # #     )
# # # # # # # # # # # # #     ax.set_xlabel(label_x, fontsize=12, labelpad=15)
# # # # # # # # # # # # #     ax.set_ylabel(nom_complet, fontsize=12)

# # # # # # # # # # # # #     ax.xaxis.set_major_formatter(date_fmt)
# # # # # # # # # # # # #     if locator:
# # # # # # # # # # # # #         ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # # #     for label in ax.get_xticklabels():
# # # # # # # # # # # # #         label.set_rotation(rot)
# # # # # # # # # # # # #         label.set_ha('right' if rot < 90 else 'center')
# # # # # # # # # # # # #         label.set_fontsize(10)

# # # # # # # # # # # # #     ax.legend(
# # # # # # # # # # # # #         title='Stations',
# # # # # # # # # # # # #         loc='upper left',
# # # # # # # # # # # # #         fontsize=10,
# # # # # # # # # # # # #         title_fontsize=12,
# # # # # # # # # # # # #         framealpha=0.9
# # # # # # # # # # # # #     )
# # # # # # # # # # # # #     ax.grid(True, alpha=0.2)

# # # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # # #     plt.subplots_adjust(
# # # # # # # # # # # # #         top=0.93,
# # # # # # # # # # # # #         hspace=0.3,
# # # # # # # # # # # # #         wspace=0.15
# # # # # # # # # # # # #     )
# # # # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station_name: str, variable_palette: list = None, metadata_variables: dict = None) -> str | None:
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     Génère un graphique comparatif normalisé de l'évolution de plusieurs variables météorologiques
# # # # # # # # # # # # #     pour une seule station sélectionnée, à différentes échelles temporelles.

# # # # # # # # # # # # #     Args:
# # # # # # # # # # # # #         df (pd.DataFrame): DataFrame contenant les données météorologiques. Doit avoir 'Datetime' comme index.
# # # # # # # # # # # # #         station_name (str): Le nom de la station pour laquelle générer le graphique.
# # # # # # # # # # # # #         variable_palette (list, optional): Liste personnalisée de couleurs pour chaque variable.
# # # # # # # # # # # # #         metadata_variables (dict, optional): Dictionnaire des métadonnées des variables.
# # # # # # # # # # # # #                                             Si None, un avertissement sera émis.

# # # # # # # # # # # # #     Returns:
# # # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # # #     """
# # # # # # # # # # # # #     if metadata_variables is None:
# # # # # # # # # # # # #         warnings.warn("metadata_variables n'a pas été fourni à generate_multi_variable_station_plot. Certaines informations pourraient être manquantes.")
# # # # # # # # # # # # #         # Fallback si METADATA_VARIABLES n'est pas passé (moins robuste)
# # # # # # # # # # # # #         metadata_variables = {
# # # # # # # # # # # # #             'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'agg_type': 'cumul'},
# # # # # # # # # # # # #             'Air_Temp_Deg_C': {'Nom': "Température ", 'Unite': "°C", 'agg_type': 'moyenne'},
# # # # # # # # # # # # #             'Rel_H_%': {'Nom': "Humidité Relative", 'Unite': "%", 'agg_type': 'moyenne'},
# # # # # # # # # # # # #             'Solar_R_W/m^2': {'Nom': "Radiation Solaire", 'Unite': "W/m²", 'agg_type': 'moyenne'},
# # # # # # # # # # # # #             'Wind_Sp_m/sec': {'Nom': "Vitesse du Vent", 'Unite': "m/s", 'agg_type': 'moyenne'},
# # # # # # # # # # # # #             'Wind_Dir_Deg': {'Nom': "Direction du Vent", 'Unite': "°", 'agg_type': 'moyenne'},
# # # # # # # # # # # # #             'BP_mbar_Avg': {'Nom': "Pression Atmospherique moyenne", 'Unite': "mbar", 'agg_type': 'moyenne'}
# # # # # # # # # # # # #         }


# # # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # # #         return None

# # # # # # # # # # # # #     df_station = df[df['Station'] == station_name].copy()

# # # # # # # # # # # # #     if df_station.empty:
# # # # # # # # # # # # #         warnings.warn(f"Station {station_name} sans données - ignorée.")
# # # # # # # # # # # # #         return None

# # # # # # # # # # # # #     annees_uniques = df_station.index.year.nunique()
# # # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # # #         }
# # # # # # # # # # # # #     else:
# # # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # # #         }

# # # # # # # # # # # # #     nrows, ncols = (2, 2) if len(freq_configs) >= 2 else (1, 1)

# # # # # # # # # # # # #     variables_to_plot = [var for var in metadata_variables.keys() if var in df_station.columns]

# # # # # # # # # # # # #     if not variables_to_plot:
# # # # # # # # # # # # #         warnings.warn("Aucune variable valide à tracer pour cette station.")
# # # # # # # # # # # # #         return None

# # # # # # # # # # # # #     num_vars_to_plot = len(variables_to_plot)
# # # # # # # # # # # # #     if (variable_palette is not None and
# # # # # # # # # # # # #         isinstance(variable_palette, list) and
# # # # # # # # # # # # #         len(variable_palette) >= num_vars_to_plot):
# # # # # # # # # # # # #         plot_palette = variable_palette
# # # # # # # # # # # # #     else:
# # # # # # # # # # # # #         plot_palette = sns.color_palette("tab10", num_vars_to_plot)
# # # # # # # # # # # # #         if variable_palette is not None:
# # # # # # # # # # # # #             warnings.warn(f"Avertissement: Palette fournie insuffisante ou invalide - utilisation de la palette par défaut pour {station_name}.")

# # # # # # # # # # # # #     fig, axs = plt.subplots(nrows, ncols, figsize=(28, 22))
# # # # # # # # # # # # #     axs = axs.flatten()

# # # # # # # # # # # # #     fig.suptitle(
# # # # # # # # # # # # #         f'Évolution Normalisée des Variables Météorologiques - Station: {station_name}',
# # # # # # # # # # # # #         fontsize=24,
# # # # # # # # # # # # #         weight='bold',
# # # # # # # # # # # # #         y=1.02
# # # # # # # # # # # # #     )

# # # # # # # # # # # # #     for i, (periode, (freq, date_format, rot, locator, label_x)) in enumerate(freq_configs.items()):
# # # # # # # # # # # # #         if i >= len(axs):
# # # # # # # # # # # # #             continue

# # # # # # # # # # # # #         ax = axs[i]
# # # # # # # # # # # # #         agg_dict = {}
# # # # # # # # # # # # #         for var in variables_to_plot:
# # # # # # # # # # # # #             if var in df_station.columns:
# # # # # # # # # # # # #                 agg_type = metadata_variables[var].get('agg_type', 'moyenne')
# # # # # # # # # # # # #                 agg_dict[var] = 'sum' if agg_type == 'cumul' else 'mean'

# # # # # # # # # # # # #         if not agg_dict:
# # # # # # # # # # # # #             warnings.warn(f"Aucune donnée à agréger pour {station_name} - période {periode}.")
# # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # #             continue

# # # # # # # # # # # # #         with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # # #             for col in agg_dict.keys():
# # # # # # # # # # # # #                 if col in df_station.columns:
# # # # # # # # # # # # #                     df_station[col] = pd.to_numeric(df_station[col], errors='coerce')

# # # # # # # # # # # # #             cols_to_agg = list(agg_dict.keys())
# # # # # # # # # # # # #             if not cols_to_agg:
# # # # # # # # # # # # #                 warnings.warn(f"Aucune colonne numérique valide pour {station_name} - période {periode}.")
# # # # # # # # # # # # #                 if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # #                 continue

# # # # # # # # # # # # #             df_agg = pd.DataFrame(index=df_station.index)

# # # # # # # # # # # # #             for var in cols_to_agg:
# # # # # # # # # # # # #                 temp_data = df_station[var]
# # # # # # # # # # # # #                 if var == 'Solar_R_W/m^2':
# # # # # # # # # # # # #                     if 'Is_Daylight' in df_station.columns:
# # # # # # # # # # # # #                         temp_data = df_station.loc[df_station['Is_Daylight'], var]
# # # # # # # # # # # # #                     else:
# # # # # # # # # # # # #                         temp_data = df_station.loc[(df_station.index.hour >= 7) & (df_station.index.hour <= 18), var]
# # # # # # # # # # # # #                         warnings.warn(f"Avertissement: 'Is_Daylight' non trouvé. Radiation solaire filtrée par heures fixes (7h-18h).")
# # # # # # # # # # # # #                 elif var == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in df_station.columns:
# # # # # # # # # # # # #                     temp_data = df_station.loc[(df_station['Wind_Sp_m/sec'] > 0) & (df_station['Wind_Dir_Deg'].notna()), var]

# # # # # # # # # # # # #                 if not temp_data.empty:
# # # # # # # # # # # # #                     df_agg[var] = temp_data.resample(freq).agg(agg_dict[var])
# # # # # # # # # # # # #                 else:
# # # # # # # # # # # # #                     df_agg[var] = np.nan

# # # # # # # # # # # # #             df_agg = df_agg.dropna(how='all').reset_index()


# # # # # # # # # # # # #         scaler = MinMaxScaler()
# # # # # # # # # # # # #         vars_to_scale = list(agg_dict.keys())
# # # # # # # # # # # # #         numeric_cols = [col for col in vars_to_scale if col in df_agg.columns]

# # # # # # # # # # # # #         if not numeric_cols or df_agg.empty:
# # # # # # # # # # # # #             warnings.warn(f"Données manquantes après agrégation ou colonnes numériques absentes pour {station_name} - période {periode}.")
# # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # #             continue

# # # # # # # # # # # # #         df_agg_scaled = df_agg.copy()
# # # # # # # # # # # # #         valid_cols_for_scaling = [col for col in numeric_cols if df_agg_scaled[col].notna().any()]
# # # # # # # # # # # # #         if valid_cols_for_scaling:
# # # # # # # # # # # # #             df_agg_scaled[valid_cols_for_scaling] = scaler.fit_transform(df_agg_scaled[valid_cols_for_scaling])
# # # # # # # # # # # # #         else:
# # # # # # # # # # # # #             warnings.warn(f"Aucune colonne avec des valeurs valides à normaliser pour {station_name} - période {periode}.")
# # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # #             continue


# # # # # # # # # # # # #         df_melted = df_agg_scaled.melt(
# # # # # # # # # # # # #             id_vars=['Datetime'],
# # # # # # # # # # # # #             value_vars=numeric_cols,
# # # # # # # # # # # # #             var_name='Variable',
# # # # # # # # # # # # #             value_name='Valeur Normalisée'
# # # # # # # # # # # # #         )

# # # # # # # # # # # # #         if df_melted.empty or df_melted['Valeur Normalisée'].dropna().empty:
# # # # # # # # # # # # #             warnings.warn(f"Données normalisées vides ou toutes NaN pour {station_name} - période {periode}.")
# # # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # # #             continue

# # # # # # # # # # # # #         variable_color_map = {var: plot_palette[j] for j, var in enumerate(variables_to_plot)}

# # # # # # # # # # # # #         sns.lineplot(
# # # # # # # # # # # # #             data=df_melted,
# # # # # # # # # # # # #             x='Datetime',
# # # # # # # # # # # # #             y='Valeur Normalisée',
# # # # # # # # # # # # #             hue='Variable',
# # # # # # # # # # # # #             palette=variable_color_map,
# # # # # # # # # # # # #             ax=ax,
# # # # # # # # # # # # #             linewidth=2,
# # # # # # # # # # # # #             marker='o' if periode == 'Annuelle' or len(df_melted['Datetime'].unique()) < 50 else None,
# # # # # # # # # # # # #             markersize=6,
# # # # # # # # # # # # #             legend='full'
# # # # # # # # # # # # #         )

# # # # # # # # # # # # #         ax.set_title(f"Évolution {periode.lower()}", fontsize=18, weight='bold', pad=15)
# # # # # # # # # # # # #         ax.set_xlabel(label_x, fontsize=14, labelpad=15)
# # # # # # # # # # # # #         ax.set_ylabel("Valeur Normalisée (0-1)", fontsize=14)

# # # # # # # # # # # # #         ax.xaxis.set_major_formatter(date_format)
# # # # # # # # # # # # #         if locator is not None:
# # # # # # # # # # # # #             ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # # #         for label in ax.get_xticklabels():
# # # # # # # # # # # # #             label.set_rotation(rot)
# # # # # # # # # # # # #             label.set_ha('right' if rot > 0 and rot < 90 else 'center')
# # # # # # # # # # # # #             label.set_fontsize(12)

# # # # # # # # # # # # #         ax.tick_params(axis='x', which='both', bottom=True, labelbottom=True)
# # # # # # # # # # # # #         ax.legend(
# # # # # # # # # # # # #             title='Variables',
# # # # # # # # # # # # #             loc='upper left',
# # # # # # # # # # # # #             fontsize=10,
# # # # # # # # # # # # #             title_fontsize=12,
# # # # # # # # # # # # #             framealpha=0.9
# # # # # # # # # # # # #         )
# # # # # # # # # # # # #         ax.grid(True, alpha=0.2)

# # # # # # # # # # # # #     for j in range(len(freq_configs), len(axs)):
# # # # # # # # # # # # #         if j < len(axs) and axs[j] is not None:
# # # # # # # # # # # # #             fig.delaxes(axs[j])

# # # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # # #     plt.subplots_adjust(top=0.94, hspace=0.4, wspace=0.15)
    
# # # # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # # import pandas as pd
# # # # # # # # # # # # import numpy as np
# # # # # # # # # # # # import matplotlib.pyplot as plt
# # # # # # # # # # # # import seaborn as sns
# # # # # # # # # # # # from matplotlib.dates import DateFormatter, MonthLocator, WeekdayLocator, YearLocator, DayLocator
# # # # # # # # # # # # from pyproj import CRS, Transformer
# # # # # # # # # # # # import pytz
# # # # # # # # # # # # from astral.location import LocationInfo
# # # # # # # # # # # # from astral import sun
# # # # # # # # # # # # from sklearn.preprocessing import MinMaxScaler
# # # # # # # # # # # # import io
# # # # # # # # # # # # import base64
# # # # # # # # # # # # import warnings
# # # # # # # # # # # # import gdown
# # # # # # # # # # # # import os

# # # # # # # # # # # # # Suppression de l'importation directe de METADATA_VARIABLES pour éviter l'importation circulaire.
# # # # # # # # # # # # # Ces métadonnées seront désormais passées en argument aux fonctions de visualisation.
# # # # # # # # # # # # # from config import METADATA_VARIABLES # Cette ligne est commentée/supprimée

# # # # # # # # # # # # # Supprimer les avertissements de SettingWithCopyWarning de Pandas,
# # # # # # # # # # # # # qui peuvent survenir avec df.loc et les chaînes d'opérations.
# # # # # # # # # # # # pd.options.mode.chained_assignment = None # default='warn'

# # # # # # # # # # # # # --- Fonctions de Traitement de Données ---

# # # # # # # # # # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # # # # # # # # # #     """
# # # # # # # # # # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # # # # # # # # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # # # # # # # # # #     Args:
# # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # # # # # # # # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # # # # # # # # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # # # # # # # # # #     Returns:
# # # # # # # # # # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # # # # # # # # # #     """
# # # # # # # # # # # #     df_copy = df.copy() # Travailler sur une copie

# # # # # # # # # # # #     # Cas spécifique pour VEA_SISSILI ou si seule la colonne 'Date' est présente
# # # # # # # # # # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not all(col in df_copy.columns for col in ['Year', 'Month', 'Day'])):
# # # # # # # # # # # #         try:
# # # # # # # # # # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # # # # # # # # # #         except Exception as e:
# # # # # # # # # # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime: {e}")
# # # # # # # # # # # #             df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # # # # # # # # # #     else:
# # # # # # # # # # # #         # Logique pour les colonnes Year, Month, Day, Hour, Minute
# # # # # # # # # # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # # # # # # # # # #         # Convertir les colonnes en numérique, forçant les erreurs en NaN
# # # # # # # # # # # #         for col in date_cols:
# # # # # # # # # # # #             if col in df_copy.columns:
# # # # # # # # # # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # # # # # # # # # #         # Supprimer les lignes où des valeurs temporelles essentielles sont manquantes
# # # # # # # # # # # #         # (seulement les colonnes qui existent)
# # # # # # # # # # # #         existing_date_cols = [col for col in date_cols if col in df_copy.columns]
# # # # # # # # # # # #         if existing_date_cols:
# # # # # # # # # # # #             df_copy = df_copy.dropna(subset=existing_date_cols)

# # # # # # # # # # # #         # Créer la colonne Datetime si toutes les colonnes nécessaires sont présentes
# # # # # # # # # # # #         if all(col in df_copy.columns for col in date_cols):
# # # # # # # # # # # #             try:
# # # # # # # # # # # #                 df_copy['Datetime'] = pd.to_datetime(df_copy[date_cols])
# # # # # # # # # # # #             except Exception as e:
# # # # # # # # # # # #                 warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées: {e}")
# # # # # # # # # # # #                 df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # # # # # # # # # #         else:
# # # # # # # # # # # #             warnings.warn("Colonnes temporelles (Year, Month, Day, Hour, Minute) ou 'Date' manquantes. La colonne 'Datetime' n'a pas pu être créée.")
# # # # # # # # # # # #             df_copy['Datetime'] = pd.NaT

# # # # # # # # # # # #     # Extraire/recréer les composantes de date/heure si Datetime a été créé
# # # # # # # # # # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # # # # # # # # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # # # # # # # # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # # # # # # # # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # # # # # # # # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # # # # # # # # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # # # # # # # # # #         df_copy['Date'] = df_copy['Datetime'].dt.date
# # # # # # # # # # # #     else:
# # # # # # # # # # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création.")

# # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # def check_compatibility_for_merge(df_list: list[pd.DataFrame]) -> tuple[bool, str]:
# # # # # # # # # # # #     """
# # # # # # # # # # # #     Vérifie si une liste de DataFrames est compatible pour la fusion (concaténation).
# # # # # # # # # # # #     Les DataFrames sont considérés compatibles si :
# # # # # # # # # # # #     - Ils ont le même nombre de colonnes.
# # # # # # # # # # # #     - Ils ont les mêmes noms de colonnes.

# # # # # # # # # # # #     Args:
# # # # # # # # # # # #         df_list (list): Une liste de pandas DataFrames.

# # # # # # # # # # # #     Returns:
# # # # # # # # # # # #         tuple: (bool, str) - True si compatible, False sinon, et un message d'erreur.
# # # # # # # # # # # #     """
# # # # # # # # # # # #     if not df_list:
# # # # # # # # # # # #         return False, "La liste de DataFrames est vide."
# # # # # # # # # # # #     if len(df_list) == 1:
# # # # # # # # # # # #         return True, "Un seul DataFrame, pas de fusion nécessaire."

# # # # # # # # # # # #     first_df = df_list[0]
# # # # # # # # # # # #     num_cols_first = first_df.shape[1]
# # # # # # # # # # # #     cols_first = set(first_df.columns)

# # # # # # # # # # # #     for i, df in enumerate(df_list[1:]):
# # # # # # # # # # # #         if df.shape[1] != num_cols_first:
# # # # # # # # # # # #             return False, f"Le DataFrame {i+2} a {df.shape[1]} colonnes, le premier en a {num_cols_first}. Fusion impossible."
# # # # # # # # # # # #         if set(df.columns) != cols_first:
# # # # # # # # # # # #             missing_in_current = list(cols_first - set(df.columns))
# # # # # # # # # # # #             extra_in_current = list(set(df.columns) - cols_first)
            
# # # # # # # # # # # #             error_msg = f"Les colonnes du DataFrame {i+2} ne correspondent pas à celles du premier."
# # # # # # # # # # # #             if missing_in_current:
# # # # # # # # # # # #                 error_msg += f" Manquantes dans le DataFrame {i+2}: {missing_in_current}."
# # # # # # # # # # # #             if extra_in_current:
# # # # # # # # # # # #                 error_msg += f" Supplémentaires dans le DataFrame {i+2}: {extra_in_current}."
# # # # # # # # # # # #             return False, error_msg
# # # # # # # # # # # #     return True, "Tous les DataFrames sont compatibles pour la fusion."

# # # # # # # # # # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # #     """
# # # # # # # # # # # #     Crée une colonne 'Rain_mm' comme moyenne des mesures de deux pluviomètres.

# # # # # # # # # # # #     Paramètres:
# # # # # # # # # # # #     -----------
# # # # # # # # # # # #     df : pandas.DataFrame
# # # # # # # # # # # #         DataFrame contenant au minimum les colonnes 'Rain_01_mm' et 'Rain_02_mm'

# # # # # # # # # # # #     Retourne:
# # # # # # # # # # # #     --------
# # # # # # # # # # # #     pandas.DataFrame
# # # # # # # # # # # #         Le même DataFrame avec une nouvelle colonne 'Rain_mm' ajoutée
# # # # # # # # # # # #     """
# # # # # # # # # # # #     df_copy = df.copy()
# # # # # # # # # # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # # # # # # # # # #         df_copy['Rain_mm'] = df_copy[['Rain_01_mm', 'Rain_02_mm']].mean(axis=1)
# # # # # # # # # # # #     else:
# # # # # # # # # # # #         df_copy['Rain_mm'] = np.nan
# # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # #     """
# # # # # # # # # # # #     Supprime les doublons complets et nettoie les lignes avec des valeurs temporelles manquantes.
# # # # # # # # # # # #     Gère les conflits où 'Datetime' est à la fois une colonne et un index.

# # # # # # # # # # # #     Args:
# # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée.

# # # # # # # # # # # #     Returns:
# # # # # # # # # # # #         pd.DataFrame: DataFrame traité.
# # # # # # # # # # # #     """
# # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # #     # Suppression des doublons complets
# # # # # # # # # # # #     dup_count = df_copy.duplicated().sum()
# # # # # # # # # # # #     if dup_count > 0:
# # # # # # # # # # # #         warnings.warn(f"Suppression de {dup_count} doublons complets.")
# # # # # # # # # # # #         df_copy = df_copy.drop_duplicates()

# # # # # # # # # # # #     # Nettoyage des colonnes temporelles si elles existent
# # # # # # # # # # # #     time_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
# # # # # # # # # # # #     existing_time_cols = [col for col in time_cols if col in df_copy.columns]
# # # # # # # # # # # #     if existing_time_cols:
# # # # # # # # # # # #         initial_rows = len(df_copy)
# # # # # # # # # # # #         df_copy = df_copy.dropna(subset=existing_time_cols)
# # # # # # # # # # # #         removed_time_rows = initial_rows - len(df_copy)
# # # # # # # # # # # #         if removed_time_rows > 0:
# # # # # # # # # # # #             warnings.warn(f"Suppression de {removed_time_rows} lignes avec valeurs temporelles manquantes.")

# # # # # # # # # # # #     # Gestion des conflits Datetime colonne/index
# # # # # # # # # # # #     if 'Datetime' in df_copy.columns and isinstance(df_copy.index, pd.DatetimeIndex) and df_copy.index.name == 'Datetime':
# # # # # # # # # # # #         pass
# # # # # # # # # # # #     elif 'Datetime' in df_copy.columns and 'Datetime' in df_copy.index.names:
# # # # # # # # # # # #         df_copy = df_copy.reset_index(drop=True)
    
# # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # #     """
# # # # # # # # # # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # # # # # # # # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # # # # # # # # # #     Args:
# # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # # # # # # # # # #     Returns:
# # # # # # # # # # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # # # # # # # # # #     """
# # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # # # # # # # # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # # # # # # # # # #         raise ValueError(
# # # # # # # # # # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # # # # # # # # # #         )

# # # # # # # # # # # #     def convert_row(row):
# # # # # # # # # # # #         try:
# # # # # # # # # # # #             zone = int(row['zone'])
# # # # # # # # # # # #             hemisphere = str(row['hemisphere']).upper()
# # # # # # # # # # # #             is_northern = hemisphere == 'N'

# # # # # # # # # # # #             proj_utm = CRS.from_proj4(
# # # # # # # # # # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # # # # # # # # # #             )
# # # # # # # # # # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # # # # # # # # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # # # # # # # # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # # # # # # # # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # # # # # # # # # #         except Exception as e:
# # # # # # # # # # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # # # # # # # # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # # # # # # # # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # # # # # # # # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # # # # # # # # # #     return df_copy


# # # # # # # # # # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # # # # # # # # # #     """
# # # # # # # # # # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # # # # # # # # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # # # # # # # # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # # # # # # # # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # # # # # # # # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.

# # # # # # # # # # # #     Returns:
# # # # # # # # # # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # # # # # # # # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # # # # # # # # # #     """
# # # # # # # # # # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # # # # # # # # # #     data_dir = 'data'
# # # # # # # # # # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # # # # # # # # # #     # 1. Téléchargement et chargement du bassin VEA SISSILI
# # # # # # # # # # # #     file_id_location = '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz'
# # # # # # # # # # # #     output_file_location = os.path.join(data_dir, "WASCAL Basins Climate Station Coordinates.xlsx")
# # # # # # # # # # # #     print(f"Téléchargement de Vea Sissili depuis Drive...")
# # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_location}', output_file_location, quiet=False)
# # # # # # # # # # # #     vea_sissili_bassin = pd.read_excel(output_file_location)

# # # # # # # # # # # #     # 2. Téléchargement et chargement du bassin DANO
# # # # # # # # # # # #     file_id_dano_basins = '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z'
# # # # # # # # # # # #     output_file_dano_basins = os.path.join(data_dir, "Dano Basins Climate Station Coordinates.xlsx")
# # # # # # # # # # # #     print(f"Téléchargement de Dano depuis Drive...")
# # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_dano_basins}', output_file_dano_basins, quiet=False)
# # # # # # # # # # # #     dano_bassin = pd.read_excel(output_file_dano_basins)

# # # # # # # # # # # #     # 3. Téléchargement et chargement du bassin DASSARI
# # # # # # # # # # # #     file_id_dassari = '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU'
# # # # # # # # # # # #     output_file_dassari = os.path.join(data_dir, "DASSARI Climate Station Coordinates.xlsx")
# # # # # # # # # # # #     print(f"Téléchargement de Dassari depuis Drive...")
# # # # # # # # # # # #     gdown.download(f'https://drive.google.com/uc?id={file_id_dassari}', output_file_dassari, quiet=False)
# # # # # # # # # # # #     dassari_bassin = pd.read_excel(output_file_dassari)

# # # # # # # # # # # #     # 4. Prétraitement des DataFrames (votre code original)
# # # # # # # # # # # #     print("Début du prétraitement des données de stations...")
    
# # # # # # # # # # # #     # Vea Sissili
# # # # # # # # # # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # # # # # # # # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # # # # # # # # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # # # # # # # # # #     # Dassari
# # # # # # # # # # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # # # # # # # # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # # # # # # # # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # # # # # # # # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # # # # # # # # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # # # # # # # # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # # # # # # # # # #     dano_bassin['zone'] = 30
# # # # # # # # # # # #     dano_bassin['hemisphere'] = 'N'
# # # # # # # # # # # #     dassari_bassin['zone'] = 31
# # # # # # # # # # # #     dassari_bassin['hemisphere'] = 'N'

# # # # # # # # # # # #     # Application de la fonction de conversion UTM vers GPS
# # # # # # # # # # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # # # # # # # # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # # # # # # # # # #     # Ajout des fuseaux horaires
# # # # # # # # # # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # # # # # # # # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # # # # # # # # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # # # # # # # # # #     # Fusion de tous les bassins
# # # # # # # # # # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # # # # # # # # # #     # Renommer 'Name' en 'Station'
# # # # # # # # # # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # # # # # # # # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # # # # # # # # # #     initial_rows_count = len(bassins)
# # # # # # # # # # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # # # # # # # # # #     if len(bassins) < initial_rows_count:
# # # # # # # # # # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # # # # # # # # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # # # # # # # # # #     if 'Station' in bassins.columns:
# # # # # # # # # # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # # # # # # # # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # # # # # # # # # #     return bassins


# # # # # # # # # # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # #     """
# # # # # # # # # # # #     Effectue toutes les interpolations météorologiques en une seule passe,
# # # # # # # # # # # #     en intégrant le calcul automatique du lever et du coucher du soleil
# # # # # # # # # # # #     pour une gestion plus précise de la radiation solaire.

# # # # # # # # # # # #     Args:
# # # # # # # # # # # #         df (pd.DataFrame): Le DataFrame d'entrée. Son index DOIT ÊTRE un DatetimeIndex.
# # # # # # # # # # # #                            Il doit également contenir une colonne 'Station'.
# # # # # # # # # # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # # # # # # # # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # # # # # # # # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # # # # # # # # # #     Returns:
# # # # # # # # # # # #         pd.DataFrame: Le DataFrame original avec les données interpolées et
# # # # # # # # # # # #                       la colonne 'Is_Daylight' calculée, ainsi que la durée du jour.
# # # # # # # # # # # #     """
# # # # # # # # # # # #     df_processed = df.copy()

# # # # # # # # # # # #     # VÉRIFICATIONS CRITIQUES POUR L'INDEX TEMPOREL
# # # # # # # # # # # #     # S'assurer que l'index est bien un DatetimeIndex
# # # # # # # # # # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # # # # # # # # # #         raise TypeError("L'index du DataFrame doit être de type DatetimeIndex pour l'interpolation.")

# # # # # # # # # # # #     # Supprime les lignes où l'index Datetime est NaT (valeur manquante)
# # # # # # # # # # # #     # C'est la source de l'erreur précédente car on cherchait une colonne 'Datetime'
# # # # # # # # # # # #     # alors qu'elle était l'index.
# # # # # # # # # # # #     initial_rows = len(df_processed)
# # # # # # # # # # # #     df_processed = df_processed[df_processed.index.notna()]
# # # # # # # # # # # #     if len(df_processed) == 0:
# # # # # # # # # # # #         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
# # # # # # # # # # # #     if initial_rows - len(df_processed) > 0:
# # # # # # # # # # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")

# # # # # # # # # # # #     # --- Pré-vérification et préparation des données GPS (inchangée) ---
# # # # # # # # # # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # # # # # # # # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # # # # # # # # # #         raise ValueError(
# # # # # # # # # # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # # # # # # # # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # # # # # # # # # #         )

# # # # # # # # # # # #     if not df_gps['Station'].is_unique:
# # # # # # # # # # # #         warnings.warn("La colonne 'Station' dans df_gps contient des noms de station dupliqués. Suppression des doublons.")
# # # # # # # # # # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # # # # # # # # # #     else:
# # # # # # # # # # # #         df_gps_unique = df_gps.copy()

# # # # # # # # # # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # # # # # # # # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # # # # # # # # # #     for col in numerical_cols:
# # # # # # # # # # # #         if col in df_processed.columns:
# # # # # # # # # # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # # # # # # # # # #     df_processed_parts = []

# # # # # # # # # # # #     for station_name, group in df_processed.groupby('Station'):
# # # # # # # # # # # #         group_copy = group.copy()

# # # # # # # # # # # #         # Extraire les infos GPS pour la station actuelle
# # # # # # # # # # # #         gps_data = gps_info_dict.get(station_name)

# # # # # # # # # # # #         # L'index est déjà un DatetimeIndex à ce stade grâce aux vérifications précédentes
# # # # # # # # # # # #         # Mais le tri est toujours une bonne pratique
# # # # # # # # # # # #         group_copy = group_copy.sort_index()


# # # # # # # # # # # #         # Calcul de lever/coucher du soleil ou fallback
# # # # # # # # # # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # # # # # # # # # #             lat = gps_data['Lat']
# # # # # # # # # # # #             long = gps_data['Long']
# # # # # # # # # # # #             timezone_str = gps_data['Timezone']

# # # # # # # # # # # #             try:
# # # # # # # # # # # #                 tz = pytz.timezone(timezone_str)
# # # # # # # # # # # #                 # S'assurer que l'index du groupe est dans le bon fuseau horaire
# # # # # # # # # # # #                 if group_copy.index.tz is None:
# # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_localize(tz, ambiguous='NaT', nonexistent='NaT')
# # # # # # # # # # # #                 else:
# # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_convert(tz)

# # # # # # # # # # # #                 daily_sun_info = {}
# # # # # # # # # # # #                 for date_only in group_copy.index.normalize().unique():
# # # # # # # # # # # #                     s = sun.sun(LocationInfo(station_name, "Site", timezone_str, lat, long).observer,
# # # # # # # # # # # #                                 date=date_only, tzinfo=tz)
# # # # # # # # # # # #                     daily_sun_info[date_only.date()] = {
# # # # # # # # # # # #                         'sunrise': s['sunrise'],
# # # # # # # # # # # #                         'sunset': s['sunset']
# # # # # # # # # # # #                     }

# # # # # # # # # # # #                 group_copy.loc[:, 'sunrise_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunrise'))
# # # # # # # # # # # #                 group_copy.loc[:, 'sunset_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunset'))

# # # # # # # # # # # #                 # Assurer que les colonnes sunrise/sunset sont dans le même fuseau horaire que l'index pour la comparaison
# # # # # # # # # # # #                 if group_copy.index.tz is not None:
# # # # # # # # # # # #                      group_copy['sunrise_time_local'] = group_copy['sunrise_time_local'].dt.tz_convert(group_copy.index.tz)
# # # # # # # # # # # #                      group_copy['sunset_time_local'] = group_copy['sunset_time_local'].dt.tz_convert(group_copy.index.tz)

# # # # # # # # # # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index >= group_copy['sunrise_time_local']) & \
# # # # # # # # # # # #                                                   (group_copy.index < group_copy['sunset_time_local'])

# # # # # # # # # # # #                 daylight_timedelta = group_copy['sunset_time_local'] - group_copy['sunrise_time_local']
# # # # # # # # # # # #                 group_copy.loc[:, 'Daylight_Duration'] = daylight_timedelta.apply(
# # # # # # # # # # # #                     lambda td: (pd.to_datetime('00:00:00') + td).strftime('%H:%M:%S') if pd.notna(td) else np.nan
# # # # # # # # # # # #                 )
# # # # # # # # # # # #             except Exception as e:
# # # # # # # # # # # #                 warnings.warn(f"Erreur Astral pour {station_name}: {e}. Utilisation du fallback 7h-18h.")
# # # # # # # # # # # #                 if group_copy.index.tz is None:
# # # # # # # # # # # #                     group_copy.index = group_copy.index.tz_localize('UTC') # Fallback pour localiser l'index
# # # # # # # # # # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # # # # # # # # # #                 group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # # # # # # # # # #         else:
# # # # # # # # # # # #             warnings.warn(f"Coordonnées/Fuseau horaire manquants/invalides pour {station_name}. Utilisation du fallback 7h-18h.")
# # # # # # # # # # # #             if group_copy.index.tz is None:
# # # # # # # # # # # #                 group_copy.index = group_copy.index.tz_localize('UTC')
# # # # # # # # # # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # # # # # # # # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"

# # # # # # # # # # # #         df_processed_parts.append(group_copy)

# # # # # # # # # # # #     if not df_processed_parts:
# # # # # # # # # # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # # # # # # # # # #     df_final = pd.concat(df_processed_parts)
# # # # # # # # # # # #     df_final = df_final.sort_index()
# # # # # # # # # # # #     df_final.index.name = 'Datetime'

# # # # # # # # # # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date',
# # # # # # # # # # # #                                   'sunrise_time_local', 'sunset_time_local']
# # # # # # # # # # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # # # # # # # # # #     # Gestion intelligente de Rain_mm
# # # # # # # # # # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # # # # # # # # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # # # # # # # # # #             df_final = create_rain_mm(df_final)
# # # # # # # # # # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # # # # # # # # # #         else:
# # # # # # # # # # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # # # # # # # # # #             df_final['Rain_mm'] = np.nan


# # # # # # # # # # # #     # Interpolation standard et bornage pour les variables numériques
# # # # # # # # # # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # # # #                      'Wind_Dir_Deg']

# # # # # # # # # # # #     for var in standard_vars:
# # # # # # # # # # # #         if var in df_final.columns:
# # # # # # # # # # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # # # # # # # # # #             if var in limits:
# # # # # # # # # # # #                 min_val = limits[var]['min']
# # # # # # # # # # # #                 max_val = limits[var]['max']
# # # # # # # # # # # #                 initial_nan_count = df_final[var].isna().sum()
# # # # # # # # # # # #                 df_final.loc[(df_final[var] < min_val) | (df_final[var] > max_val), var] = np.nan
# # # # # # # # # # # #                 if df_final[var].isna().sum() > initial_nan_count:
# # # # # # # # # # # #                     warnings.warn(f"Remplacement de {df_final[var].isna().sum() - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # # # # # # # # # #             # Interpolation
# # # # # # # # # # # #             df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # # # # # # # # # #             df_final[var] = df_final[var].bfill().ffill() # Pour les NaNs aux extrémités

# # # # # # # # # # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # # # # # # # # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # # # # # # # # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # # # # # # # # # #         if 'Solar_R_W/m^2' in limits:
# # # # # # # # # # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # # # # # # # # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # # # # # # # # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # # # # # # # # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # # # # # # # # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # # # # # # # # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # # # # # # # # # #         if 'Is_Daylight' in df_final.columns:
# # # # # # # # # # # #             # Radiation positive hors du jour -> 0
# # # # # # # # # # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # # # # # # # # # #             # Zéros suspects en journée sans pluie -> NaN
# # # # # # # # # # # #             if 'Rain_mm' in df_final.columns:
# # # # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # # # # # # # # # #             else:
# # # # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # # # # # # # # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # # # # # # # # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # # # # # # # # # #             # Interpolation dans les périodes de jour
# # # # # # # # # # # #             is_day = df_final['Is_Daylight']
# # # # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # # # # # # # # # #             # Les NaNs restants hors des heures de jour -> 0
# # # # # # # # # # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # # # # # # # # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # # # # # # # # # #         else:
# # # # # # # # # # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # # # # # # # # # #             df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()

# # # # # # # # # # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # # # # # # # # # #     missing_after_interp = df_final.isna().sum()
# # # # # # # # # # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # # # # # # # # # #     if not columns_with_missing.empty:
# # # # # # # # # # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # # # # # # # # # #     else:
# # # # # # # # # # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # # # # # # # # # #     return df_final

# # # # # # # # # # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # # # # # # # # # #     """
# # # # # # # # # # # #     Traite les valeurs aberrantes (outliers) dans un DataFrame de données météorologiques
# # # # # # # # # # # #     en appliquant la méthode de l'écart interquartile (IQR) pour limiter les valeurs extrêmes.

# # # # # # # # # # # #     Args:
# # # # # # # # # # # #         df (pd.DataFrame): DataFrame contenant les données météorologiques à traiter.
# # # # # # # # # # # #         limits (dict): Dictionnaire des variables à traiter (clés) avec leurs limites potentielles.
# # # # # # # # # # # #                        (Note: les limites min/max du dict ne sont pas utilisées pour l'IQR,
# # # # # # # # # # # #                        mais les clés du dict définissent les variables à analyser).

# # # # # # # # # # # #     Returns:
# # # # # # # # # # # #         pd.DataFrame: Copie du DataFrame avec les outliers corrigés par la méthode IQR.
# # # # # # # # # # # #     """
# # # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # # #     for var in limits.keys(): # On utilise les clés de limits pour savoir quelles variables traiter
# # # # # # # # # # # #         if var in df_copy.columns:
# # # # # # # # # # # #             # S'assurer que la colonne est numérique
# # # # # # # # # # # #             df_copy[var] = pd.to_numeric(df_copy[var], errors='coerce')

# # # # # # # # # # # #             # Calcul des quartiles
# # # # # # # # # # # #             Q1 = df_copy[var].quantile(0.25)
# # # # # # # # # # # #             Q3 = df_copy[var].quantile(0.75)
# # # # # # # # # # # #             IQR = Q3 - Q1

# # # # # # # # # # # #             # Bornes IQR
# # # # # # # # # # # #             borne_inf = Q1 - 1.5 * IQR
# # # # # # # # # # # #             borne_sup = Q3 + 1.5 * IQR

# # # # # # # # # # # #             # Application du bornage (clipping)
# # # # # # # # # # # #             initial_outliers_count = df_copy[(df_copy[var] < borne_inf) | (df_copy[var] > borne_sup)][var].count()
# # # # # # # # # # # #             df_copy[var] = df_copy[var].clip(lower=borne_inf, upper=borne_sup)
# # # # # # # # # # # #             if initial_outliers_count > 0:
# # # # # # # # # # # #                 warnings.warn(f"Outliers traités par IQR pour '{var}'. {initial_outliers_count} valeurs ajustées.")
# # # # # # # # # # # #     return df_copy

# # # # # # # # # # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # # #     """
# # # # # # # # # # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables météorologiques.

# # # # # # # # # # # #     Args:
# # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec 'Datetime' comme index.

# # # # # # # # # # # #     Returns:
# # # # # # # # # # # #         pd.DataFrame: DataFrame contenant les statistiques journalières.
# # # # # # # # # # # #     """
# # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # #         raise TypeError("L'index du DataFrame doit être de type DatetimeIndex.")

# # # # # # # # # # # #     # Définir les variables pour lesquelles nous voulons des statistiques
# # # # # # # # # # # #     # et leurs méthodes d'agrégation.
# # # # # # # # # # # #     # Rain_mm doit être sommée, les autres moyennées.
# # # # # # # # # # # #     # Wind_Dir_Deg est un cas particulier, souvent on calcule une direction moyenne vectorielle
# # # # # # # # # # # #     # mais pour des stats simples, une moyenne arithmétique peut être misleading.
# # # # # # # # # # # #     # Pour cet exemple, on le met en moyenne, mais gardez cela à l'esprit.
# # # # # # # # # # # #     agg_funcs = {
# # # # # # # # # # # #         'Air_Temp_Deg_C': 'mean',
# # # # # # # # # # # #         'Rel_H_%': 'mean',
# # # # # # # # # # # #         'BP_mbar_Avg': 'mean',
# # # # # # # # # # # #         'Rain_mm': 'sum', # Précipitation cumulée
# # # # # # # # # # # #         'Wind_Sp_m/sec': 'mean',
# # # # # # # # # # # #         'Solar_R_W/m^2': 'mean',
# # # # # # # # # # # #         'Wind_Dir_Deg': 'mean' # Moyenne simple, à interpréter avec prudence
# # # # # # # # # # # #     }

# # # # # # # # # # # #     # Filtrer les colonnes qui existent dans le DataFrame
# # # # # # # # # # # #     existing_cols_to_agg = {col: func for col, func in agg_funcs.items() if col in df.columns}

# # # # # # # # # # # #     if not existing_cols_to_agg:
# # # # # # # # # # # #         warnings.warn("Aucune colonne pertinente trouvée pour les statistiques journalières.")
# # # # # # # # # # # #         return pd.DataFrame()

# # # # # # # # # # # #     # Regrouper par jour (en utilisant l'index Datetime) et par station, puis agréger
# # # # # # # # # # # #     daily_summary = df.groupby([df.index.date, 'Station']).agg(
# # # # # # # # # # # #         **{f"{col}_{func}": (col, func) for col, func in existing_cols_to_agg.items()}
# # # # # # # # # # # #     )
# # # # # # # # # # # #     daily_summary.index.names = ['Date', 'Station']
# # # # # # # # # # # #     return daily_summary.reset_index()


# # # # # # # # # # # # # --- Fonctions de Visualisation ---

# # # # # # # # # # # # # Fonction utilitaire pour convertir une figure Matplotlib en base64
# # # # # # # # # # # # def fig_to_base64(fig: plt.Figure) -> str | None:
# # # # # # # # # # # #     """
# # # # # # # # # # # #     Convertit une figure Matplotlib en une chaîne Base64 encodée (format PNG).

# # # # # # # # # # # #     Args:
# # # # # # # # # # # #         fig (plt.Figure): La figure Matplotlib à convertir.

# # # # # # # # # # # #     Returns:
# # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG, ou None si la figure est invalide.
# # # # # # # # # # # #     """
# # # # # # # # # # # #     if fig is None:
# # # # # # # # # # # #         return None
# # # # # # # # # # # #     buf = io.BytesIO()
# # # # # # # # # # # #     try:
# # # # # # # # # # # #         fig.savefig(buf, format='png', bbox_inches='tight')
# # # # # # # # # # # #         plt.close(fig) # Fermer la figure pour libérer la mémoire
# # # # # # # # # # # #         data = base64.b64encode(buf.getbuffer()).decode("ascii")
# # # # # # # # # # # #         return data
# # # # # # # # # # # #     except Exception as e:
# # # # # # # # # # # #         warnings.warn(f"Erreur lors de la conversion de la figure en base64: {e}")
# # # # # # # # # # # #         plt.close(fig) # Assurez-vous de fermer la figure même en cas d'erreur
# # # # # # # # # # # #         return None

# # # # # # # # # # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station_name: str, variable_name: str, periode: str, custom_palette: list = None, metadata_variables: dict = None) -> str | None:
# # # # # # # # # # # #     """
# # # # # # # # # # # #     Génère un seul graphique de l'évolution d'une variable météorologique pour une station
# # # # # # # # # # # #     spécifique et une période donnée.

# # # # # # # # # # # #     Args:
# # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée contenant les données météorologiques.
# # # # # # # # # # # #                            Doit inclure 'Datetime' comme index et 'Station'.
# # # # # # # # # # # #         station_name (str): Nom de la station à visualiser.
# # # # # # # # # # # #         variable_name (str): Nom de la variable à visualiser.
# # # # # # # # # # # #         periode (str): Période d'agrégation ('Journalière', 'Hebdomadaire', 'Mensuelle', 'Annuelle').
# # # # # # # # # # # #         custom_palette (list, optional): Palette de couleurs personnalisée.
# # # # # # # # # # # #         metadata_variables (dict, optional): Dictionnaire des métadonnées des variables.
# # # # # # # # # # # #                                             Si None, un avertissement sera émis.

# # # # # # # # # # # #     Returns:
# # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # #     """
# # # # # # # # # # # #     if metadata_variables is None:
# # # # # # # # # # # #         warnings.warn("metadata_variables n'a pas été fourni à generer_graphique_par_variable_et_periode. Certaines informations pourraient être manquantes.")
# # # # # # # # # # # #         # Fallback si METADATA_VARIABLES n'est pas passé (moins robuste)
# # # # # # # # # # # #         metadata_variables = {
# # # # # # # # # # # #             'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'agg_type': 'cumul'},
# # # # # # # # # # # #             'Air_Temp_Deg_C': {'Nom': "Température ", 'Unite': "°C", 'agg_type': 'moyenne'},
# # # # # # # # # # # #             'Rel_H_%': {'Nom': "Humidité Relative", 'Unite': "%", 'agg_type': 'moyenne'},
# # # # # # # # # # # #             'Solar_R_W/m^2': {'Nom': "Radiation Solaire", 'Unite': "W/m²", 'agg_type': 'moyenne'},
# # # # # # # # # # # #             'Wind_Sp_m/sec': {'Nom': "Vitesse du Vent", 'Unite': "m/s", 'agg_type': 'moyenne'},
# # # # # # # # # # # #             'Wind_Dir_Deg': {'Nom': "Direction du Vent", 'Unite': "°", 'agg_type': 'moyenne'},
# # # # # # # # # # # #             'BP_mbar_Avg': {'Nom': "Pression Atmospherique moyenne", 'Unite': "mbar", 'agg_type': 'moyenne'}
# # # # # # # # # # # #         }


# # # # # # # # # # # #     if variable_name not in metadata_variables or variable_name not in df.columns:
# # # # # # # # # # # #         warnings.warn(f"Variable '{variable_name}' non valide ou absente du DataFrame.")
# # # # # # # # # # # #         return None

# # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # #         return None

# # # # # # # # # # # #     df_station = df[df['Station'] == station_name].copy()
# # # # # # # # # # # #     if df_station.empty:
# # # # # # # # # # # #         warnings.warn(f"Aucune donnée pour la station '{station_name}'.")
# # # # # # # # # # # #         return None

# # # # # # # # # # # #     df_station[variable_name] = pd.to_numeric(df_station[variable_name], errors='coerce')

# # # # # # # # # # # #     annees_uniques = df_station.index.year.nunique()
# # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # #         }
# # # # # # # # # # # #     else:
# # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # #         }

# # # # # # # # # # # #     if periode not in freq_configs:
# # # # # # # # # # # #         warnings.warn(f"Période '{periode}' non valide.")
# # # # # # # # # # # #         return None

# # # # # # # # # # # #     freq, date_fmt, rot, locator, label_x = freq_configs[periode]
# # # # # # # # # # # #     meta = metadata_variables[variable_name]
# # # # # # # # # # # #     nom_complet = f"{meta['Nom']} ({meta['Unite']})"
# # # # # # # # # # # #     agg_func = 'sum' if meta['agg_type'] == 'cumul' else 'mean'

# # # # # # # # # # # #     # Déterminer la couleur de la station
# # # # # # # # # # # #     stations_list = sorted(df['Station'].unique())
# # # # # # # # # # # #     try:
# # # # # # # # # # # #         station_idx = stations_list.index(station_name)
# # # # # # # # # # # #     except ValueError:
# # # # # # # # # # # #         warnings.warn(f"Station '{station_name}' non trouvée dans la liste des stations uniques.")
# # # # # # # # # # # #         station_idx = 0 # Fallback
    
# # # # # # # # # # # #     palette = custom_palette if custom_palette and len(custom_palette) > station_idx else sns.color_palette("husl", len(stations_list))
# # # # # # # # # # # #     station_color = palette[station_idx] if len(palette) > station_idx else 'blue' # Fallback couleur

# # # # # # # # # # # #     fig, ax = plt.subplots(figsize=(12, 7))

# # # # # # # # # # # #     df_temp = df_station.copy()

# # # # # # # # # # # #     if variable_name == 'Solar_R_W/m^2':
# # # # # # # # # # # #         if 'Is_Daylight' in df_temp.columns:
# # # # # # # # # # # #             df_temp = df_temp[df_temp['Is_Daylight']].copy()
# # # # # # # # # # # #         else:
# # # # # # # # # # # #             warnings.warn("Avertissement: Colonne 'Is_Daylight' manquante pour la radiation solaire. Filtrage horaire non appliqué.")
# # # # # # # # # # # #             df_temp = df_temp[(df_temp.index.hour >= 7) & (df_temp.index.hour <= 18)]
            
# # # # # # # # # # # #     elif variable_name == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in df_temp.columns:
# # # # # # # # # # # #         df_temp = df_temp[(df_temp['Wind_Sp_m/sec'] > 0) & (df_temp['Wind_Dir_Deg'].notna())]

# # # # # # # # # # # #     with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # #         df_agg = df_temp.groupby(pd.Grouper(freq=freq))[variable_name].agg(agg_func).reset_index()
# # # # # # # # # # # #     df_agg.rename(columns={'index': 'Datetime'}, inplace=True)

# # # # # # # # # # # #     if df_agg.empty:
# # # # # # # # # # # #         plt.close(fig)
# # # # # # # # # # # #         warnings.warn(f"Aucune donnée agrégée pour {variable_name} à la période {periode} pour {station_name}.")
# # # # # # # # # # # #         return None

# # # # # # # # # # # #     sns.lineplot(
# # # # # # # # # # # #         data=df_agg,
# # # # # # # # # # # #         x='Datetime',
# # # # # # # # # # # #         y=variable_name,
# # # # # # # # # # # #         color=station_color,
# # # # # # # # # # # #         linewidth=2,
# # # # # # # # # # # #         marker='o' if periode == 'Annuelle' else None,
# # # # # # # # # # # #         markersize=6 if periode == 'Annuelle' else 0,
# # # # # # # # # # # #         ax=ax
# # # # # # # # # # # #     )

# # # # # # # # # # # #     ax.set_title(f"Évolution {periode.lower()} de {nom_complet} pour {station_name}", pad=10, fontsize=14)
# # # # # # # # # # # #     ax.set_ylabel(nom_complet, fontsize=12)
# # # # # # # # # # # #     ax.set_xlabel(label_x, fontsize=10)

# # # # # # # # # # # #     ax.xaxis.set_major_formatter(date_fmt)
# # # # # # # # # # # #     if locator:
# # # # # # # # # # # #         ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # #     for label in ax.get_xticklabels():
# # # # # # # # # # # #         label.set_rotation(rot)
# # # # # # # # # # # #         label.set_ha('right' if rot > 45 else 'center')

# # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # # def generer_graphique_comparatif(df: pd.DataFrame, variable_name: str, periode: str, custom_palette: list = None, metadata_variables: dict = None) -> str | None:
# # # # # # # # # # # #     """
# # # # # # # # # # # #     Génère un seul graphique comparatif de l'évolution d'une variable météorologique
# # # # # # # # # # # #     entre toutes les stations pour une période donnée.

# # # # # # # # # # # #     Args:
# # # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée contenant les données météorologiques.
# # # # # # # # # # # #                            Doit inclure 'Datetime' comme index et 'Station'.
# # # # # # # # # # # #         variable_name (str): Nom de la variable à visualiser.
# # # # # # # # # # # #         periode (str): Période d'agrégation ('Journalière', 'Hebdomadaire', 'Mensuelle', 'Annuelle').
# # # # # # # # # # # #         custom_palette (list, optional): Palette de couleurs personnalisée.
# # # # # # # # # # # #         metadata_variables (dict, optional): Dictionnaire des métadonnées des variables.
# # # # # # # # # # # #                                             Si None, un avertissement sera émis.

# # # # # # # # # # # #     Returns:
# # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # #     """
# # # # # # # # # # # #     if metadata_variables is None:
# # # # # # # # # # # #         warnings.warn("metadata_variables n'a pas été fourni à generer_graphique_comparatif. Certaines informations pourraient être manquantes.")
# # # # # # # # # # # #         # Fallback si METADATA_VARIABLES n'est pas passé (moins robuste)
# # # # # # # # # # # #         metadata_variables = {
# # # # # # # # # # # #             'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'is_rain': True},
# # # # # # # # # # # #             'Air_Temp_Deg_C': {'Nom': "Température", 'Unite': "°C", 'is_rain': False},
# # # # # # # # # # # #             'Rel_H_%': {'Nom': "Humidité", 'Unite': "%", 'is_rain': False},
# # # # # # # # # # # #             'Solar_R_W/m^2': {'Nom': "Radiation solaire", 'Unite': "W/m²", 'is_rain': False},
# # # # # # # # # # # #             'Wind_Sp_m/sec': {'Nom': "Vitesse du vent", 'Unite': "m/s", 'is_rain': False},
# # # # # # # # # # # #             'Wind_Dir_Deg': {'Nom': "Direction du vent", 'Unite': "°", 'is_rain': False},
# # # # # # # # # # # #             'BP_mbar_Avg': {'Nom': "Pression atmospherique moyenne", 'Unite': "mbar", 'is_rain': False}
# # # # # # # # # # # #         }


# # # # # # # # # # # #     if variable_name not in metadata_variables or variable_name not in df.columns:
# # # # # # # # # # # #         warnings.warn(f"Variable '{variable_name}' non valide ou absente du DataFrame.")
# # # # # # # # # # # #         return None

# # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # #         return None

# # # # # # # # # # # #     annees_uniques = df.index.year.nunique()
# # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # #         }
# # # # # # # # # # # #     else:
# # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # #         }

# # # # # # # # # # # #     if periode not in freq_configs:
# # # # # # # # # # # #         warnings.warn(f"Période '{periode}' non valide.")
# # # # # # # # # # # #         return None

# # # # # # # # # # # #     freq, date_fmt, rot, locator, label_x = freq_configs[periode]
# # # # # # # # # # # #     meta = metadata_variables[variable_name]
# # # # # # # # # # # #     nom_complet = f"{meta['Nom']} ({meta['Unite']})"
# # # # # # # # # # # #     agg_text = " (cumul)" if meta['is_rain'] else " (moyenne)"
# # # # # # # # # # # #     agg_func = 'sum' if meta['is_rain'] else 'mean'

# # # # # # # # # # # #     stations = sorted(df['Station'].unique())
# # # # # # # # # # # #     palette = custom_palette if custom_palette and len(custom_palette) >= len(stations) \
# # # # # # # # # # # #               else sns.color_palette("husl", len(stations))

# # # # # # # # # # # #     fig, ax = plt.subplots(figsize=(14, 8))

# # # # # # # # # # # #     temp_df = df.copy()

# # # # # # # # # # # #     if variable_name == 'Solar_R_W/m^2':
# # # # # # # # # # # #         if 'Is_Daylight' in temp_df.columns:
# # # # # # # # # # # #             temp_df = temp_df[temp_df['Is_Daylight']].copy()
# # # # # # # # # # # #         else:
# # # # # # # # # # # #             warnings.warn("Avertissement: Colonne 'Is_Daylight' manquante. Filtrage horaire par défaut (7h-18h) pour radiation solaire.")
# # # # # # # # # # # #             temp_df = temp_df[(temp_df.index.hour >= 7) & (temp_df.index.hour <= 18)]
            
# # # # # # # # # # # #     elif variable_name == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in temp_df.columns:
# # # # # # # # # # # #         temp_df = temp_df[(temp_df['Wind_Sp_m/sec'] > 0) & (temp_df['Wind_Dir_Deg'].notna())]

# # # # # # # # # # # #     with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # #         df_agg = temp_df.groupby(['Station', pd.Grouper(freq=freq)])[variable_name].agg(agg_func).reset_index()

# # # # # # # # # # # #     if df_agg.empty:
# # # # # # # # # # # #         plt.close(fig)
# # # # # # # # # # # #         warnings.warn(f"Aucune donnée agrégée pour {variable_name} à la période {periode}.")
# # # # # # # # # # # #         return None

# # # # # # # # # # # #     sns.lineplot(
# # # # # # # # # # # #         data=df_agg,
# # # # # # # # # # # #         x='Datetime',
# # # # # # # # # # # #         y=variable_name,
# # # # # # # # # # # #         hue='Station',
# # # # # # # # # # # #         palette=palette,
# # # # # # # # # # # #         ax=ax,
# # # # # # # # # # # #         linewidth=2,
# # # # # # # # # # # #         marker='o' if periode == 'Annuelle' else None,
# # # # # # # # # # # #         markersize=6 if periode == 'Annuelle' else 0,
# # # # # # # # # # # #         legend='full'
# # # # # # # # # # # #     )

# # # # # # # # # # # #     ax.set_title(
# # # # # # # # # # # #         f"Comparaison de {meta['Nom']} {agg_text} ({meta['Unite']}) - Période: {periode}",
# # # # # # # # # # # #         fontsize=16,
# # # # # # # # # # # #         pad=15,
# # # # # # # # # # # #         weight='bold'
# # # # # # # # # # # #     )
# # # # # # # # # # # #     ax.set_xlabel(label_x, fontsize=12, labelpad=15)
# # # # # # # # # # # #     ax.set_ylabel(nom_complet, fontsize=12)

# # # # # # # # # # # #     ax.xaxis.set_major_formatter(date_fmt)
# # # # # # # # # # # #     if locator:
# # # # # # # # # # # #         ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # #     for label in ax.get_xticklabels():
# # # # # # # # # # # #         label.set_rotation(rot)
# # # # # # # # # # # #         label.set_ha('right' if rot < 90 else 'center')
# # # # # # # # # # # #         label.set_fontsize(10)

# # # # # # # # # # # #     ax.legend(
# # # # # # # # # # # #         title='Stations',
# # # # # # # # # # # #         loc='upper left',
# # # # # # # # # # # #         fontsize=10,
# # # # # # # # # # # #         title_fontsize=12,
# # # # # # # # # # # #         framealpha=0.9
# # # # # # # # # # # #     )
# # # # # # # # # # # #     ax.grid(True, alpha=0.2)

# # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # #     plt.subplots_adjust(
# # # # # # # # # # # #         top=0.93,
# # # # # # # # # # # #         hspace=0.3,
# # # # # # # # # # # #         wspace=0.15
# # # # # # # # # # # #     )
# # # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station_name: str, variable_palette: list = None, metadata_variables: dict = None) -> str | None:
# # # # # # # # # # # #     """
# # # # # # # # # # # #     Génère un graphique comparatif normalisé de l'évolution de plusieurs variables météorologiques
# # # # # # # # # # # #     pour une seule station sélectionnée, à différentes échelles temporelles.

# # # # # # # # # # # #     Args:
# # # # # # # # # # # #         df (pd.DataFrame): DataFrame contenant les données météorologiques. Doit avoir 'Datetime' comme index.
# # # # # # # # # # # #         station_name (str): Le nom de la station pour laquelle générer le graphique.
# # # # # # # # # # # #         variable_palette (list, optional): Liste personnalisée de couleurs pour chaque variable.
# # # # # # # # # # # #         metadata_variables (dict, optional): Dictionnaire des métadonnées des variables.
# # # # # # # # # # # #                                             Si None, un avertissement sera émis.

# # # # # # # # # # # #     Returns:
# # # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # # #     """
# # # # # # # # # # # #     if metadata_variables is None:
# # # # # # # # # # # #         warnings.warn("metadata_variables n'a pas été fourni à generate_multi_variable_station_plot. Certaines informations pourraient être manquantes.")
# # # # # # # # # # # #         # Fallback si METADATA_VARIABLES n'est pas passé (moins robuste)
# # # # # # # # # # # #         metadata_variables = {
# # # # # # # # # # # #             'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'agg_type': 'cumul'},
# # # # # # # # # # # #             'Air_Temp_Deg_C': {'Nom': "Température ", 'Unite': "°C", 'agg_type': 'moyenne'},
# # # # # # # # # # # #             'Rel_H_%': {'Nom': "Humidité Relative", 'Unite': "%", 'agg_type': 'moyenne'},
# # # # # # # # # # # #             'Solar_R_W/m^2': {'Nom': "Radiation Solaire", 'Unite': "W/m²", 'agg_type': 'moyenne'},
# # # # # # # # # # # #             'Wind_Sp_m/sec': {'Nom': "Vitesse du Vent", 'Unite': "m/s", 'agg_type': 'moyenne'},
# # # # # # # # # # # #             'Wind_Dir_Deg': {'Nom': "Direction du Vent", 'Unite': "°", 'agg_type': 'moyenne'},
# # # # # # # # # # # #             'BP_mbar_Avg': {'Nom': "Pression Atmospherique moyenne", 'Unite': "mbar", 'agg_type': 'moyenne'}
# # # # # # # # # # # #         }


# # # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # # #         return None

# # # # # # # # # # # #     df_station = df[df['Station'] == station_name].copy()

# # # # # # # # # # # #     if df_station.empty:
# # # # # # # # # # # #         warnings.warn(f"Station {station_name} sans données - ignorée.")
# # # # # # # # # # # #         return None

# # # # # # # # # # # #     annees_uniques = df_station.index.year.nunique()
# # # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # # #         }
# # # # # # # # # # # #     else:
# # # # # # # # # # # #         freq_configs = {
# # # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # # #         }

# # # # # # # # # # # #     nrows, ncols = (2, 2) if len(freq_configs) >= 2 else (1, 1)

# # # # # # # # # # # #     variables_to_plot = [var for var in metadata_variables.keys() if var in df_station.columns]

# # # # # # # # # # # #     if not variables_to_plot:
# # # # # # # # # # # #         warnings.warn("Aucune variable valide à tracer pour cette station.")
# # # # # # # # # # # #         return None

# # # # # # # # # # # #     num_vars_to_plot = len(variables_to_plot)
# # # # # # # # # # # #     if (variable_palette is not None and
# # # # # # # # # # # #         isinstance(variable_palette, list) and
# # # # # # # # # # # #         len(variable_palette) >= num_vars_to_plot):
# # # # # # # # # # # #         plot_palette = variable_palette
# # # # # # # # # # # #     else:
# # # # # # # # # # # #         plot_palette = sns.color_palette("tab10", num_vars_to_plot)
# # # # # # # # # # # #         if variable_palette is not None:
# # # # # # # # # # # #             warnings.warn(f"Avertissement: Palette fournie insuffisante ou invalide - utilisation de la palette par défaut pour {station_name}.")

# # # # # # # # # # # #     fig, axs = plt.subplots(nrows, ncols, figsize=(28, 22))
# # # # # # # # # # # #     axs = axs.flatten()

# # # # # # # # # # # #     fig.suptitle(
# # # # # # # # # # # #         f'Évolution Normalisée des Variables Météorologiques - Station: {station_name}',
# # # # # # # # # # # #         fontsize=24,
# # # # # # # # # # # #         weight='bold',
# # # # # # # # # # # #         y=1.02
# # # # # # # # # # # #     )

# # # # # # # # # # # #     for i, (periode, (freq, date_format, rot, locator, label_x)) in enumerate(freq_configs.items()):
# # # # # # # # # # # #         if i >= len(axs):
# # # # # # # # # # # #             continue

# # # # # # # # # # # #         ax = axs[i]
# # # # # # # # # # # #         agg_dict = {}
# # # # # # # # # # # #         for var in variables_to_plot:
# # # # # # # # # # # #             if var in df_station.columns:
# # # # # # # # # # # #                 agg_type = metadata_variables[var].get('agg_type', 'moyenne')
# # # # # # # # # # # #                 agg_dict[var] = 'sum' if agg_type == 'cumul' else 'mean'

# # # # # # # # # # # #         if not agg_dict:
# # # # # # # # # # # #             warnings.warn(f"Aucune donnée à agréger pour {station_name} - période {periode}.")
# # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # #             continue

# # # # # # # # # # # #         with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # # #             for col in agg_dict.keys():
# # # # # # # # # # # #                 if col in df_station.columns:
# # # # # # # # # # # #                     df_station[col] = pd.to_numeric(df_station[col], errors='coerce')

# # # # # # # # # # # #             cols_to_agg = list(agg_dict.keys())
# # # # # # # # # # # #             if not cols_to_agg:
# # # # # # # # # # # #                 warnings.warn(f"Aucune colonne numérique valide pour {station_name} - période {periode}.")
# # # # # # # # # # # #                 if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # #                 continue

# # # # # # # # # # # #             df_agg = pd.DataFrame(index=df_station.index)

# # # # # # # # # # # #             for var in cols_to_agg:
# # # # # # # # # # # #                 temp_data = df_station[var]
# # # # # # # # # # # #                 if var == 'Solar_R_W/m^2':
# # # # # # # # # # # #                     if 'Is_Daylight' in df_station.columns:
# # # # # # # # # # # #                         temp_data = df_station.loc[df_station['Is_Daylight'], var]
# # # # # # # # # # # #                     else:
# # # # # # # # # # # #                         temp_data = df_station.loc[(df_station.index.hour >= 7) & (df_station.index.hour <= 18), var]
# # # # # # # # # # # #                         warnings.warn(f"Avertissement: 'Is_Daylight' non trouvé. Radiation solaire filtrée par heures fixes (7h-18h).")
# # # # # # # # # # # #                 elif var == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in df_station.columns:
# # # # # # # # # # # #                     temp_data = df_station.loc[(df_station['Wind_Sp_m/sec'] > 0) & (df_station['Wind_Dir_Deg'].notna()), var]

# # # # # # # # # # # #                 if not temp_data.empty:
# # # # # # # # # # # #                     df_agg[var] = temp_data.resample(freq).agg(agg_dict[var])
# # # # # # # # # # # #                 else:
# # # # # # # # # # # #                     df_agg[var] = np.nan

# # # # # # # # # # # #             df_agg = df_agg.dropna(how='all').reset_index()


# # # # # # # # # # # #         scaler = MinMaxScaler()
# # # # # # # # # # # #         vars_to_scale = list(agg_dict.keys())
# # # # # # # # # # # #         numeric_cols = [col for col in vars_to_scale if col in df_agg.columns]

# # # # # # # # # # # #         if not numeric_cols or df_agg.empty:
# # # # # # # # # # # #             warnings.warn(f"Données manquantes après agrégation ou colonnes numériques absentes pour {station_name} - période {periode}.")
# # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # #             continue

# # # # # # # # # # # #         df_agg_scaled = df_agg.copy()
# # # # # # # # # # # #         valid_cols_for_scaling = [col for col in numeric_cols if df_agg_scaled[col].notna().any()]
# # # # # # # # # # # #         if valid_cols_for_scaling:
# # # # # # # # # # # #             df_agg_scaled[valid_cols_for_scaling] = scaler.fit_transform(df_agg_scaled[valid_cols_for_scaling])
# # # # # # # # # # # #         else:
# # # # # # # # # # # #             warnings.warn(f"Aucune colonne avec des valeurs valides à normaliser pour {station_name} - période {periode}.")
# # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # #             continue


# # # # # # # # # # # #         df_melted = df_agg_scaled.melt(
# # # # # # # # # # # #             id_vars=['Datetime'],
# # # # # # # # # # # #             value_vars=numeric_cols,
# # # # # # # # # # # #             var_name='Variable',
# # # # # # # # # # # #             value_name='Valeur Normalisée'
# # # # # # # # # # # #         )

# # # # # # # # # # # #         if df_melted.empty or df_melted['Valeur Normalisée'].dropna().empty:
# # # # # # # # # # # #             warnings.warn(f"Données normalisées vides ou toutes NaN pour {station_name} - période {periode}.")
# # # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # # #             continue

# # # # # # # # # # # #         variable_color_map = {var: plot_palette[j] for j, var in enumerate(variables_to_plot)}

# # # # # # # # # # # #         sns.lineplot(
# # # # # # # # # # # #             data=df_melted,
# # # # # # # # # # # #             x='Datetime',
# # # # # # # # # # # #             y='Valeur Normalisée',
# # # # # # # # # # # #             hue='Variable',
# # # # # # # # # # # #             palette=variable_color_map,
# # # # # # # # # # # #             ax=ax,
# # # # # # # # # # # #             linewidth=2,
# # # # # # # # # # # #             marker='o' if periode == 'Annuelle' or len(df_melted['Datetime'].unique()) < 50 else None,
# # # # # # # # # # # #             markersize=6,
# # # # # # # # # # # #             legend='full'
# # # # # # # # # # # #         )

# # # # # # # # # # # #         ax.set_title(f"Évolution {periode.lower()}", fontsize=18, weight='bold', pad=15)
# # # # # # # # # # # #         ax.set_xlabel(label_x, fontsize=14, labelpad=15)
# # # # # # # # # # # #         ax.set_ylabel("Valeur Normalisée (0-1)", fontsize=14)

# # # # # # # # # # # #         ax.xaxis.set_major_formatter(date_format)
# # # # # # # # # # # #         if locator is not None:
# # # # # # # # # # # #             ax.xaxis.set_major_locator(locator)

# # # # # # # # # # # #         for label in ax.get_xticklabels():
# # # # # # # # # # # #             label.set_rotation(rot)
# # # # # # # # # # # #             label.set_ha('right' if rot > 0 and rot < 90 else 'center')
# # # # # # # # # # # #             label.set_fontsize(12)

# # # # # # # # # # # #         ax.tick_params(axis='x', which='both', bottom=True, labelbottom=True)
# # # # # # # # # # # #         ax.legend(
# # # # # # # # # # # #             title='Variables',
# # # # # # # # # # # #             loc='upper left',
# # # # # # # # # # # #             fontsize=10,
# # # # # # # # # # # #             title_fontsize=12,
# # # # # # # # # # # #             framealpha=0.9
# # # # # # # # # # # #         )
# # # # # # # # # # # #         ax.grid(True, alpha=0.2)

# # # # # # # # # # # #     for j in range(len(freq_configs), len(axs)):
# # # # # # # # # # # #         if j < len(axs) and axs[j] is not None:
# # # # # # # # # # # #             fig.delaxes(axs[j])

# # # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # # #     plt.subplots_adjust(top=0.94, hspace=0.4, wspace=0.15)
    
# # # # # # # # # # # #     return fig_to_base64(fig)



# # # # # # # # # # # import pandas as pd
# # # # # # # # # # # import numpy as np
# # # # # # # # # # # import matplotlib.pyplot as plt
# # # # # # # # # # # import seaborn as sns
# # # # # # # # # # # from matplotlib.dates import DateFormatter, MonthLocator, WeekdayLocator, YearLocator, DayLocator
# # # # # # # # # # # from pyproj import CRS, Transformer
# # # # # # # # # # # import pytz
# # # # # # # # # # # from astral.location import LocationInfo
# # # # # # # # # # # from astral import sun
# # # # # # # # # # # from sklearn.preprocessing import MinMaxScaler
# # # # # # # # # # # import io
# # # # # # # # # # # import base64
# # # # # # # # # # # import warnings
# # # # # # # # # # # import gdown
# # # # # # # # # # # import os

# # # # # # # # # # # # Importation des métadonnées des variables depuis le fichier de configuration
# # # # # # # # # # # # from config import METADATA_VARIABLES # Cette ligne reste commentée/supprimée pour éviter l'importation circulaire

# # # # # # # # # # # # Supprimer les avertissements de SettingWithCopyWarning de Pandas,
# # # # # # # # # # # # qui peuvent survenir avec df.loc et les chaînes d'opérations.
# # # # # # # # # # # pd.options.mode.chained_assignment = None # default='warn'

# # # # # # # # # # # # --- Fonctions de Traitement de Données ---

# # # # # # # # # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # # # # # # # # #     """
# # # # # # # # # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # # # # # # # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # # # # # # # # #     Args:
# # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # # # # # # # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # # # # # # # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # # # # # # # # #     Returns:
# # # # # # # # # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # # # # # # # # #     """
# # # # # # # # # # #     df_copy = df.copy() # Travailler sur une copie

# # # # # # # # # # #     # Cas spécifique pour VEA_SISSILI ou si seule la colonne 'Date' est présente
# # # # # # # # # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not all(col in df_copy.columns for col in ['Year', 'Month', 'Day'])):
# # # # # # # # # # #         try:
# # # # # # # # # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # # # # # # # # #         except Exception as e:
# # # # # # # # # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime: {e}")
# # # # # # # # # # #             df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # # # # # # # # #     else:
# # # # # # # # # # #         # Logique pour les colonnes Year, Month, Day, Hour, Minute
# # # # # # # # # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # # # # # # # # #         # Convertir les colonnes en numérique, forçant les erreurs en NaN
# # # # # # # # # # #         for col in date_cols:
# # # # # # # # # # #             if col in df_copy.columns:
# # # # # # # # # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # # # # # # # # #         # Supprimer les lignes où des valeurs temporelles essentielles sont manquantes
# # # # # # # # # # #         # (seulement les colonnes qui existent)
# # # # # # # # # # #         existing_date_cols = [col for col in date_cols if col in df_copy.columns]
# # # # # # # # # # #         if existing_date_cols:
# # # # # # # # # # #             df_copy = df_copy.dropna(subset=existing_date_cols)

# # # # # # # # # # #         # Créer la colonne Datetime si toutes les colonnes nécessaires sont présentes
# # # # # # # # # # #         if all(col in df_copy.columns for col in date_cols):
# # # # # # # # # # #             try:
# # # # # # # # # # #                 df_copy['Datetime'] = pd.to_datetime(df_copy[date_cols])
# # # # # # # # # # #             except Exception as e:
# # # # # # # # # # #                 warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées: {e}")
# # # # # # # # # # #                 df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # # # # # # # # #         else:
# # # # # # # # # # #             warnings.warn("Colonnes temporelles (Year, Month, Day, Hour, Minute) ou 'Date' manquantes. La colonne 'Datetime' n'a pas pu être créée.")
# # # # # # # # # # #             df_copy['Datetime'] = pd.NaT

# # # # # # # # # # #     # Extraire/recréer les composantes de date/heure si Datetime a été créé
# # # # # # # # # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # # # # # # # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # # # # # # # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # # # # # # # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # # # # # # # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # # # # # # # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # # # # # # # # #         df_copy['Date'] = df_copy['Datetime'].dt.date
# # # # # # # # # # #     else:
# # # # # # # # # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création.")

# # # # # # # # # # #     return df_copy

# # # # # # # # # # # def check_compatibility_for_merge(df_list: list[pd.DataFrame]) -> tuple[bool, str]:
# # # # # # # # # # #     """
# # # # # # # # # # #     Vérifie si une liste de DataFrames est compatible pour la fusion (concaténation).
# # # # # # # # # # #     Les DataFrames sont considérés compatibles si :
# # # # # # # # # # #     - Ils ont le même nombre de colonnes.
# # # # # # # # # # #     - Ils ont les mêmes noms de colonnes.

# # # # # # # # # # #     Args:
# # # # # # # # # # #         df_list (list): Une liste de pandas DataFrames.

# # # # # # # # # # #     Returns:
# # # # # # # # # # #         tuple: (bool, str) - True si compatible, False sinon, et un message d'erreur.
# # # # # # # # # # #     """
# # # # # # # # # # #     if not df_list:
# # # # # # # # # # #         return False, "La liste de DataFrames est vide."
# # # # # # # # # # #     if len(df_list) == 1:
# # # # # # # # # # #         return True, "Un seul DataFrame, pas de fusion nécessaire."

# # # # # # # # # # #     first_df = df_list[0]
# # # # # # # # # # #     num_cols_first = first_df.shape[1]
# # # # # # # # # # #     cols_first = set(first_df.columns)

# # # # # # # # # # #     for i, df in enumerate(df_list[1:]):
# # # # # # # # # # #         if df.shape[1] != num_cols_first:
# # # # # # # # # # #             return False, f"Le DataFrame {i+2} a {df.shape[1]} colonnes, le premier en a {num_cols_first}. Fusion impossible."
# # # # # # # # # # #         if set(df.columns) != cols_first:
# # # # # # # # # # #             missing_in_current = list(cols_first - set(df.columns))
# # # # # # # # # # #             extra_in_current = list(set(df.columns) - cols_first)
            
# # # # # # # # # # #             error_msg = f"Les colonnes du DataFrame {i+2} ne correspondent pas à celles du premier."
# # # # # # # # # # #             if missing_in_current:
# # # # # # # # # # #                 error_msg += f" Manquantes dans le DataFrame {i+2}: {missing_in_current}."
# # # # # # # # # # #             if extra_in_current:
# # # # # # # # # # #                 error_msg += f" Supplémentaires dans le DataFrame {i+2}: {extra_in_current}."
# # # # # # # # # # #             return False, error_msg
# # # # # # # # # # #     return True, "Tous les DataFrames sont compatibles pour la fusion."

# # # # # # # # # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # #     """
# # # # # # # # # # #     Crée une colonne 'Rain_mm' comme moyenne des mesures de deux pluviomètres.

# # # # # # # # # # #     Paramètres:
# # # # # # # # # # #     -----------
# # # # # # # # # # #     df : pandas.DataFrame
# # # # # # # # # # #         DataFrame contenant au minimum les colonnes 'Rain_01_mm' et 'Rain_02_mm'

# # # # # # # # # # #     Retourne:
# # # # # # # # # # #     --------
# # # # # # # # # # #     pandas.DataFrame
# # # # # # # # # # #         Le même DataFrame avec une nouvelle colonne 'Rain_mm' ajoutée
# # # # # # # # # # #     """
# # # # # # # # # # #     df_copy = df.copy()
# # # # # # # # # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # # # # # # # # #         df_copy['Rain_mm'] = df_copy[['Rain_01_mm', 'Rain_02_mm']].mean(axis=1)
# # # # # # # # # # #     else:
# # # # # # # # # # #         df_copy['Rain_mm'] = np.nan
# # # # # # # # # # #     return df_copy

# # # # # # # # # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # #     """
# # # # # # # # # # #     Supprime les doublons complets et nettoie les lignes avec des valeurs temporelles manquantes.
# # # # # # # # # # #     Gère les conflits où 'Datetime' est à la fois une colonne et un index.

# # # # # # # # # # #     Args:
# # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée.

# # # # # # # # # # #     Returns:
# # # # # # # # # # #         pd.DataFrame: DataFrame traité.
# # # # # # # # # # #     """
# # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # #     # Suppression des doublons complets
# # # # # # # # # # #     dup_count = df_copy.duplicated().sum()
# # # # # # # # # # #     if dup_count > 0:
# # # # # # # # # # #         warnings.warn(f"Suppression de {dup_count} doublons complets.")
# # # # # # # # # # #         df_copy = df_copy.drop_duplicates()

# # # # # # # # # # #     # Nettoyage des colonnes temporelles si elles existent
# # # # # # # # # # #     time_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
# # # # # # # # # # #     existing_time_cols = [col for col in time_cols if col in df_copy.columns]
# # # # # # # # # # #     if existing_time_cols:
# # # # # # # # # # #         initial_rows = len(df_copy)
# # # # # # # # # # #         df_copy = df_copy.dropna(subset=existing_time_cols)
# # # # # # # # # # #         removed_time_rows = initial_rows - len(df_copy)
# # # # # # # # # # #         if removed_time_rows > 0:
# # # # # # # # # # #             warnings.warn(f"Suppression de {removed_time_rows} lignes avec valeurs temporelles manquantes.")

# # # # # # # # # # #     # Gestion des conflits Datetime colonne/index
# # # # # # # # # # #     if 'Datetime' in df_copy.columns and isinstance(df_copy.index, pd.DatetimeIndex) and df_copy.index.name == 'Datetime':
# # # # # # # # # # #         pass
# # # # # # # # # # #     elif 'Datetime' in df_copy.columns and 'Datetime' in df_copy.index.names:
# # # # # # # # # # #         df_copy = df_copy.reset_index(drop=True)
    
# # # # # # # # # # #     return df_copy

# # # # # # # # # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # #     """
# # # # # # # # # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # # # # # # # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # # # # # # # # #     Args:
# # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # # # # # # # # #     Returns:
# # # # # # # # # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # # # # # # # # #     """
# # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # # # # # # # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # # # # # # # # #         raise ValueError(
# # # # # # # # # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # # # # # # # # #         )

# # # # # # # # # # #     def convert_row(row):
# # # # # # # # # # #         try:
# # # # # # # # # # #             zone = int(row['zone'])
# # # # # # # # # # #             hemisphere = str(row['hemisphere']).upper()
# # # # # # # # # # #             is_northern = hemisphere == 'N'

# # # # # # # # # # #             proj_utm = CRS.from_proj4(
# # # # # # # # # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # # # # # # # # #             )
# # # # # # # # # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # # # # # # # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # # # # # # # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # # # # # # # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # # # # # # # # #         except Exception as e:
# # # # # # # # # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # # # # # # # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # # # # # # # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # # # # # # # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # # # # # # # # #     return df_copy


# # # # # # # # # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # # # # # # # # #     """
# # # # # # # # # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # # # # # # # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # # # # # # # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # # # # # # # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # # # # # # # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
# # # # # # # # # # #     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

# # # # # # # # # # #     Returns:
# # # # # # # # # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # # # # # # # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # # # # # # # # #     """
# # # # # # # # # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # # # # # # # # #     data_dir = 'data'
# # # # # # # # # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # # # # # # # # #     files_info = [
# # # # # # # # # # #         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
# # # # # # # # # # #         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
# # # # # # # # # # #         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
# # # # # # # # # # #     ]

# # # # # # # # # # #     loaded_dfs = []

# # # # # # # # # # #     for file_info in files_info:
# # # # # # # # # # #         output_file_path = os.path.join(data_dir, file_info['name'])
        
# # # # # # # # # # #         if not os.path.exists(output_file_path):
# # # # # # # # # # #             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
# # # # # # # # # # #             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
# # # # # # # # # # #             print(f"Téléchargement de {file_info['bassin']} terminé.")
# # # # # # # # # # #         else:
# # # # # # # # # # #             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
# # # # # # # # # # #         loaded_dfs.append(pd.read_excel(output_file_path))

# # # # # # # # # # #     vea_sissili_bassin = loaded_dfs[0]
# # # # # # # # # # #     dano_bassin = loaded_dfs[1]
# # # # # # # # # # #     dassari_bassin = loaded_dfs[2]

# # # # # # # # # # #     # 4. Prétraitement des DataFrames (votre code original)
# # # # # # # # # # #     print("Début du prétraitement des données de stations...")
    
# # # # # # # # # # #     # Vea Sissili
# # # # # # # # # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # # # # # # # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # # # # # # # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # # # # # # # # #     # Dassari
# # # # # # # # # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # # # # # # # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # # # # # # # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # # # # # # # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # # # # # # # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # # # # # # # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # # # # # # # # #     dano_bassin['zone'] = 30
# # # # # # # # # # #     dano_bassin['hemisphere'] = 'N'
# # # # # # # # # # #     dassari_bassin['zone'] = 31
# # # # # # # # # # #     dassari_bassin['hemisphere'] = 'N'

# # # # # # # # # # #     # Application de la fonction de conversion UTM vers GPS
# # # # # # # # # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # # # # # # # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # # # # # # # # #     # Ajout des fuseaux horaires
# # # # # # # # # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # # # # # # # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # # # # # # # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # # # # # # # # #     # Fusion de tous les bassins
# # # # # # # # # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # # # # # # # # #     # Renommer 'Name' en 'Station'
# # # # # # # # # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # # # # # # # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # # # # # # # # #     initial_rows_count = len(bassins)
# # # # # # # # # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # # # # # # # # #     if len(bassins) < initial_rows_count:
# # # # # # # # # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # # # # # # # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # # # # # # # # #     if 'Station' in bassins.columns:
# # # # # # # # # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # # # # # # # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # # # # # # # # #     return bassins


# # # # # # # # # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # #     """
# # # # # # # # # # #     Effectue toutes les interpolations météorologiques en une seule passe,
# # # # # # # # # # #     en intégrant le calcul automatique du lever et du coucher du soleil
# # # # # # # # # # #     pour une gestion plus précise de la radiation solaire.

# # # # # # # # # # #     Args:
# # # # # # # # # # #         df (pd.DataFrame): Le DataFrame d'entrée. Son index DOIT ÊTRE un DatetimeIndex.
# # # # # # # # # # #                            Il doit également contenir une colonne 'Station'.
# # # # # # # # # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # # # # # # # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # # # # # # # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # # # # # # # # #     Returns:
# # # # # # # # # # #         pd.DataFrame: Le DataFrame original avec les données interpolées et
# # # # # # # # # # #                       la colonne 'Is_Daylight' calculée, ainsi que la durée du jour.
# # # # # # # # # # #     """
# # # # # # # # # # #     df_processed = df.copy()

# # # # # # # # # # #     # VÉRIFICATIONS CRITIQUES POUR L'INDEX TEMPOREL
# # # # # # # # # # #     # S'assurer que l'index est bien un DatetimeIndex
# # # # # # # # # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # # # # # # # # #         raise TypeError("L'index du DataFrame doit être de type DatetimeIndex pour l'interpolation.")

# # # # # # # # # # #     # Supprime les lignes où l'index Datetime est NaT (valeur manquante)
# # # # # # # # # # #     # C'est la source de l'erreur précédente car on cherchait une colonne 'Datetime'
# # # # # # # # # # #     # alors qu'elle était l'index.
# # # # # # # # # # #     initial_rows = len(df_processed)
# # # # # # # # # # #     df_processed = df_processed[df_processed.index.notna()]
# # # # # # # # # # #     if len(df_processed) == 0:
# # # # # # # # # # #         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
# # # # # # # # # # #     if initial_rows - len(df_processed) > 0:
# # # # # # # # # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")

# # # # # # # # # # #     # --- Pré-vérification et préparation des données GPS (inchangée) ---
# # # # # # # # # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # # # # # # # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # # # # # # # # #         raise ValueError(
# # # # # # # # # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # # # # # # # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # # # # # # # # #         )

# # # # # # # # # # #     if not df_gps['Station'].is_unique:
# # # # # # # # # # #         warnings.warn("La colonne 'Station' dans df_gps contient des noms de station dupliqués. Suppression des doublons.")
# # # # # # # # # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # # # # # # # # #     else:
# # # # # # # # # # #         df_gps_unique = df_gps.copy()

# # # # # # # # # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # # # # # # # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # # # # # # # # #     for col in numerical_cols:
# # # # # # # # # # #         if col in df_processed.columns:
# # # # # # # # # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # # # # # # # # #     df_processed_parts = []

# # # # # # # # # # #     for station_name, group in df_processed.groupby('Station'):
# # # # # # # # # # #         group_copy = group.copy()

# # # # # # # # # # #         # Extraire les infos GPS pour la station actuelle
# # # # # # # # # # #         gps_data = gps_info_dict.get(station_name)

# # # # # # # # # # #         # L'index est déjà un DatetimeIndex à ce stade grâce aux vérifications précédentes
# # # # # # # # # # #         # Mais le tri est toujours une bonne pratique
# # # # # # # # # # #         group_copy = group_copy.sort_index()


# # # # # # # # # # #         # Calcul de lever/coucher du soleil ou fallback
# # # # # # # # # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # # # # # # # # #             lat = gps_data['Lat']
# # # # # # # # # # #             long = gps_data['Long']
# # # # # # # # # # #             timezone_str = gps_data['Timezone']

# # # # # # # # # # #             try:
# # # # # # # # # # #                 tz = pytz.timezone(timezone_str)
# # # # # # # # # # #                 # S'assurer que l'index du groupe est dans le bon fuseau horaire
# # # # # # # # # # #                 if group_copy.index.tz is None:
# # # # # # # # # # #                     group_copy.index = group_copy.index.tz_localize(tz, ambiguous='NaT', nonexistent='NaT')
# # # # # # # # # # #                 else:
# # # # # # # # # # #                     group_copy.index = group_copy.index.tz_convert(tz)

# # # # # # # # # # #                 daily_sun_info = {}
# # # # # # # # # # #                 for date_only in group_copy.index.normalize().unique():
# # # # # # # # # # #                     s = sun.sun(LocationInfo(station_name, "Site", timezone_str, lat, long).observer,
# # # # # # # # # # #                                 date=date_only, tzinfo=tz)
# # # # # # # # # # #                     daily_sun_info[date_only.date()] = {
# # # # # # # # # # #                         'sunrise': s['sunrise'],
# # # # # # # # # # #                         'sunset': s['sunset']
# # # # # # # # # # #                     }

# # # # # # # # # # #                 group_copy.loc[:, 'sunrise_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunrise'))
# # # # # # # # # # #                 group_copy.loc[:, 'sunset_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunset'))

# # # # # # # # # # #                 # Assurer que les colonnes sunrise/sunset sont dans le même fuseau horaire que l'index pour la comparaison
# # # # # # # # # # #                 if group_copy.index.tz is not None:
# # # # # # # # # # #                      group_copy['sunrise_time_local'] = group_copy['sunrise_time_local'].dt.tz_convert(group_copy.index.tz)
# # # # # # # # # # #                      group_copy['sunset_time_local'] = group_copy['sunset_time_local'].dt.tz_convert(group_copy.index.tz)

# # # # # # # # # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index >= group_copy['sunrise_time_local']) & \
# # # # # # # # # # #                                                   (group_copy.index < group_copy['sunset_time_local'])

# # # # # # # # # # #                 daylight_timedelta = group_copy['sunset_time_local'] - group_copy['sunrise_time_local']
# # # # # # # # # # #                 group_copy.loc[:, 'Daylight_Duration'] = daylight_timedelta.apply(
# # # # # # # # # # #                     lambda td: (pd.to_datetime('00:00:00') + td).strftime('%H:%M:%S') if pd.notna(td) else np.nan
# # # # # # # # # # #                 )
# # # # # # # # # # #             except Exception as e:
# # # # # # # # # # #                 warnings.warn(f"Erreur Astral pour {station_name}: {e}. Utilisation du fallback 7h-18h.")
# # # # # # # # # # #                 if group_copy.index.tz is None:
# # # # # # # # # # #                     group_copy.index = group_copy.index.tz_localize('UTC') # Fallback pour localiser l'index
# # # # # # # # # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # # # # # # # # #                 group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # # # # # # # # #         else:
# # # # # # # # # # #             warnings.warn(f"Coordonnées/Fuseau horaire manquants/invalides pour {station_name}. Utilisation du fallback 7h-18h.")
# # # # # # # # # # #             if group_copy.index.tz is None:
# # # # # # # # # # #                 group_copy.index = group_copy.index.tz_localize('UTC')
# # # # # # # # # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # # # # # # # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"

# # # # # # # # # # #         df_processed_parts.append(group_copy)

# # # # # # # # # # #     if not df_processed_parts:
# # # # # # # # # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # # # # # # # # #     df_final = pd.concat(df_processed_parts)
# # # # # # # # # # #     df_final = df_final.sort_index()
# # # # # # # # # # #     df_final.index.name = 'Datetime'

# # # # # # # # # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date',
# # # # # # # # # # #                                   'sunrise_time_local', 'sunset_time_local']
# # # # # # # # # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # # # # # # # # #     # Gestion intelligente de Rain_mm
# # # # # # # # # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # # # # # # # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # # # # # # # # #             df_final = create_rain_mm(df_final)
# # # # # # # # # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # # # # # # # # #         else:
# # # # # # # # # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # # # # # # # # #             df_final['Rain_mm'] = np.nan


# # # # # # # # # # #     # Interpolation standard et bornage pour les variables numériques
# # # # # # # # # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # # #                      'Wind_Dir_Deg']

# # # # # # # # # # #     for var in standard_vars:
# # # # # # # # # # #         if var in df_final.columns:
# # # # # # # # # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # # # # # # # # #             if var in limits:
# # # # # # # # # # #                 min_val = limits[var]['min']
# # # # # # # # # # #                 max_val = limits[var]['max']
# # # # # # # # # # #                 initial_nan_count = df_final[var].isna().sum()
# # # # # # # # # # #                 df_final.loc[(df_final[var] < min_val) | (df_final[var] > max_val), var] = np.nan
# # # # # # # # # # #                 if df_final[var].isna().sum() > initial_nan_count:
# # # # # # # # # # #                     warnings.warn(f"Remplacement de {df_final[var].isna().sum() - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # # # # # # # # #             # Interpolation
# # # # # # # # # # #             df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # # # # # # # # #             df_final[var] = df_final[var].bfill().ffill() # Pour les NaNs aux extrémités

# # # # # # # # # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # # # # # # # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # # # # # # # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # # # # # # # # #         if 'Solar_R_W/m^2' in limits:
# # # # # # # # # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # # # # # # # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # # # # # # # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # # # # # # # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # # # # # # # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # # # # # # # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # # # # # # # # #         if 'Is_Daylight' in df_final.columns:
# # # # # # # # # # #             # Radiation positive hors du jour -> 0
# # # # # # # # # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # # # # # # # # #             # Zéros suspects en journée sans pluie -> NaN
# # # # # # # # # # #             if 'Rain_mm' in df_final.columns:
# # # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # # # # # # # # #             else:
# # # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # # # # # # # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # # # # # # # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # # # # # # # # #             # Interpolation dans les périodes de jour
# # # # # # # # # # #             is_day = df_final['Is_Daylight']
# # # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # # # # # # # # #             # Les NaNs restants hors des heures de jour -> 0
# # # # # # # # # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # # # # # # # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # # # # # # # # #         else:
# # # # # # # # # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # # # # # # # # #             df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()

# # # # # # # # # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # # # # # # # # #     missing_after_interp = df_final.isna().sum()
# # # # # # # # # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # # # # # # # # #     if not columns_with_missing.empty:
# # # # # # # # # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # # # # # # # # #     else:
# # # # # # # # # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # # # # # # # # #     return df_final

# # # # # # # # # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # # # # # # # # #     """
# # # # # # # # # # #     Traite les valeurs aberrantes (outliers) dans un DataFrame de données météorologiques
# # # # # # # # # # #     en appliquant la méthode de l'écart interquartile (IQR) pour limiter les valeurs extrêmes.

# # # # # # # # # # #     Args:
# # # # # # # # # # #         df (pd.DataFrame): DataFrame contenant les données météorologiques à traiter.
# # # # # # # # # # #         limits (dict): Dictionnaire des variables à traiter (clés) avec leurs limites potentielles.
# # # # # # # # # # #                        (Note: les limites min/max du dict ne sont pas utilisées pour l'IQR,
# # # # # # # # # # #                        mais les clés du dict définissent les variables à analyser).

# # # # # # # # # # #     Returns:
# # # # # # # # # # #         pd.DataFrame: Copie du DataFrame avec les outliers corrigés par la méthode IQR.
# # # # # # # # # # #     """
# # # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # # #     for var in limits.keys(): # On utilise les clés de limits pour savoir quelles variables traiter
# # # # # # # # # # #         if var in df_copy.columns:
# # # # # # # # # # #             # S'assurer que la colonne est numérique
# # # # # # # # # # #             df_copy[var] = pd.to_numeric(df_copy[var], errors='coerce')

# # # # # # # # # # #             # Calcul des quartiles
# # # # # # # # # # #             Q1 = df_copy[var].quantile(0.25)
# # # # # # # # # # #             Q3 = df_copy[var].quantile(0.75)
# # # # # # # # # # #             IQR = Q3 - Q1

# # # # # # # # # # #             # Bornes IQR
# # # # # # # # # # #             borne_inf = Q1 - 1.5 * IQR
# # # # # # # # # # #             borne_sup = Q3 + 1.5 * IQR

# # # # # # # # # # #             # Application du bornage (clipping)
# # # # # # # # # # #             initial_outliers_count = df_copy[(df_copy[var] < borne_inf) | (df_copy[var] > borne_sup)][var].count()
# # # # # # # # # # #             df_copy[var] = df_copy[var].clip(lower=borne_inf, upper=borne_sup)
# # # # # # # # # # #             if initial_outliers_count > 0:
# # # # # # # # # # #                 warnings.warn(f"Outliers traités par IQR pour '{var}'. {initial_outliers_count} valeurs ajustées.")
# # # # # # # # # # #     return df_copy

# # # # # # # # # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # # #     """
# # # # # # # # # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables météorologiques.

# # # # # # # # # # #     Args:
# # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec 'Datetime' comme index.

# # # # # # # # # # #     Returns:
# # # # # # # # # # #         pd.DataFrame: DataFrame contenant les statistiques journalières.
# # # # # # # # # # #     """
# # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # #         raise TypeError("L'index du DataFrame doit être de type DatetimeIndex.")

# # # # # # # # # # #     # Définir les variables pour lesquelles nous voulons des statistiques
# # # # # # # # # # #     # et leurs méthodes d'agrégation.
# # # # # # # # # # #     # Rain_mm doit être sommée, les autres moyennées.
# # # # # # # # # # #     # Wind_Dir_Deg est un cas particulier, souvent on calcule une direction moyenne vectorielle
# # # # # # # # # # #     # mais pour des stats simples, une moyenne arithmétique peut être misleading.
# # # # # # # # # # #     # Pour cet exemple, on le met en moyenne, mais gardez cela à l'esprit.
# # # # # # # # # # #     agg_funcs = {
# # # # # # # # # # #         'Air_Temp_Deg_C': 'mean',
# # # # # # # # # # #         'Rel_H_%': 'mean',
# # # # # # # # # # #         'BP_mbar_Avg': 'mean',
# # # # # # # # # # #         'Rain_mm': 'sum', # Précipitation cumulée
# # # # # # # # # # #         'Wind_Sp_m/sec': 'mean',
# # # # # # # # # # #         'Solar_R_W/m^2': 'mean',
# # # # # # # # # # #         'Wind_Dir_Deg': 'mean' # Moyenne simple, à interpréter avec prudence
# # # # # # # # # # #     }

# # # # # # # # # # #     # Filtrer les colonnes qui existent dans le DataFrame
# # # # # # # # # # #     existing_cols_to_agg = {col: func for col, func in agg_funcs.items() if col in df.columns}

# # # # # # # # # # #     if not existing_cols_to_agg:
# # # # # # # # # # #         warnings.warn("Aucune colonne pertinente trouvée pour les statistiques journalières.")
# # # # # # # # # # #         return pd.DataFrame()

# # # # # # # # # # #     # Regrouper par jour (en utilisant l'index Datetime) et par station, puis agréger
# # # # # # # # # # #     daily_summary = df.groupby([df.index.date, 'Station']).agg(
# # # # # # # # # # #         **{f"{col}_{func}": (col, func) for col, func in existing_cols_to_agg.items()}
# # # # # # # # # # #     )
# # # # # # # # # # #     daily_summary.index.names = ['Date', 'Station']
# # # # # # # # # # #     return daily_summary.reset_index()


# # # # # # # # # # # # --- Fonctions de Visualisation ---

# # # # # # # # # # # # Fonction utilitaire pour convertir une figure Matplotlib en base64
# # # # # # # # # # # def fig_to_base64(fig: plt.Figure) -> str | None:
# # # # # # # # # # #     """
# # # # # # # # # # #     Convertit une figure Matplotlib en une chaîne Base64 encodée (format PNG).

# # # # # # # # # # #     Args:
# # # # # # # # # # #         fig (plt.Figure): La figure Matplotlib à convertir.

# # # # # # # # # # #     Returns:
# # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG, ou None si la figure est invalide.
# # # # # # # # # # #     """
# # # # # # # # # # #     if fig is None:
# # # # # # # # # # #         return None
# # # # # # # # # # #     buf = io.BytesIO()
# # # # # # # # # # #     try:
# # # # # # # # # # #         fig.savefig(buf, format='png', bbox_inches='tight')
# # # # # # # # # # #         plt.close(fig) # Fermer la figure pour libérer la mémoire
# # # # # # # # # # #         data = base64.b64encode(buf.getbuffer()).decode("ascii")
# # # # # # # # # # #         return data
# # # # # # # # # # #     except Exception as e:
# # # # # # # # # # #         warnings.warn(f"Erreur lors de la conversion de la figure en base64: {e}")
# # # # # # # # # # #         plt.close(fig) # Assurez-vous de fermer la figure même en cas d'erreur
# # # # # # # # # # #         return None

# # # # # # # # # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station_name: str, variable_name: str, periode: str, custom_palette: list = None, metadata_variables: dict = None) -> str | None:
# # # # # # # # # # #     """
# # # # # # # # # # #     Génère un seul graphique de l'évolution d'une variable météorologique pour une station
# # # # # # # # # # #     spécifique et une période donnée.

# # # # # # # # # # #     Args:
# # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée contenant les données météorologiques.
# # # # # # # # # # #                            Doit inclure 'Datetime' comme index et 'Station'.
# # # # # # # # # # #         station_name (str): Nom de la station à visualiser.
# # # # # # # # # # #         variable_name (str): Nom de la variable à visualiser.
# # # # # # # # # # #         periode (str): Période d'agrégation ('Journalière', 'Hebdomadaire', 'Mensuelle', 'Annuelle').
# # # # # # # # # # #         custom_palette (list, optional): Palette de couleurs personnalisée.
# # # # # # # # # # #         metadata_variables (dict, optional): Dictionnaire des métadonnées des variables.
# # # # # # # # # # #                                             Si None, un avertissement sera émis.

# # # # # # # # # # #     Returns:
# # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # #     """
# # # # # # # # # # #     if metadata_variables is None:
# # # # # # # # # # #         warnings.warn("metadata_variables n'a pas été fourni à generer_graphique_par_variable_et_periode. Certaines informations pourraient être manquantes.")
# # # # # # # # # # #         # Fallback si METADATA_VARIABLES n'est pas passé (moins robuste)
# # # # # # # # # # #         metadata_variables = {
# # # # # # # # # # #             'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'agg_type': 'cumul'},
# # # # # # # # # # #             'Air_Temp_Deg_C': {'Nom': "Température ", 'Unite': "°C", 'agg_type': 'moyenne'},
# # # # # # # # # # #             'Rel_H_%': {'Nom': "Humidité Relative", 'Unite': "%", 'agg_type': 'moyenne'},
# # # # # # # # # # #             'Solar_R_W/m^2': {'Nom': "Radiation Solaire", 'Unite': "W/m²", 'agg_type': 'moyenne'},
# # # # # # # # # # #             'Wind_Sp_m/sec': {'Nom': "Vitesse du Vent", 'Unite': "m/s", 'agg_type': 'moyenne'},
# # # # # # # # # # #             'Wind_Dir_Deg': {'Nom': "Direction du Vent", 'Unite': "°", 'agg_type': 'moyenne'},
# # # # # # # # # # #             'BP_mbar_Avg': {'Nom': "Pression Atmospherique moyenne", 'Unite': "mbar", 'agg_type': 'moyenne'}
# # # # # # # # # # #         }


# # # # # # # # # # #     if variable_name not in metadata_variables or variable_name not in df.columns:
# # # # # # # # # # #         warnings.warn(f"Variable '{variable_name}' non valide ou absente du DataFrame.")
# # # # # # # # # # #         return None

# # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # #         return None

# # # # # # # # # # #     df_station = df[df['Station'] == station_name].copy()
# # # # # # # # # # #     if df_station.empty:
# # # # # # # # # # #         warnings.warn(f"Aucune donnée pour la station '{station_name}'.")
# # # # # # # # # # #         return None

# # # # # # # # # # #     df_station[variable_name] = pd.to_numeric(df_station[variable_name], errors='coerce')

# # # # # # # # # # #     annees_uniques = df_station.index.year.nunique()
# # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # #         freq_configs = {
# # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # #         }
# # # # # # # # # # #     else:
# # # # # # # # # # #         freq_configs = {
# # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # #         }

# # # # # # # # # # #     if periode not in freq_configs:
# # # # # # # # # # #         warnings.warn(f"Période '{periode}' non valide.")
# # # # # # # # # # #         return None

# # # # # # # # # # #     freq, date_fmt, rot, locator, label_x = freq_configs[periode]
# # # # # # # # # # #     meta = metadata_variables[variable_name]
# # # # # # # # # # #     nom_complet = f"{meta['Nom']} ({meta['Unite']})"
# # # # # # # # # # #     agg_func = 'sum' if meta['agg_type'] == 'cumul' else 'mean'

# # # # # # # # # # #     # Déterminer la couleur de la station
# # # # # # # # # # #     stations_list = sorted(df['Station'].unique())
# # # # # # # # # # #     try:
# # # # # # # # # # #         station_idx = stations_list.index(station_name)
# # # # # # # # # # #     except ValueError:
# # # # # # # # # # #         warnings.warn(f"Station '{station_name}' non trouvée dans la liste des stations uniques.")
# # # # # # # # # # #         station_idx = 0 # Fallback
    
# # # # # # # # # # #     palette = custom_palette if custom_palette and len(custom_palette) > station_idx else sns.color_palette("husl", len(stations_list))
# # # # # # # # # # #     station_color = palette[station_idx] if len(palette) > station_idx else 'blue' # Fallback couleur

# # # # # # # # # # #     fig, ax = plt.subplots(figsize=(12, 7))

# # # # # # # # # # #     df_temp = df_station.copy()

# # # # # # # # # # #     if variable_name == 'Solar_R_W/m^2':
# # # # # # # # # # #         if 'Is_Daylight' in df_temp.columns:
# # # # # # # # # # #             df_temp = df_temp[df_temp['Is_Daylight']].copy()
# # # # # # # # # # #         else:
# # # # # # # # # # #             warnings.warn("Avertissement: Colonne 'Is_Daylight' manquante pour la radiation solaire. Filtrage horaire non appliqué.")
# # # # # # # # # # #             df_temp = df_temp[(df_temp.index.hour >= 7) & (df_temp.index.hour <= 18)]
            
# # # # # # # # # # #     elif variable_name == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in df_temp.columns:
# # # # # # # # # # #         df_temp = df_temp[(df_temp['Wind_Sp_m/sec'] > 0) & (df_temp['Wind_Dir_Deg'].notna())]

# # # # # # # # # # #     with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # #         df_agg = df_temp.groupby(pd.Grouper(freq=freq))[variable_name].agg(agg_func).reset_index()
# # # # # # # # # # #     df_agg.rename(columns={'index': 'Datetime'}, inplace=True)

# # # # # # # # # # #     if df_agg.empty:
# # # # # # # # # # #         plt.close(fig)
# # # # # # # # # # #         warnings.warn(f"Aucune donnée agrégée pour {variable_name} à la période {periode} pour {station_name}.")
# # # # # # # # # # #         return None

# # # # # # # # # # #     sns.lineplot(
# # # # # # # # # # #         data=df_agg,
# # # # # # # # # # #         x='Datetime',
# # # # # # # # # # #         y=variable_name,
# # # # # # # # # # #         color=station_color,
# # # # # # # # # # #         linewidth=2,
# # # # # # # # # # #         marker='o' if periode == 'Annuelle' else None,
# # # # # # # # # # #         markersize=6 if periode == 'Annuelle' else 0,
# # # # # # # # # # #         ax=ax
# # # # # # # # # # #     )

# # # # # # # # # # #     ax.set_title(f"Évolution {periode.lower()} de {nom_complet} pour {station_name}", pad=10, fontsize=14)
# # # # # # # # # # #     ax.set_ylabel(nom_complet, fontsize=12)
# # # # # # # # # # #     ax.set_xlabel(label_x, fontsize=10)

# # # # # # # # # # #     ax.xaxis.set_major_formatter(date_fmt)
# # # # # # # # # # #     if locator:
# # # # # # # # # # #         ax.xaxis.set_major_locator(locator)

# # # # # # # # # # #     for label in ax.get_xticklabels():
# # # # # # # # # # #         label.set_rotation(rot)
# # # # # # # # # # #         label.set_ha('right' if rot > 45 else 'center')

# # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # def generer_graphique_comparatif(df: pd.DataFrame, variable_name: str, periode: str, custom_palette: list = None, metadata_variables: dict = None) -> str | None:
# # # # # # # # # # #     """
# # # # # # # # # # #     Génère un seul graphique comparatif de l'évolution d'une variable météorologique
# # # # # # # # # # #     entre toutes les stations pour une période donnée.

# # # # # # # # # # #     Args:
# # # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée contenant les données météorologiques.
# # # # # # # # # # #                            Doit inclure 'Datetime' comme index et 'Station'.
# # # # # # # # # # #         variable_name (str): Nom de la variable à visualiser.
# # # # # # # # # # #         periode (str): Période d'agrégation ('Journalière', 'Hebdomadaire', 'Mensuelle', 'Annuelle').
# # # # # # # # # # #         custom_palette (list, optional): Palette de couleurs personnalisée.
# # # # # # # # # # #         metadata_variables (dict, optional): Dictionnaire des métadonnées des variables.
# # # # # # # # # # #                                             Si None, un avertissement sera émis.

# # # # # # # # # # #     Returns:
# # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # #     """
# # # # # # # # # # #     if metadata_variables is None:
# # # # # # # # # # #         warnings.warn("metadata_variables n'a pas été fourni à generer_graphique_comparatif. Certaines informations pourraient être manquantes.")
# # # # # # # # # # #         # Fallback si METADATA_VARIABLES n'est pas passé (moins robuste)
# # # # # # # # # # #         metadata_variables = {
# # # # # # # # # # #             'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'is_rain': True},
# # # # # # # # # # #             'Air_Temp_Deg_C': {'Nom': "Température", 'Unite': "°C", 'is_rain': False},
# # # # # # # # # # #             'Rel_H_%': {'Nom': "Humidité", 'Unite': "%", 'is_rain': False},
# # # # # # # # # # #             'Solar_R_W/m^2': {'Nom': "Radiation solaire", 'Unite': "W/m²", 'is_rain': False},
# # # # # # # # # # #             'Wind_Sp_m/sec': {'Nom': "Vitesse du vent", 'Unite': "m/s", 'is_rain': False},
# # # # # # # # # # #             'Wind_Dir_Deg': {'Nom': "Direction du vent", 'Unite': "°", 'is_rain': False},
# # # # # # # # # # #             'BP_mbar_Avg': {'Nom': "Pression atmospherique moyenne", 'Unite': "mbar", 'is_rain': False}
# # # # # # # # # # #         }


# # # # # # # # # # #     if variable_name not in metadata_variables or variable_name not in df.columns:
# # # # # # # # # # #         warnings.warn(f"Variable '{variable_name}' non valide ou absente du DataFrame.")
# # # # # # # # # # #         return None

# # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # #         return None

# # # # # # # # # # #     annees_uniques = df.index.year.nunique()
# # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # #         freq_configs = {
# # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # #         }
# # # # # # # # # # #     else:
# # # # # # # # # # #         freq_configs = {
# # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # #         }

# # # # # # # # # # #     if periode not in freq_configs:
# # # # # # # # # # #         warnings.warn(f"Période '{periode}' non valide.")
# # # # # # # # # # #         return None

# # # # # # # # # # #     freq, date_fmt, rot, locator, label_x = freq_configs[periode]
# # # # # # # # # # #     meta = metadata_variables[variable_name]
# # # # # # # # # # #     nom_complet = f"{meta['Nom']} ({meta['Unite']})"
# # # # # # # # # # #     agg_text = " (cumul)" if meta['is_rain'] else " (moyenne)"
# # # # # # # # # # #     agg_func = 'sum' if meta['is_rain'] else 'mean'

# # # # # # # # # # #     stations = sorted(df['Station'].unique())
# # # # # # # # # # #     palette = custom_palette if custom_palette and len(custom_palette) >= len(stations) \
# # # # # # # # # # #               else sns.color_palette("husl", len(stations))

# # # # # # # # # # #     fig, ax = plt.subplots(figsize=(14, 8))

# # # # # # # # # # #     temp_df = df.copy()

# # # # # # # # # # #     if variable_name == 'Solar_R_W/m^2':
# # # # # # # # # # #         if 'Is_Daylight' in temp_df.columns:
# # # # # # # # # # #             temp_df = temp_df[temp_df['Is_Daylight']].copy()
# # # # # # # # # # #         else:
# # # # # # # # # # #             warnings.warn("Avertissement: Colonne 'Is_Daylight' manquante. Filtrage horaire par défaut (7h-18h) pour radiation solaire.")
# # # # # # # # # # #             temp_df = temp_df[(temp_df.index.hour >= 7) & (temp_df.index.hour <= 18)]
            
# # # # # # # # # # #     elif variable_name == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in temp_df.columns:
# # # # # # # # # # #         temp_df = temp_df[(temp_df['Wind_Sp_m/sec'] > 0) & (temp_df['Wind_Dir_Deg'].notna())]

# # # # # # # # # # #     with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # #         df_agg = temp_df.groupby(['Station', pd.Grouper(freq=freq)])[variable_name].agg(agg_func).reset_index()

# # # # # # # # # # #     if df_agg.empty:
# # # # # # # # # # #         plt.close(fig)
# # # # # # # # # # #         warnings.warn(f"Aucune donnée agrégée pour {variable_name} à la période {periode}.")
# # # # # # # # # # #         return None

# # # # # # # # # # #     sns.lineplot(
# # # # # # # # # # #         data=df_agg,
# # # # # # # # # # #         x='Datetime',
# # # # # # # # # # #         y=variable_name,
# # # # # # # # # # #         hue='Station',
# # # # # # # # # # #         palette=palette,
# # # # # # # # # # #         ax=ax,
# # # # # # # # # # #         linewidth=2,
# # # # # # # # # # #         marker='o' if periode == 'Annuelle' else None,
# # # # # # # # # # #         markersize=6 if periode == 'Annuelle' else 0,
# # # # # # # # # # #         legend='full'
# # # # # # # # # # #     )

# # # # # # # # # # #     ax.set_title(
# # # # # # # # # # #         f"Comparaison de {meta['Nom']} {agg_text} ({meta['Unite']}) - Période: {periode}",
# # # # # # # # # # #         fontsize=16,
# # # # # # # # # # #         pad=15,
# # # # # # # # # # #         weight='bold'
# # # # # # # # # # #     )
# # # # # # # # # # #     ax.set_xlabel(label_x, fontsize=12, labelpad=15)
# # # # # # # # # # #     ax.set_ylabel(nom_complet, fontsize=12)

# # # # # # # # # # #     ax.xaxis.set_major_formatter(date_fmt)
# # # # # # # # # # #     if locator:
# # # # # # # # # # #         ax.xaxis.set_major_locator(locator)

# # # # # # # # # # #     for label in ax.get_xticklabels():
# # # # # # # # # # #         label.set_rotation(rot)
# # # # # # # # # # #         label.set_ha('right' if rot < 90 else 'center')
# # # # # # # # # # #         label.set_fontsize(10)

# # # # # # # # # # #     ax.legend(
# # # # # # # # # # #         title='Stations',
# # # # # # # # # # #         loc='upper left',
# # # # # # # # # # #         fontsize=10,
# # # # # # # # # # #         title_fontsize=12,
# # # # # # # # # # #         framealpha=0.9
# # # # # # # # # # #     )
# # # # # # # # # # #     ax.grid(True, alpha=0.2)

# # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # #     plt.subplots_adjust(
# # # # # # # # # # #         top=0.93,
# # # # # # # # # # #         hspace=0.3,
# # # # # # # # # # #         wspace=0.15
# # # # # # # # # # #     )
# # # # # # # # # # #     return fig_to_base64(fig)


# # # # # # # # # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station_name: str, variable_palette: list = None, metadata_variables: dict = None) -> str | None:
# # # # # # # # # # #     """
# # # # # # # # # # #     Génère un graphique comparatif normalisé de l'évolution de plusieurs variables météorologiques
# # # # # # # # # # #     pour une seule station sélectionnée, à différentes échelles temporelles.

# # # # # # # # # # #     Args:
# # # # # # # # # # #         df (pd.DataFrame): DataFrame contenant les données météorologiques. Doit avoir 'Datetime' comme index.
# # # # # # # # # # #         station_name (str): Le nom de la station pour laquelle générer le graphique.
# # # # # # # # # # #         variable_palette (list, optional): Liste personnalisée de couleurs pour chaque variable.
# # # # # # # # # # #         metadata_variables (dict, optional): Dictionnaire des métadonnées des variables.
# # # # # # # # # # #                                             Si None, un avertissement sera émis.

# # # # # # # # # # #     Returns:
# # # # # # # # # # #         str: Une chaîne Base64 représentant l'image PNG du graphique, ou None si erreur/pas de données.
# # # # # # # # # # #     """
# # # # # # # # # # #     if metadata_variables is None:
# # # # # # # # # # #         warnings.warn("metadata_variables n'a pas été fourni à generate_multi_variable_station_plot. Certaines informations pourraient être manquantes.")
# # # # # # # # # # #         # Fallback si METADATA_VARIABLES n'est pas passé (moins robuste)
# # # # # # # # # # #         metadata_variables = {
# # # # # # # # # # #             'Rain_mm': {'Nom': "Précipitation", 'Unite': "mm", 'agg_type': 'cumul'},
# # # # # # # # # # #             'Air_Temp_Deg_C': {'Nom': "Température ", 'Unite': "°C", 'agg_type': 'moyenne'},
# # # # # # # # # # #             'Rel_H_%': {'Nom': "Humidité Relative", 'Unite': "%", 'agg_type': 'moyenne'},
# # # # # # # # # # #             'Solar_R_W/m^2': {'Nom': "Radiation Solaire", 'Unite': "W/m²", 'agg_type': 'moyenne'},
# # # # # # # # # # #             'Wind_Sp_m/sec': {'Nom': "Vitesse du Vent", 'Unite': "m/s", 'agg_type': 'moyenne'},
# # # # # # # # # # #             'Wind_Dir_Deg': {'Nom': "Direction du Vent", 'Unite': "°", 'agg_type': 'moyenne'},
# # # # # # # # # # #             'BP_mbar_Avg': {'Nom': "Pression Atmospherique moyenne", 'Unite': "mbar", 'agg_type': 'moyenne'}
# # # # # # # # # # #         }


# # # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # # #         warnings.warn("Erreur: L'index du DataFrame n'est pas un DatetimeIndex. Impossible de générer le graphique.")
# # # # # # # # # # #         return None

# # # # # # # # # # #     df_station = df[df['Station'] == station_name].copy()

# # # # # # # # # # #     if df_station.empty:
# # # # # # # # # # #         warnings.warn(f"Station {station_name} sans données - ignorée.")
# # # # # # # # # # #         return None

# # # # # # # # # # #     annees_uniques = df_station.index.year.nunique()
# # # # # # # # # # #     multiple_annees = annees_uniques >= 2

# # # # # # # # # # #     # Configuration des fréquences d'agrégation
# # # # # # # # # # #     if multiple_annees:
# # # # # # # # # # #         freq_configs = {
# # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(interval=4), "Jours"),
# # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=1, interval=8), "Semaines"),
# # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(interval=4), "Mois"),
# # # # # # # # # # #             'Annuelle': ('YS', DateFormatter("%Y"), 0, YearLocator(), "Années")
# # # # # # # # # # #         }
# # # # # # # # # # #     else:
# # # # # # # # # # #         freq_configs = {
# # # # # # # # # # #             'Journalière': ('D', DateFormatter("%d %b %Y"), 90, MonthLocator(bymonthday=1), "Jours"),
# # # # # # # # # # #             'Hebdomadaire': ('W-MON', DateFormatter("%d/%m/%Y"), 90, WeekdayLocator(byweekday=0), "Semaines"),
# # # # # # # # # # #             'Mensuelle': ('ME', DateFormatter("%b %Y"), 45, MonthLocator(), "Mois")
# # # # # # # # # # #         }

# # # # # # # # # # #     nrows, ncols = (2, 2) if len(freq_configs) >= 2 else (1, 1)

# # # # # # # # # # #     variables_to_plot = [var for var in metadata_variables.keys() if var in df_station.columns]

# # # # # # # # # # #     if not variables_to_plot:
# # # # # # # # # # #         warnings.warn("Aucune variable valide à tracer pour cette station.")
# # # # # # # # # # #         return None

# # # # # # # # # # #     num_vars_to_plot = len(variables_to_plot)
# # # # # # # # # # #     if (variable_palette is not None and
# # # # # # # # # # #         isinstance(variable_palette, list) and
# # # # # # # # # # #         len(variable_palette) >= num_vars_to_plot):
# # # # # # # # # # #         plot_palette = variable_palette
# # # # # # # # # # #     else:
# # # # # # # # # # #         plot_palette = sns.color_palette("tab10", num_vars_to_plot)
# # # # # # # # # # #         if variable_palette is not None:
# # # # # # # # # # #             warnings.warn(f"Avertissement: Palette fournie insuffisante ou invalide - utilisation de la palette par défaut pour {station_name}.")

# # # # # # # # # # #     fig, axs = plt.subplots(nrows, ncols, figsize=(28, 22))
# # # # # # # # # # #     axs = axs.flatten()

# # # # # # # # # # #     fig.suptitle(
# # # # # # # # # # #         f'Évolution Normalisée des Variables Météorologiques - Station: {station_name}',
# # # # # # # # # # #         fontsize=24,
# # # # # # # # # # #         weight='bold',
# # # # # # # # # # #         y=1.02
# # # # # # # # # # #     )

# # # # # # # # # # #     for i, (periode, (freq, date_format, rot, locator, label_x)) in enumerate(freq_configs.items()):
# # # # # # # # # # #         if i >= len(axs):
# # # # # # # # # # #             continue

# # # # # # # # # # #         ax = axs[i]
# # # # # # # # # # #         agg_dict = {}
# # # # # # # # # # #         for var in variables_to_plot:
# # # # # # # # # # #             if var in df_station.columns:
# # # # # # # # # # #                 agg_type = metadata_variables[var].get('agg_type', 'moyenne')
# # # # # # # # # # #                 agg_dict[var] = 'sum' if agg_type == 'cumul' else 'mean'

# # # # # # # # # # #         if not agg_dict:
# # # # # # # # # # #             warnings.warn(f"Aucune donnée à agréger pour {station_name} - période {periode}.")
# # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # #             continue

# # # # # # # # # # #         with pd.option_context('future.no_silent_downcasting', True):
# # # # # # # # # # #             for col in agg_dict.keys():
# # # # # # # # # # #                 if col in df_station.columns:
# # # # # # # # # # #                     df_station[col] = pd.to_numeric(df_station[col], errors='coerce')

# # # # # # # # # # #             cols_to_agg = list(agg_dict.keys())
# # # # # # # # # # #             if not cols_to_agg:
# # # # # # # # # # #                 warnings.warn(f"Aucune colonne numérique valide pour {station_name} - période {periode}.")
# # # # # # # # # # #                 if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # #                 continue

# # # # # # # # # # #             df_agg = pd.DataFrame(index=df_station.index)

# # # # # # # # # # #             for var in cols_to_agg:
# # # # # # # # # # #                 temp_data = df_station[var]
# # # # # # # # # # #                 if var == 'Solar_R_W/m^2':
# # # # # # # # # # #                     if 'Is_Daylight' in df_station.columns:
# # # # # # # # # # #                         temp_data = df_station.loc[df_station['Is_Daylight'], var]
# # # # # # # # # # #                     else:
# # # # # # # # # # #                         temp_data = df_station.loc[(df_station.index.hour >= 7) & (df_station.index.hour <= 18), var]
# # # # # # # # # # #                         warnings.warn(f"Avertissement: 'Is_Daylight' non trouvé. Radiation solaire filtrée par heures fixes (7h-18h).")
# # # # # # # # # # #                 elif var == 'Wind_Dir_Deg' and 'Wind_Sp_m/sec' in df_station.columns:
# # # # # # # # # # #                     temp_data = df_station.loc[(df_station['Wind_Sp_m/sec'] > 0) & (df_station['Wind_Dir_Deg'].notna()), var]

# # # # # # # # # # #                 if not temp_data.empty:
# # # # # # # # # # #                     df_agg[var] = temp_data.resample(freq).agg(agg_dict[var])
# # # # # # # # # # #                 else:
# # # # # # # # # # #                     df_agg[var] = np.nan

# # # # # # # # # # #             df_agg = df_agg.dropna(how='all').reset_index()


# # # # # # # # # # #         scaler = MinMaxScaler()
# # # # # # # # # # #         vars_to_scale = list(agg_dict.keys())
# # # # # # # # # # #         numeric_cols = [col for col in vars_to_scale if col in df_agg.columns]

# # # # # # # # # # #         if not numeric_cols or df_agg.empty:
# # # # # # # # # # #             warnings.warn(f"Données manquantes après agrégation ou colonnes numériques absentes pour {station_name} - période {periode}.")
# # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # #             continue

# # # # # # # # # # #         df_agg_scaled = df_agg.copy()
# # # # # # # # # # #         valid_cols_for_scaling = [col for col in numeric_cols if df_agg_scaled[col].notna().any()]
# # # # # # # # # # #         if valid_cols_for_scaling:
# # # # # # # # # # #             df_agg_scaled[valid_cols_for_scaling] = scaler.fit_transform(df_agg_scaled[valid_cols_for_scaling])
# # # # # # # # # # #         else:
# # # # # # # # # # #             warnings.warn(f"Aucune colonne avec des valeurs valides à normaliser pour {station_name} - période {periode}.")
# # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # #             continue


# # # # # # # # # # #         df_melted = df_agg_scaled.melt(
# # # # # # # # # # #             id_vars=['Datetime'],
# # # # # # # # # # #             value_vars=numeric_cols,
# # # # # # # # # # #             var_name='Variable',
# # # # # # # # # # #             value_name='Valeur Normalisée'
# # # # # # # # # # #         )

# # # # # # # # # # #         if df_melted.empty or df_melted['Valeur Normalisée'].dropna().empty:
# # # # # # # # # # #             warnings.warn(f"Données normalisées vides ou toutes NaN pour {station_name} - période {periode}.")
# # # # # # # # # # #             if i < len(axs): fig.delaxes(axs[i])
# # # # # # # # # # #             continue

# # # # # # # # # # #         variable_color_map = {var: plot_palette[j] for j, var in enumerate(variables_to_plot)}

# # # # # # # # # # #         sns.lineplot(
# # # # # # # # # # #             data=df_melted,
# # # # # # # # # # #             x='Datetime',
# # # # # # # # # # #             y='Valeur Normalisée',
# # # # # # # # # # #             hue='Variable',
# # # # # # # # # # #             palette=variable_color_map,
# # # # # # # # # # #             ax=ax,
# # # # # # # # # # #             linewidth=2,
# # # # # # # # # # #             marker='o' if periode == 'Annuelle' or len(df_melted['Datetime'].unique()) < 50 else None,
# # # # # # # # # # #             markersize=6,
# # # # # # # # # # #             legend='full'
# # # # # # # # # # #         )

# # # # # # # # # # #         ax.set_title(f"Évolution {periode.lower()}", fontsize=18, weight='bold', pad=15)
# # # # # # # # # # #         ax.set_xlabel(label_x, fontsize=14, labelpad=15)
# # # # # # # # # # #         ax.set_ylabel("Valeur Normalisée (0-1)", fontsize=14)

# # # # # # # # # # #         ax.xaxis.set_major_formatter(date_format)
# # # # # # # # # # #         if locator is not None:
# # # # # # # # # # #             ax.xaxis.set_major_locator(locator)

# # # # # # # # # # #         for label in ax.get_xticklabels():
# # # # # # # # # # #             label.set_rotation(rot)
# # # # # # # # # # #             label.set_ha('right' if rot > 0 and rot < 90 else 'center')
# # # # # # # # # # #             label.set_fontsize(12)

# # # # # # # # # # #         ax.tick_params(axis='x', which='both', bottom=True, labelbottom=True)
# # # # # # # # # # #         ax.legend(
# # # # # # # # # # #             title='Variables',
# # # # # # # # # # #             loc='upper left',
# # # # # # # # # # #             fontsize=10,
# # # # # # # # # # #             title_fontsize=12,
# # # # # # # # # # #             framealpha=0.9
# # # # # # # # # # #         )
# # # # # # # # # # #         ax.grid(True, alpha=0.2)

# # # # # # # # # # #     for j in range(len(freq_configs), len(axs)):
# # # # # # # # # # #         if j < len(axs) and axs[j] is not None:
# # # # # # # # # # #             fig.delaxes(axs[j])

# # # # # # # # # # #     plt.tight_layout()
# # # # # # # # # # #     plt.subplots_adjust(top=0.94, hspace=0.4, wspace=0.15)
    
# # # # # # # # # # #     return fig_to_base64(fig)





# # # # # # # # # # import pandas as pd
# # # # # # # # # # from pyproj import CRS, Transformer
# # # # # # # # # # import pytz
# # # # # # # # # # from astral.location import LocationInfo
# # # # # # # # # # from astral import sun
# # # # # # # # # # import numpy as np
# # # # # # # # # # import warnings
# # # # # # # # # # import os # <-- AJOUTÉ : Importation du module os

# # # # # # # # # # # Fonction utilitaire pour créer Rain_mm si nécessaire (dépend de votre implémentation)
# # # # # # # # # # # Assurez-vous que cette fonction existe et est fonctionnelle dans votre data_processing.py
# # # # # # # # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # #     """
# # # # # # # # # #     Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
# # # # # # # # # #     Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
# # # # # # # # # #     """
# # # # # # # # # #     df_copy = df.copy()
# # # # # # # # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # # # # # # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
# # # # # # # # # #     elif 'Rain_01_mm' in df_copy.columns:
# # # # # # # # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm']
# # # # # # # # # #     elif 'Rain_02_mm' in df_copy.columns:
# # # # # # # # # #         df_copy['Rain_mm'] = df_copy['Rain_02_mm']
# # # # # # # # # #     else:
# # # # # # # # # #         df_copy['Rain_mm'] = np.nan # Crée la colonne même si aucune source n'est disponible
# # # # # # # # # #         warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents pour créer 'Rain_mm'. 'Rain_mm' est rempli de NaN.")
# # # # # # # # # #     return df_copy


# # # # # # # # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # # # # # # # #     """
# # # # # # # # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # # # # # # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # # # # # # # #     Args:
# # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # # # # # # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # # # # # # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # # # # # # # #     Returns:
# # # # # # # # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # # # # # # # #     """
# # # # # # # # # #     df_copy = df.copy() # Travailler sur une copie

# # # # # # # # # #     # Prioriser la colonne 'Date' si elle existe pour VEA_SISSILI ou si pas de colonnes Y/M/D/H/M
# # # # # # # # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not any(col in df_copy.columns for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'])):
# # # # # # # # # #         try:
# # # # # # # # # #             # Tenter de convertir 'Date' en datetime, avec gestion des erreurs
# # # # # # # # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # # # # # # # #         except Exception as e:
# # # # # # # # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime pour le bassin {bassin}: {e}")
# # # # # # # # # #             df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # # # # # # # #     else:
# # # # # # # # # #         # Logique pour les colonnes Year, Month, Day, Hour, Minute
# # # # # # # # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # # # # # # # #         # Convertir les colonnes en numérique, forçant les erreurs en NaN
# # # # # # # # # #         for col in date_cols:
# # # # # # # # # #             if col in df_copy.columns:
# # # # # # # # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # # # # # # # #         # Créer une série de strings de date/heure et la convertir
# # # # # # # # # #         try:
# # # # # # # # # #             # Filtrer les colonnes de date qui existent réellement dans le DataFrame
# # # # # # # # # #             existing_date_components = [col for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'] if col in df_copy.columns]
            
# # # # # # # # # #             if not existing_date_components:
# # # # # # # # # #                 raise ValueError("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute) trouvée.")

# # # # # # # # # #             # Créer une chaîne de caractères à partir des composantes existantes, avec un remplissage par défaut
# # # # # # # # # #             # pour les colonnes manquantes afin que to_datetime puisse les interpréter.
# # # # # # # # # #             # Par exemple, si 'Minute' est manquante, utiliser '00'
# # # # # # # # # #             date_strings = df_copy.apply(
# # # # # # # # # #                 lambda row: f"{int(row.get('Year', 2000))}-"
# # # # # # # # # #                             f"{int(row.get('Month', 1)):02d}-"
# # # # # # # # # #                             f"{int(row.get('Day', 1)):02d} "
# # # # # # # # # #                             f"{int(row.get('Hour', 0)):02d}:"
# # # # # # # # # #                             f"{int(row.get('Minute', 0)):02d}",
# # # # # # # # # #                 axis=1
# # # # # # # # # #             )
# # # # # # # # # #             df_copy['Datetime'] = pd.to_datetime(date_strings, errors='coerce')
            
# # # # # # # # # #         except Exception as e:
# # # # # # # # # #             warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
# # # # # # # # # #             df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
            
# # # # # # # # # #     # Extraire/recréer les composantes de date/heure si Datetime a été créé et n'est pas vide
# # # # # # # # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # # # # # # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # # # # # # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # # # # # # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # # # # # # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # # # # # # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # # # # # # # #         # Recréer la colonne 'Date' comme objet date (sans heure) si elle n'est pas déjà présente ou si elle est de type non-date
# # # # # # # # # #         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
# # # # # # # # # #              df_copy['Date'] = df_copy['Datetime'].dt.date
# # # # # # # # # #     else:
# # # # # # # # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

# # # # # # # # # #     return df_copy

# # # # # # # # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # #     """
# # # # # # # # # #     Effectue toutes les interpolations météorologiques en une seule passe.
# # # # # # # # # #     Son index DOIT ÊTRE un DatetimeIndex.
# # # # # # # # # #     Il doit également contenir une colonne 'Station'.

# # # # # # # # # #     Args:
# # # # # # # # # #         df (pd.DataFrame): Le DataFrame d'entrée. Son index DOIT ÊTRE un DatetimeIndex.
# # # # # # # # # #                            Il doit également contenir une colonne 'Station'.
# # # # # # # # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # # # # # # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # # # # # # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # # # # # # # #     Returns:
# # # # # # # # # #         pd.DataFrame: Le DataFrame original avec les données interpolées et
# # # # # # # # # #                       la colonne 'Is_Daylight' calculée, ainsi que la durée du jour.
# # # # # # # # # #     """
# # # # # # # # # #     df_processed = df.copy()

# # # # # # # # # #     # --- Vérifications d'intégrité de l'index au début de la fonction (simplifiées) ---
# # # # # # # # # #     # Cette vérification est cruciale. Si l'index n'est pas un DatetimeIndex à ce stade,
# # # # # # # # # #     # c'est que quelque chose en amont (dans app.py) n'a pas fonctionné comme prévu.
# # # # # # # # # #     # Cependant, on va gérer les NaT dans l'index ici.
    
# # # # # # # # # #     # Supprime les lignes où l'index Datetime est NaT (valeur manquante)
# # # # # # # # # #     initial_rows = len(df_processed)
# # # # # # # # # #     df_processed = df_processed[df_processed.index.notna()]
# # # # # # # # # #     if len(df_processed) == 0:
# # # # # # # # # #         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
# # # # # # # # # #     if initial_rows - len(df_processed) > 0:
# # # # # # # # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")

# # # # # # # # # #     # --- Pré-vérification et préparation des données GPS (inchangée) ---
# # # # # # # # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # # # # # # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # # # # # # # #         raise ValueError(
# # # # # # # # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # # # # # # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # # # # # # # #         )

# # # # # # # # # #     if not df_gps['Station'].is_unique:
# # # # # # # # # #         warnings.warn("La colonne 'Station' dans df_gps contient des noms de station dupliqués. Suppression des doublons.")
# # # # # # # # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # # # # # # # #     else:
# # # # # # # # # #         df_gps_unique = df_gps.copy()

# # # # # # # # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # # # # # # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # # # # # # # #     for col in numerical_cols:
# # # # # # # # # #         if col in df_processed.columns:
# # # # # # # # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # # # # # # # #     df_processed_parts = []
# # # # # # # # # #     stations_fallback = []

# # # # # # # # # #     for station_name, group in df_processed.groupby('Station'):
# # # # # # # # # #         group_copy = group.copy()

# # # # # # # # # #         # L'index du groupe est déjà un DatetimeIndex à ce stade (garanti par app.py et nettoyage ci-dessus)
# # # # # # # # # #         group_copy = group_copy.sort_index()

# # # # # # # # # #         gps_data = gps_info_dict.get(station_name)
# # # # # # # # # #         apply_fixed_daylight = True # Par défaut, utiliser le fallback

# # # # # # # # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # # # # # # # #             lat = gps_data['Lat']
# # # # # # # # # #             long = gps_data['Long']
# # # # # # # # # #             timezone_str = gps_data['Timezone']

# # # # # # # # # #             try:
# # # # # # # # # #                 tz = pytz.timezone(timezone_str)
# # # # # # # # # #                 # Localiser l'index si nécessaire
# # # # # # # # # #                 if group_copy.index.tz is None:
# # # # # # # # # #                     group_copy.index = group_copy.index.tz_localize(tz, ambiguous='NaT', nonexistent='NaT')
# # # # # # # # # #                 else:
# # # # # # # # # #                     group_copy.index = group_copy.index.tz_convert(tz)

# # # # # # # # # #                 daily_sun_info = {}
# # # # # # # # # #                 for date_only in group_copy.index.normalize().unique():
# # # # # # # # # #                     # Utilisation de LocationInfo comme dans votre Colab pour garantir la compatibilité
# # # # # # # # # #                     loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
# # # # # # # # # #                     s = sun.sun(loc.observer, date=date_only, tzinfo=loc.tzinfo) # Utiliser loc.tzinfo ici
# # # # # # # # # #                     daily_sun_info[date_only.date()] = {
# # # # # # # # # #                         'sunrise': s['sunrise'],
# # # # # # # # # #                         'sunset': s['sunset']
# # # # # # # # # #                     }

# # # # # # # # # #                 group_copy.loc[:, 'sunrise_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunrise'))
# # # # # # # # # #                 group_copy.loc[:, 'sunset_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunset'))

# # # # # # # # # #                 # Assurer que les colonnes sunrise/sunset sont dans le même fuseau horaire que l'index pour la comparaison
# # # # # # # # # #                 if group_copy.index.tz is not None: # Vérifier si l'index est bien localisé
# # # # # # # # # #                      if not group_copy['sunrise_time_local'].isnull().all():
# # # # # # # # # #                          group_copy.loc[:, 'sunrise_time_local'] = group_copy['sunrise_time_local'].dt.tz_convert(group_copy.index.tz)
# # # # # # # # # #                      if not group_copy['sunset_time_local'].isnull().all():
# # # # # # # # # #                          group_copy.loc[:, 'sunset_time_local'] = group_copy['sunset_time_local'].dt.tz_convert(group_copy.index.tz)


# # # # # # # # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index >= group_copy['sunrise_time_local']) & \
# # # # # # # # # #                                                   (group_copy.index < group_copy['sunset_time_local'])

# # # # # # # # # #                 daylight_timedelta = group_copy['sunset_time_local'] - group_copy['sunrise_time_local']
                
# # # # # # # # # #                 # Initialiser 'Daylight_Duration' comme Series d'objets pour éviter les FutuerWarnings
# # # # # # # # # #                 group_copy.loc[:, 'Daylight_Duration'] = pd.Series(dtype='object', index=group_copy.index)
                
# # # # # # # # # #                 valid_times = daylight_timedelta.notna()
# # # # # # # # # #                 group_copy.loc[valid_times, 'Daylight_Duration'] = daylight_timedelta[valid_times].apply(
# # # # # # # # # #                     lambda td: (pd.to_datetime('00:00:00') + td).strftime('%H:%M:%S') if pd.notna(td) else np.nan
# # # # # # # # # #                 )
# # # # # # # # # #                 apply_fixed_daylight = False

# # # # # # # # # #             except Exception as e:
# # # # # # # # # #                 warnings.warn(f"Erreur Astral pour {station_name}: {e}. Utilisation du fallback 7h-18h.")
# # # # # # # # # #                 stations_fallback.append(station_name)
# # # # # # # # # #         else:
# # # # # # # # # #             warnings.warn(f"Coordonnées/Fuseau horaire manquants/invalides pour {station_name}. Utilisation du fallback 7h-18h.")
# # # # # # # # # #             stations_fallback.append(station_name)

# # # # # # # # # #         # Appliquer le fallback si Astral n'a pas été calculé ou a échoué
# # # # # # # # # #         if apply_fixed_daylight:
# # # # # # # # # #             if group_copy.index.tz is None:
# # # # # # # # # #                 # Si l'index n'est pas localisé, le localiser à UTC pour le fallback
# # # # # # # # # #                 group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # # # # # # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # # # # # # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"


# # # # # # # # # #         df_processed_parts.append(group_copy)

# # # # # # # # # #     if not df_processed_parts:
# # # # # # # # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # # # # # # # #     df_final = pd.concat(df_processed_parts)
# # # # # # # # # #     df_final = df_final.sort_index()
# # # # # # # # # #     df_final.index.name = 'Datetime' # S'assurer que le nom de l'index est bien 'Datetime'


# # # # # # # # # #     # Nettoyage des colonnes temporaires
# # # # # # # # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date',
# # # # # # # # # #                                   'sunrise_time_local', 'sunset_time_local']
# # # # # # # # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # # # # # # # #     # Gestion intelligente de Rain_mm
# # # # # # # # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # # # # # # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # # # # # # # #             df_final = create_rain_mm(df_final)
# # # # # # # # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # # # # # # # #         else:
# # # # # # # # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # # # # # # # #             # Assurer que Rain_mm existe même si c'est avec des NaNs
# # # # # # # # # #             if 'Rain_mm' not in df_final.columns:
# # # # # # # # # #                 df_final['Rain_mm'] = np.nan


# # # # # # # # # #     # Interpolation standard et bornage pour les variables numériques
# # # # # # # # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # # # # # # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # # # # # # # #                      'Wind_Dir_Deg']

# # # # # # # # # #     for var in standard_vars:
# # # # # # # # # #         if var in df_final.columns:
# # # # # # # # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # # # # # # # #             if var in limits:
# # # # # # # # # #                 min_val = limits[var]['min']
# # # # # # # # # #                 max_val = limits[var]['max']
# # # # # # # # # #                 initial_nan_count = df_final[var].isna().sum()
# # # # # # # # # #                 df_final.loc[(df_final[var] < min_val) | (df_final[var] > max_val), var] = np.nan
# # # # # # # # # #                 if df_final[var].isna().sum() > initial_nan_count:
# # # # # # # # # #                     warnings.warn(f"Remplacement de {df_final[var].isna().sum() - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # # # # # # # #             # Interpolation utilisant 'time' car l'index est garanti DatetimeIndex
# # # # # # # # # #             df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # # # # # # # #             df_final[var] = df_final[var].bfill().ffill() # Pour les NaNs aux extrémités

# # # # # # # # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # # # # # # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # # # # # # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # # # # # # # #         if 'Solar_R_W/m^2' in limits:
# # # # # # # # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # # # # # # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # # # # # # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # # # # # # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # # # # # # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # # # # # # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # # # # # # # #         if 'Is_Daylight' in df_final.columns:
# # # # # # # # # #             # Radiation positive hors du jour -> 0
# # # # # # # # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # # # # # # # #             # Zéros suspects en journée sans pluie -> NaN
# # # # # # # # # #             if 'Rain_mm' in df_final.columns:
# # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # # # # # # # #             else:
# # # # # # # # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # # # # # # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # # # # # # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # # # # # # # #             # Interpolation dans les périodes de jour
# # # # # # # # # #             is_day = df_final['Is_Daylight']
# # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # # # # # # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # # # # # # # #             # Les NaNs restants hors des heures de jour -> 0
# # # # # # # # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # # # # # # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # # # # # # # #         else:
# # # # # # # # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # # # # # # # #             df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()

# # # # # # # # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # # # # # # # #     missing_after_interp = df_final.isna().sum()
# # # # # # # # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # # # # # # # #     if not columns_with_missing.empty:
# # # # # # # # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # # # # # # # #     else:
# # # # # # # # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # # # # # # # #     return df_final


# # # # # # # # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # #     """
# # # # # # # # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # # # # # # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # # # # # # # #     Args:
# # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # # # # # # # #     Returns:
# # # # # # # # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # # # # # # # #     """
# # # # # # # # # #     df_copy = df.copy()

# # # # # # # # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # # # # # # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # # # # # # # #         raise ValueError(
# # # # # # # # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # # # # # # # #         )

# # # # # # # # # #     def convert_row(row):
# # # # # # # # # #         try:
# # # # # # # # # #             zone = int(row['zone'])
# # # # # # # # # #             hemisphere = str(row['hemisphere']).upper()
# # # # # # # # # #             is_northern = hemisphere == 'N'

# # # # # # # # # #             proj_utm = CRS.from_proj4(
# # # # # # # # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # # # # # # # #             )
# # # # # # # # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # # # # # # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # # # # # # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # # # # # # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # # # # # # # #         except Exception as e:
# # # # # # # # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # # # # # # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # # # # # # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # # # # # # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # # # # # # # #     return df_copy

# # # # # # # # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # # # # # # # #     """
# # # # # # # # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # # # # # # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # # # # # # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # # # # # # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # # # # # # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
# # # # # # # # # #     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

# # # # # # # # # #     Returns:
# # # # # # # # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # # # # # # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # # # # # # # #     """
# # # # # # # # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # # # # # # # #     data_dir = 'data'
# # # # # # # # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # # # # # # # #     files_info = [
# # # # # # # # # #         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
# # # # # # # # # #         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
# # # # # # # # # #         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
# # # # # # # # # #     ]

# # # # # # # # # #     loaded_dfs = []

# # # # # # # # # #     for file_info in files_info:
# # # # # # # # # #         output_file_path = os.path.join(data_dir, file_info['name'])
        
# # # # # # # # # #         if not os.path.exists(output_file_path):
# # # # # # # # # #             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
# # # # # # # # # #             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
# # # # # # # # # #             print(f"Téléchargement de {file_info['bassin']} terminé.")
# # # # # # # # # #         else:
# # # # # # # # # #             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
# # # # # # # # # #         loaded_dfs.append(pd.read_excel(output_file_path))

# # # # # # # # # #     vea_sissili_bassin = loaded_dfs[0]
# # # # # # # # # #     dano_bassin = loaded_dfs[1]
# # # # # # # # # #     dassari_bassin = loaded_dfs[2]

# # # # # # # # # #     # 4. Prétraitement des DataFrames (votre code original)
# # # # # # # # # #     print("Début du prétraitement des données de stations...")
    
# # # # # # # # # #     # Vea Sissili
# # # # # # # # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # # # # # # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # # # # # # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # # # # # # # #     # Dassari
# # # # # # # # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # # # # # # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # # # # # # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # # # # # # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # # # # # # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # # # # # # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # # # # # # # #     dano_bassin['zone'] = 30
# # # # # # # # # #     dano_bassin['hemisphere'] = 'N'
# # # # # # # # # #     dassari_bassin['zone'] = 31
# # # # # # # # # #     dassari_bassin['hemisphere'] = 'N'

# # # # # # # # # #     # Application de la fonction de conversion UTM vers GPS
# # # # # # # # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # # # # # # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # # # # # # # #     # Ajout des fuseaux horaires
# # # # # # # # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # # # # # # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # # # # # # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # # # # # # # #     # Fusion de tous les bassins
# # # # # # # # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # # # # # # # #     # Renommer 'Name' en 'Station'
# # # # # # # # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # # # # # # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # # # # # # # #     initial_rows_count = len(bassins)
# # # # # # # # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # # # # # # # #     if len(bassins) < initial_rows_count:
# # # # # # # # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # # # # # # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # # # # # # # #     if 'Station' in bassins.columns:
# # # # # # # # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # # # # # # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # # # # # # # #     return bassins

# # # # # # # # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # #     """
# # # # # # # # # #     Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
# # # # # # # # # #     Conserve la première occurrence en cas de doublon.

# # # # # # # # # #     Args:
# # # # # # # # # #         df (pd.DataFrame): Le DataFrame d'entrée.

# # # # # # # # # #     Returns:
# # # # # # # # # #         pd.DataFrame: Le DataFrame sans doublons.
# # # # # # # # # #     """
# # # # # # # # # #     if 'Station' in df.columns and 'Datetime' in df.columns:
# # # # # # # # # #         initial_rows = len(df)
# # # # # # # # # #         df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
# # # # # # # # # #         if len(df_cleaned) < initial_rows:
# # # # # # # # # #             warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
# # # # # # # # # #         return df_cleaned
# # # # # # # # # #     else:
# # # # # # # # # #         warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
# # # # # # # # # #         return df

# # # # # # # # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # # # # # # # #     """
# # # # # # # # # #     Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

# # # # # # # # # #     Args:
# # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # # # # # # # #         limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

# # # # # # # # # #     Returns:
# # # # # # # # # #         pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
# # # # # # # # # #     """
# # # # # # # # # #     df_processed = df.copy()

# # # # # # # # # #     # S'assurer que l'index est bien un DatetimeIndex pour éviter les problèmes
# # # # # # # # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # # # # # # # #         warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
# # # # # # # # # #         try:
# # # # # # # # # #             df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
# # # # # # # # # #             df_processed = df_processed[df_processed.index.notna()] # Supprimer les NaT dans l'index
# # # # # # # # # #             if df_processed.empty:
# # # # # # # # # #                 raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
# # # # # # # # # #         except Exception as e:
# # # # # # # # # #             raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

# # # # # # # # # #     for var, vals in limits.items():
# # # # # # # # # #         if var in df_processed.columns:
# # # # # # # # # #             min_val = vals.get('min')
# # # # # # # # # #             max_val = vals.get('max')
# # # # # # # # # #             if min_val is not None or max_val is not None:
# # # # # # # # # #                 initial_nan_count = df_processed[var].isna().sum()
# # # # # # # # # #                 if min_val is not None:
# # # # # # # # # #                     df_processed.loc[df_processed[var] < min_val, var] = np.nan
# # # # # # # # # #                 if max_val is not None:
# # # # # # # # # #                     df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
# # # # # # # # # #                 # Compter les valeurs remplacées
# # # # # # # # # #                 new_nan_count = df_processed[var].isna().sum()
# # # # # # # # # #                 if new_nan_count > initial_nan_count:
# # # # # # # # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs aberrantes dans '{var}' par NaN.")
# # # # # # # # # #     return df_processed

# # # # # # # # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # # # # # # # #     """
# # # # # # # # # #     Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
# # # # # # # # # #     """
# # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

# # # # # # # # # #     filtered_df = df[df['Station'] == station].copy()
# # # # # # # # # #     if filtered_df.empty:
# # # # # # # # # #         return None

# # # # # # # # # #     # Agrégation des données
# # # # # # # # # #     if periode == 'Journalière':
# # # # # # # # # #         resampled_df = filtered_df[variable].resample('D').mean()
# # # # # # # # # #     elif periode == 'Hebdomadaire':
# # # # # # # # # #         resampled_df = filtered_df[variable].resample('W').mean()
# # # # # # # # # #     elif periode == 'Mensuelle':
# # # # # # # # # #         resampled_df = filtered_df[variable].resample('M').mean()
# # # # # # # # # #     elif periode == 'Annuelle':
# # # # # # # # # #         resampled_df = filtered_df[variable].resample('Y').mean()
# # # # # # # # # #     else:
# # # # # # # # # #         resampled_df = filtered_df[variable] # Pas de resample si 'periode' inconnue

# # # # # # # # # #     # Supprimer les lignes où la variable est NaN après le resample
# # # # # # # # # #     resampled_df = resampled_df.dropna()

# # # # # # # # # #     if resampled_df.empty:
# # # # # # # # # #         return None

# # # # # # # # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # # # # # # # #     color = colors.get(station, '#1f77b4') # Couleur par défaut si non trouvée

# # # # # # # # # #     import plotly.graph_objects as go # Importation locale
# # # # # # # # # #     import plotly.io as pio # Importation locale

# # # # # # # # # #     fig = go.Figure()
# # # # # # # # # #     fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # # # # # # # #                              mode='lines', name=f'{variable_meta["Nom"]} - {station}',
# # # # # # # # # #                              line=dict(color=color)))

# # # # # # # # # #     fig.update_layout(
# # # # # # # # # #         title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
# # # # # # # # # #         xaxis_title="Date",
# # # # # # # # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # # # # # # # #         hovermode="x unified"
# # # # # # # # # #     )
# # # # # # # # # #     return pio.to_html(fig, full_html=False)

# # # # # # # # # # def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # # # # # # # #     """
# # # # # # # # # #     Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
# # # # # # # # # #     """
# # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

# # # # # # # # # #     import plotly.graph_objects as go # Importation locale
# # # # # # # # # #     import plotly.io as pio # Importation locale

# # # # # # # # # #     fig = go.Figure()
    
# # # # # # # # # #     all_stations = df['Station'].unique()
# # # # # # # # # #     if len(all_stations) < 2:
# # # # # # # # # #         warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
# # # # # # # # # #         return None

# # # # # # # # # #     for station in all_stations:
# # # # # # # # # #         filtered_df = df[df['Station'] == station].copy()
# # # # # # # # # #         if filtered_df.empty:
# # # # # # # # # #             continue

# # # # # # # # # #         # Agrégation des données
# # # # # # # # # #         if periode == 'Journalière':
# # # # # # # # # #             resampled_df = filtered_df[variable].resample('D').mean()
# # # # # # # # # #         elif periode == 'Hebdomadaire':
# # # # # # # # # #             resampled_df = filtered_df[variable].resample('W').mean()
# # # # # # # # # #         elif periode == 'Mensuelle':
# # # # # # # # # #             resampled_df = filtered_df[variable].resample('M').mean()
# # # # # # # # # #         elif periode == 'Annuelle':
# # # # # # # # # #             resampled_df = filtered_df[variable].resample('Y').mean()
# # # # # # # # # #         else:
# # # # # # # # # #             resampled_df = filtered_df[variable]

# # # # # # # # # #         resampled_df = resampled_df.dropna()
# # # # # # # # # #         if resampled_df.empty:
# # # # # # # # # #             continue
        
# # # # # # # # # #         color = colors.get(station, '#1f77b4') # Couleur par défaut
# # # # # # # # # #         fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # # # # # # # #                                  mode='lines', name=station,
# # # # # # # # # #                                  line=dict(color=color)))

# # # # # # # # # #     if not fig.data: # Si aucune trace n'a été ajoutée
# # # # # # # # # #         return None

# # # # # # # # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # # # # # # # #     fig.update_layout(
# # # # # # # # # #         title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
# # # # # # # # # #         xaxis_title="Date",
# # # # # # # # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # # # # # # # #         hovermode="x unified"
# # # # # # # # # #     )
# # # # # # # # # #     return pio.to_html(fig, full_html=False)


# # # # # # # # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> str:
# # # # # # # # # #     """
# # # # # # # # # #     Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
# # # # # # # # # #     """
# # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

# # # # # # # # # #     filtered_df = df[df['Station'] == station].copy()
# # # # # # # # # #     if filtered_df.empty:
# # # # # # # # # #         return None

# # # # # # # # # #     # Sélectionne les variables numériques à normaliser
# # # # # # # # # #     # Exclure 'Station' et 'Is_Daylight' qui ne sont pas des variables à tracer comme ça
# # # # # # # # # #     numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight']]

# # # # # # # # # #     if not numerical_vars:
# # # # # # # # # #         warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
# # # # # # # # # #         return None

# # # # # # # # # #     # Normalisation Min-Max
# # # # # # # # # #     normalized_df = filtered_df[numerical_vars].copy()
# # # # # # # # # #     for col in normalized_df.columns:
# # # # # # # # # #         min_val = normalized_df[col].min()
# # # # # # # # # #         max_val = normalized_df[col].max()
# # # # # # # # # #         if max_val != min_val:
# # # # # # # # # #             normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
# # # # # # # # # #         else: # Si toutes les valeurs sont identiques, normaliser à 0.5 ou laisser NaN
# # # # # # # # # #             normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

# # # # # # # # # #     # Supprimer les lignes où toutes les variables normalisées sont NaN
# # # # # # # # # #     normalized_df = normalized_df.dropna(how='all')

# # # # # # # # # #     if normalized_df.empty:
# # # # # # # # # #         return None
    
# # # # # # # # # #     import plotly.graph_objects as go # Importation locale
# # # # # # # # # #     import plotly.io as pio # Importation locale

# # # # # # # # # #     fig = go.Figure()
# # # # # # # # # #     for var in normalized_df.columns:
# # # # # # # # # #         var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
# # # # # # # # # #         color = colors.get(var, None) # Utiliser la couleur spécifique à la variable

# # # # # # # # # #         fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
# # # # # # # # # #                                  mode='lines', name=var_meta['Nom'],
# # # # # # # # # #                                  line=dict(color=color)))

# # # # # # # # # #     fig.update_layout(
# # # # # # # # # #         title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
# # # # # # # # # #         xaxis_title="Date",
# # # # # # # # # #         yaxis_title="Valeur Normalisée (0-1)",
# # # # # # # # # #         hovermode="x unified",
# # # # # # # # # #         legend_title="Variables"
# # # # # # # # # #     )
# # # # # # # # # #     return pio.to_html(fig, full_html=False)

# # # # # # # # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # # # # # # # #     """
# # # # # # # # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # # # # # # # # #     groupées par station.

# # # # # # # # # #     Args:
# # # # # # # # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # # # # # # # # #     Returns:
# # # # # # # # # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # # # # # # # # #     """
# # # # # # # # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # # # # # # # #         warnings.warn("L'index n'est pas un DatetimeIndex dans daily_stats. Tentative de conversion.")
# # # # # # # # # #         try:
# # # # # # # # # #             df.index = pd.to_datetime(df.index, errors='coerce')
# # # # # # # # # #             df = df[df.index.notna()]
# # # # # # # # # #             if df.empty:
# # # # # # # # # #                 return pd.DataFrame() # Retourner un DataFrame vide si tout est supprimé
# # # # # # # # # #         except Exception as e:
# # # # # # # # # #             warnings.warn(f"Impossible de convertir l'index en DatetimeIndex pour daily_stats: {e}")
# # # # # # # # # #             return pd.DataFrame() # Retourner un DataFrame vide en cas d'échec critique

# # # # # # # # # #     # Sélectionner uniquement les colonnes numériques, en excluant 'Station' et 'Is_Daylight'
# # # # # # # # # #     numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Is_Daylight']]

# # # # # # # # # #     if not numerical_cols:
# # # # # # # # # #         return pd.DataFrame() # Retourner un DataFrame vide si aucune colonne numérique n'est trouvée

# # # # # # # # # #     # Grouper par station et par jour, puis calculer les statistiques
# # # # # # # # # #     # Il faut s'assurer que les noms de colonnes dans agg existent dans numerical_cols ou df.columns
# # # # # # # # # #     # Les noms de colonnes passés à agg doivent être les vrais noms de colonnes
# # # # # # # # # #     # Si 'Mean', 'Min', 'Max' sont des noms génériques, vous devrez les remplacer par les vraies variables
# # # # # # # # # #     # Ex: 'Air_Temp_Deg_C_Mean' = ('Air_Temp_Deg_C', 'mean')
    
# # # # # # # # # #     # Pour l'exemple, j'agrège toutes les colonnes numériques par moyenne, min, max
# # # # # # # # # #     # et la pluie par somme
    
# # # # # # # # # #     agg_dict = {col: ['mean', 'min', 'max'] for col in numerical_cols if col != 'Rain_mm'}
# # # # # # # # # #     if 'Rain_mm' in numerical_cols:
# # # # # # # # # #         agg_dict['Rain_mm'] = ['mean', 'min', 'max', 'sum'] # Somme de la pluie


# # # # # # # # # #     daily_stats_df = df.groupby(['Station', df.index.date]).agg(agg_dict)
    
# # # # # # # # # #     # Apllatir les MultiIndex des colonnes après l'agrégation
# # # # # # # # # #     daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

# # # # # # # # # #     # Renommer l'index de date
# # # # # # # # # #     daily_stats_df.index.names = ['Station', 'Date']
    
# # # # # # # # # #     return daily_stats_df.reset_index()



# # # # # # # # # ###########################

# # # # import pandas as pd
# # # # from pyproj import CRS, Transformer
# # # # import pytz
# # # # from astral.location import LocationInfo
# # # # from astral import sun
# # # # import numpy as np
# # # # import warnings
# # # # import os # <-- Importation du module os

# # # # # Fonction utilitaire pour créer Rain_mm si nécessaire (dépend de votre implémentation)
# # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
# # # #     Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
# # # #     """
# # # #     df_copy = df.copy()
# # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
# # # #     elif 'Rain_01_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm']
# # # #     elif 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_02_mm']
# # # #     else:
# # # #         df_copy['Rain_mm'] = np.nan # Crée la colonne même si aucune source n'est disponible
# # # #         warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents pour créer 'Rain_mm'. 'Rain_mm' est rempli de NaN.")
# # # #     return df_copy


# # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # #     """
# # # #     df_copy = df.copy() # Travailler sur une copie

# # # #     # Prioriser la colonne 'Date' si elle existe pour VEA_SISSILI ou si pas de colonnes Y/M/D/H/M
# # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not any(col in df_copy.columns for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'])):
# # # #         try:
# # # #             # Tenter de convertir 'Date' en datetime, avec gestion des erreurs
# # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime pour le bassin {bassin}: {e}")
# # # #             df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # #     else:
# # # #         # Logique pour les colonnes Year, Month, Day, Hour, Minute
# # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # #         # Convertir les colonnes en numérique, forçant les erreurs en NaN
# # # #         for col in date_cols:
# # # #             if col in df_copy.columns:
# # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # #         # Créer une série de strings de date/heure et la convertir
# # # #         try:
# # # #             # Filtrer les colonnes de date qui existent réellement dans le DataFrame
# # # #             existing_date_components = [col for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'] if col in df_copy.columns]
            
# # # #             if not existing_date_components:
# # # #                 raise ValueError("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute) trouvée.")

# # # #             # Créer une chaîne de caractères à partir des composantes existantes, avec un remplissage par défaut
# # # #             # pour les colonnes manquantes afin que to_datetime puisse les interpréter.
# # # #             # Par exemple, si 'Minute' est manquante, utiliser '00'
# # # #             date_strings = df_copy.apply(
# # # #                 lambda row: f"{int(row.get('Year', 2000))}-"
# # # #                             f"{int(row.get('Month', 1)):02d}-"
# # # #                             f"{int(row.get('Day', 1)):02d} "
# # # #                             f"{int(row.get('Hour', 0)):02d}:"
# # # #                             f"{int(row.get('Minute', 0)):02d}",
# # # #                 axis=1
# # # #             )
# # # #             df_copy['Datetime'] = pd.to_datetime(date_strings, errors='coerce')
            
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
# # # #             df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
            
# # # #     # Extraire/recréer les composantes de date/heure si Datetime a été créé et n'est pas vide
# # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # #         # Recréer la colonne 'Date' comme objet date (sans heure) si elle n'est pas déjà présente ou si elle est de type non-date
# # # #         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
# # # #              df_copy['Date'] = df_copy['Datetime'].dt.date
# # # #     else:
# # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

# # # #     return df_copy

# # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Effectue toutes les interpolations météorologiques en une seule passe.
# # # #     Son index DOIT ÊTRE un DatetimeIndex.
# # # #     Il doit également contenir une colonne 'Station'.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée. Son index DOIT ÊTRE un DatetimeIndex.
# # # #                            Il doit également contenir une colonne 'Station'.
# # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame original avec les données interpolées et
# # # #                       la colonne 'Is_Daylight' calculée, ainsi que la durée du jour.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     # --- Vérifications d'intégrité de l'index au début de la fonction ---
# # # #     print(f"DEBUG (interpolation): Index du DataFrame initial avant nettoyage des NaT: {type(df_processed.index)}")
# # # #     print(f"DEBUG (interpolation): Dtypes du DataFrame initial:\n{df_processed.dtypes}")
    
# # # #     # Supprime les lignes où l'index Datetime est NaT (valeur manquante)
# # # #     initial_rows = len(df_processed)
# # # #     df_processed = df_processed[df_processed.index.notna()]
# # # #     if len(df_processed) == 0:
# # # #         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
# # # #     if initial_rows - len(df_processed) > 0:
# # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")

# # # #     print(f"DEBUG (interpolation): Index du DataFrame après nettoyage des NaT: {type(df_processed.index)}")
# # # #     print(f"DEBUG (interpolation): Premières 5 valeurs de l'index après nettoyage des NaT: {df_processed.index[:5].tolist() if not df_processed.empty else 'DataFrame vide'}")

# # # #     # --- Pré-vérification et préparation des données GPS (inchangée) ---
# # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # #         raise ValueError(
# # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # #         )

# # # #     if not df_gps['Station'].is_unique:
# # # #         print("Avertissement: La colonne 'Station' dans df_gps contient des noms de station dupliqués.")
# # # #         print("Ceci peut entraîner des comportements inattendus ou des stations non reconnues.")
# # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # #         print(f"Suppression de {len(df_gps) - len(df_gps_unique)} doublons dans df_gps (en gardant la première occurrence).")
# # # #     else:
# # # #         df_gps_unique = df_gps.copy()

# # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # #     for col in numerical_cols:
# # # #         if col in df_processed.columns:
# # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # #     df_processed_parts = []
# # # #     stations_fallback = []

# # # #     for station_name, group in df_processed.groupby('Station'):
# # # #         group_copy = group.copy()

# # # #         print(f"DEBUG (interpolation/groupby): Début du traitement du groupe '{station_name}'.")
# # # #         print(f"DEBUG (interpolation/groupby): Type de l'index du groupe: {type(group_copy.index)}")
# # # #         print(f"DEBUG (interpolation/groupby): Dtypes du groupe '{station_name}':\n{group_copy.dtypes}")
# # # #         print(f"DEBUG (interpolation/groupby): Index du groupe '{station_name}' a des NaT? {group_copy.index.isna().any()}")
# # # #         print(f"DEBUG (interpolation/groupby): Premières 5 valeurs de l'index du groupe '{station_name}': {group_copy.index[:5].tolist() if not group_copy.empty else 'DataFrame vide'}")


# # # #         group_copy = group_copy.sort_index()

# # # #         gps_data = gps_info_dict.get(station_name)
# # # #         apply_fixed_daylight = True # Par défaut, utiliser le fallback

# # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # #             lat = gps_data['Lat']
# # # #             long = gps_data['Long']
# # # #             timezone_str = gps_data['Timezone']

# # # #             try:
# # # #                 tz = pytz.timezone(timezone_str)
# # # #                 # Localiser l'index si nécessaire
# # # #                 if group_copy.index.tz is None:
# # # #                     group_copy.index = group_copy.index.tz_localize(tz, ambiguous='NaT', nonexistent='NaT')
# # # #                 else:
# # # #                     group_copy.index = group_copy.index.tz_convert(tz)
# # # #                 print(f"Index Datetime localisé au fuseau horaire : {tz}")


# # # #                 daily_sun_info = {}
# # # #                 for date_only in group_copy.index.normalize().unique():
# # # #                     loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
# # # #                     s = sun.sun(loc.observer, date=date_only, tzinfo=loc.tzinfo)
# # # #                     daily_sun_info[date_only.date()] = {
# # # #                         'sunrise': s['sunrise'],
# # # #                         'sunset': s['sunset']
# # # #                     }

# # # #                 group_copy.loc[:, 'sunrise_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunrise'))
# # # #                 group_copy.loc[:, 'sunset_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunset'))

# # # #                 if group_copy.index.tz is not None:
# # # #                      if not group_copy['sunrise_time_local'].isnull().all():
# # # #                          group_copy.loc[:, 'sunrise_time_local'] = group_copy['sunrise_time_local'].dt.tz_convert(group_copy.index.tz)
# # # #                      if not group_copy['sunset_time_local'].isnull().all():
# # # #                          group_copy.loc[:, 'sunset_time_local'] = group_copy['sunset_time_local'].dt.tz_convert(group_copy.index.tz)


# # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index >= group_copy['sunrise_time_local']) & \
# # # #                                                   (group_copy.index < group_copy['sunset_time_local'])

# # # #                 daylight_timedelta = group_copy['sunset_time_local'] - group_copy['sunrise_time_local']
                
# # # #                 group_copy.loc[:, 'Daylight_Duration'] = pd.Series(dtype='object', index=group_copy.index)
                
# # # #                 valid_times = daylight_timedelta.notna()
# # # #                 group_copy.loc[valid_times, 'Daylight_Duration'] = daylight_timedelta[valid_times].apply(
# # # #                     lambda td: (pd.to_datetime('00:00:00') + td).strftime('%H:%M:%S') if pd.notna(td) else np.nan
# # # #                 )
# # # #                 print(f"Lever et coucher du soleil calculés pour {station_name}.")
# # # #                 apply_fixed_daylight = False

# # # #             except Exception as e:
# # # #                 print(f"Erreur lors du calcul du lever/coucher du soleil avec Astral pour {station_name}: {e}.")
# # # #                 stations_fallback.append(station_name)
# # # #         else:
# # # #             print(f"Avertissement: Coordonnées ou Fuseau horaire manquants/invalides pour le site '{station_name}' dans df_gps. Index localisé à UTC.")
# # # #             stations_fallback.append(station_name)

# # # #         if apply_fixed_daylight:
# # # #             if group_copy.index.tz is None:
# # # #                 group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # #             print(f"Utilisation de l'indicateur jour/nuit fixe (7h-18h) pour {station_name}.")


# # # #         df_processed_parts.append(group_copy)

# # # #     if not df_processed_parts:
# # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # #     df_final = pd.concat(df_processed_parts)

# # # #     # --- RÉINTÉGRATION DE LA LOGIQUE DE COLAB POUR LA VÉRIFICATION DE L'INDEX APRÈS CONCATÉNATION ---
# # # #     print(f"DEBUG (interpolation/concat): Index du DataFrame final après concaténation: {type(df_final.index)}")
# # # #     if not isinstance(df_final.index, pd.DatetimeIndex):
# # # #         print("Avertissement (interpolation/concat): L'index n'est pas un DatetimeIndex après concaténation. Tentative de correction...")
# # # #         if 'Datetime' in df_final.columns: # Vérifier si la colonne 'Datetime' est dispo pour recréer l'index
# # # #             df_final = df_final.set_index('Datetime', drop=True)
# # # #             df_final.index.name = 'Datetime'
# # # #             print("DEBUG (interpolation/concat): Index 'Datetime' recréé et trié avec succès.")
# # # #         else:
# # # #             print("Erreur (interpolation/concat): Colonne 'Datetime' manquante pour définir l'index après concaténation. Impossible d'interpoler avec la méthode 'time'.")
# # # #             # Si Datetime est vraiment absent ici, l'interpolation 'time' échouera.
# # # #             # Ne pas retourner df_final tout de suite pour permettre les print suivants, mais l'erreur se produira.
    
# # # #     df_final = df_final.sort_index()
# # # #     if df_final.index.name is None: # Assurer que le nom de l'index est défini
# # # #         df_final.index.name = 'Datetime'
# # # #     # --- FIN RÉINTÉGRATION COLAB ---


# # # #     # Nettoyage des colonnes temporaires
# # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date',
# # # #                                   'sunrise_time_local', 'sunset_time_local']
# # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # #     # Gestion intelligente de Rain_mm
# # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # #             df_final = create_rain_mm(df_final)
# # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # #         else:
# # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # #             if 'Rain_mm' not in df_final.columns:
# # # #                 df_final['Rain_mm'] = np.nan


# # # #     # Interpolation standard et bornage pour les variables numériques
# # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                      'Wind_Dir_Deg']

# # # #     for var in standard_vars:
# # # #         if var in df_final.columns:
# # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # #             if var in limits:
# # # #                 min_val = limits[var]['min']
# # # #                 max_val = limits[var]['max']
# # # #                 initial_nan_count = df_final[var].isna().sum()
# # # #                 df_final.loc[(df_final[var] < min_val) | (df_final[var] > max_val), var] = np.nan
# # # #                 if df_final[var].isna().sum() > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {df_final[var].isna().sum() - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # #             # --- DEBUG : Avant interpolation ---
# # # #             print(f"DEBUG (interpolation/variable): Interpolation de '{var}'. Type de l'index de df_final: {type(df_final.index)}")
# # # #             print(f"DEBUG (interpolation/variable): Index de df_final a des NaT? {df_final.index.isna().any()}")
# # # #             print(f"DEBUG (interpolation/variable): Premières 5 valeurs de l'index de df_final: {df_final.index[:5].tolist() if not df_final.empty else 'DataFrame vide'}")
            
# # # #             # --- RÉINTÉGRATION DE LA LOGIQUE DE COLAB POUR L'INTERPOLATION CONDITIONNELLE ---
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/variable): L'index n'est pas un DatetimeIndex pour l'interpolation de '{var}'. Utilisation de la méthode 'linear'.")
# # # #                 df_final[var] = df_final[var].interpolate(method='linear', limit_direction='both')
# # # #             # --- FIN RÉINTÉGRATION COLAB ---
# # # #             df_final[var] = df_final[var].bfill().ffill() # Pour les NaNs aux extrémités

# # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # #         if 'Solar_R_W/m^2' in limits:
# # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # #         if 'Is_Daylight' in df_final.columns:
# # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # #             if 'Rain_mm' in df_final.columns:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # #             else:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # #             # --- DEBUG : Avant interpolation solaire conditionnelle ---
# # # #             print(f"DEBUG (interpolation/solaire): Interpolation de 'Solar_R_W/m^2' (conditionnel). Type de l'index de df_final: {type(df_final.index)}")
# # # #             print(f"DEBUG (interpolation/solaire): Index de df_final a des NaT? {df_final.index.isna().any()}")
# # # #             print(f"DEBUG (interpolation/solaire): Premières 5 valeurs de l'index de df_final: {df_final.index[:5].tolist() if not df_final.empty else 'DataFrame vide'}")

# # # #             # --- RÉINTÉGRATION DE LA LOGIQUE DE COLAB POUR L'INTERPOLATION CONDITIONNELLE ---
# # # #             is_day = df_final['Is_Daylight'] # Définir is_day ici pour être sûr
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/solaire): L'index n'est pas un DatetimeIndex pour l'interpolation de 'Solar_R_W/m^2'. Utilisation de la méthode 'linear'.")
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both')
# # # #             # --- FIN RÉINTÉGRATION COLAB ---

# # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # #         else:
# # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # #             # --- RÉINTÉGRATION DE LA LOGIQUE DE COLAB POUR L'INTERPOLATION CONDITIONNELLE ---
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()
# # # #             else:
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both').bfill().ffill()
# # # #             # --- FIN RÉINTÉGRATION COLAB ---

# # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # #     missing_after_interp = df_final.isna().sum()
# # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # #     if not columns_with_missing.empty:
# # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # #     else:
# # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # #     return df_final


# # # # # def _load_and_prepare_gps_data():
# # # # #     """
# # # # #     Charge les données GPS des stations depuis un fichier CSV (ou crée un DataFrame vide).
# # # # #     Cette fonction est appelée au démarrage de l'application.
# # # # #     """
# # # # #     # Chemin vers le fichier CSV avec les coordonnées GPS
# # # # #     gps_data_path = 'station_coordinates.csv' # Assurez-vous que ce fichier existe

# # # # #     if os.path.exists(gps_data_path):
# # # # #         try:
# # # # #             df_gps_info = pd.read_csv(gps_data_path)
# # # # #             required_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # # #             for col in required_cols:
# # # # #                 if col not in df_gps_info.columns:
# # # # #                     raise ValueError(f"Colonne '{col}' manquante dans '{gps_data_path}'.")
            
# # # # #             # S'assurer que les colonnes numériques sont bien des nombres
# # # # #             df_gps_info['Lat'] = pd.to_numeric(df_gps_info['Lat'], errors='coerce')
# # # # #             df_gps_info['Long'] = pd.to_numeric(df_gps_info['Long'], errors='coerce')
# # # # #             df_gps_info.dropna(subset=['Lat', 'Long'], inplace=True)

# # # # #             if df_gps_info.empty:
# # # # #                 warnings.warn(f"Le fichier {gps_data_path} est vide ou ne contient pas de données GPS valides après nettoyage.")
# # # # #                 return pd.DataFrame({'Station': [], 'Lat': [], 'Long': [], 'Timezone': []})

# # # # #             return df_gps_info
# # # # #         except Exception as e:
# # # # #             warnings.warn(f"Erreur lors du chargement ou du traitement de {gps_data_path}: {e}. Création d'un DataFrame GPS vide.")
# # # # #             return pd.DataFrame({'Station': [], 'Lat': [], 'Long': [], 'Timezone': []})
# # # # #     else:
# # # # #         warnings.warn(f"Fichier '{gps_data_path}' non trouvé. Création d'un DataFrame GPS vide.")
# # # # #         return pd.DataFrame({'Station': [], 'Lat': [], 'Long': [], 'Timezone': []})


# # # # ################################
# # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # #         raise ValueError(
# # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # #         )

# # # #     def convert_row(row):
# # # #         try:
# # # #             zone = int(row['zone'])
# # # #             hemisphere = str(row['hemisphere']).upper()
# # # #             is_northern = hemisphere == 'N'

# # # #             proj_utm = CRS.from_proj4(
# # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # #             )
# # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # #         except Exception as e:
# # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # #     return df_copy

# # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # #     """
# # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
# # # #     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

# # # #     Returns:
# # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # #     """
# # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # #     data_dir = 'data'
# # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # #     files_info = [
# # # #         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
# # # #         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
# # # #         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
# # # #     ]

# # # #     loaded_dfs = []

# # # #     for file_info in files_info:
# # # #         output_file_path = os.path.join(data_dir, file_info['name'])
        
# # # #         if not os.path.exists(output_file_path):
# # # #             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
# # # #             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
# # # #             print(f"Téléchargement de {file_info['bassin']} terminé.")
# # # #         else:
# # # #             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
# # # #         loaded_dfs.append(pd.read_excel(output_file_path))

# # # #     vea_sissili_bassin = loaded_dfs[0]
# # # #     dano_bassin = loaded_dfs[1]
# # # #     dassari_bassin = loaded_dfs[2]

# # # #     # 4. Prétraitement des DataFrames (votre code original)
# # # #     print("Début du prétraitement des données de stations...")
    
# # # #     # Vea Sissili
# # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # #     # Dassari
# # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # #     dano_bassin['zone'] = 30
# # # #     dano_bassin['hemisphere'] = 'N'
# # # #     dassari_bassin['zone'] = 31
# # # #     dassari_bassin['hemisphere'] = 'N'

# # # #     # Application de la fonction de conversion UTM vers GPS
# # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # #     # Ajout des fuseaux horaires
# # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # #     # Fusion de tous les bassins
# # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # #     # Renommer 'Name' en 'Station'
# # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # #     initial_rows_count = len(bassins)
# # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # #     if len(bassins) < initial_rows_count:
# # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # #     if 'Station' in bassins.columns:
# # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # #     return bassins
# # # # ####################


# # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
# # # #     Conserve la première occurrence en cas de doublon.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée.

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame sans doublons.
# # # #     """
# # # #     if 'Station' in df.columns and 'Datetime' in df.columns:
# # # #         initial_rows = len(df)
# # # #         df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
# # # #         if len(df_cleaned) < initial_rows:
# # # #             warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
# # # #         return df_cleaned
# # # #     else:
# # # #         warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
# # # #         return df

# # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # #     """
# # # #     Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     # S'assurer que l'index est bien un DatetimeIndex pour éviter les problèmes
# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
# # # #         try:
# # # #             df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
# # # #             df_processed = df_processed[df_processed.index.notna()] # Supprimer les NaT dans l'index
# # # #             if df_processed.empty:
# # # #                 raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
# # # #         except Exception as e:
# # # #             raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

# # # #     for var, vals in limits.items():
# # # #         if var in df_processed.columns:
# # # #             min_val = vals.get('min')
# # # #             max_val = vals.get('max')
# # # #             if min_val is not None or max_val is not None:
# # # #                 initial_nan_count = df_processed[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_processed.loc[df_processed[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
# # # #                 # Compter les valeurs remplacées
# # # #                 new_nan_count = df_processed[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs aberrantes dans '{var}' par NaN.")
# # # #     return df_processed

# # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return None

# # # #     # Agrégation des données
# # # #     if periode == 'Journalière':
# # # #         resampled_df = filtered_df[variable].resample('D').mean()
# # # #     elif periode == 'Hebdomadaire':
# # # #         resampled_df = filtered_df[variable].resample('W').mean()
# # # #     elif periode == 'Mensuelle':
# # # #         resampled_df = filtered_df[variable].resample('M').mean()
# # # #     elif periode == 'Annuelle':
# # # #         resampled_df = filtered_df[variable].resample('Y').mean()
# # # #     else:
# # # #         resampled_df = filtered_df[variable] # Pas de resample si 'periode' inconnue

# # # #     # Supprimer les lignes où la variable est NaN après le resample
# # # #     resampled_df = resampled_df.dropna()

# # # #     if resampled_df.empty:
# # # #         return None

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     color = colors.get(station, '#1f77b4') # Couleur par défaut si non trouvée

# # # #     import plotly.graph_objects as go # Importation locale
# # # #     import plotly.io as pio # Importation locale

# # # #     fig = go.Figure()
# # # #     fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                              mode='lines', name=f'{variable_meta["Nom"]} - {station}',
# # # #                              line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)

# # # # def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

# # # #     import plotly.graph_objects as go # Importation locale
# # # #     import plotly.io as pio # Importation locale

# # # #     fig = go.Figure()
    
# # # #     all_stations = df['Station'].unique()
# # # #     if len(all_stations) < 2:
# # # #         warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
# # # #         return None

# # # #     for station in all_stations:
# # # #         filtered_df = df[df['Station'] == station].copy()
# # # #         if filtered_df.empty:
# # # #             continue

# # # #         # Agrégation des données
# # # #         if periode == 'Journalière':
# # # #             resampled_df = filtered_df[variable].resample('D').mean()
# # # #         elif periode == 'Hebdomadaire':
# # # #             resampled_df = filtered_df[variable].resample('W').mean()
# # # #         elif periode == 'Mensuelle':
# # # #             resampled_df = filtered_df[variable].resample('M').mean()
# # # #         elif periode == 'Annuelle':
# # # #             resampled_df = filtered_df[variable].resample('Y').mean()
# # # #         else:
# # # #             resampled_df = filtered_df[variable]

# # # #         resampled_df = resampled_df.dropna()
# # # #         if resampled_df.empty:
# # # #             continue
        
# # # #         color = colors.get(station, '#1f77b4') # Couleur par défaut
# # # #         fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                                  mode='lines', name=station,
# # # #                                  line=dict(color=color)))

# # # #     if not fig.data: # Si aucune trace n'a été ajoutée
# # # #         return None

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     fig.update_layout(
# # # #         title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)


# # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return None

# # # #     # Sélectionne les variables numériques à normaliser
# # # #     # Exclure 'Station' et 'Is_Daylight' qui ne sont pas des variables à tracer comme ça
# # # #     numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight']]

# # # #     if not numerical_vars:
# # # #         warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
# # # #         return None

# # # #     # Normalisation Min-Max
# # # #     normalized_df = filtered_df[numerical_vars].copy()
# # # #     for col in normalized_df.columns:
# # # #         min_val = normalized_df[col].min()
# # # #         max_val = normalized_df[col].max()
# # # #         if max_val != min_val:
# # # #             normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
# # # #         else: # Si toutes les valeurs sont identiques, normaliser à 0.5 ou laisser NaN
# # # #             normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

# # # #     # Supprimer les lignes où toutes les variables normalisées sont NaN
# # # #     normalized_df = normalized_df.dropna(how='all')

# # # #     if normalized_df.empty:
# # # #         return None
    
# # # #     import plotly.graph_objects as go # Importation locale
# # # #     import plotly.io as pio # Importation locale

# # # #     fig = go.Figure()
# # # #     for var in normalized_df.columns:
# # # #         var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
# # # #         color = colors.get(var, None) # Utiliser la couleur spécifique à la variable

# # # #         fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
# # # #                                  mode='lines', name=var_meta['Nom'],
# # # #                                  line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
# # # #         xaxis_title="Date",
# # # #         yaxis_title="Valeur Normalisée (0-1)",
# # # #         hovermode="x unified",
# # # #         legend_title="Variables"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)

# # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # # #     groupées par station.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         warnings.warn("L'index n'est pas un DatetimeIndex dans daily_stats. Tentative de conversion.")
# # # #         try:
# # # #             df.index = pd.to_datetime(df.index, errors='coerce')
# # # #             df = df[df.index.notna()]
# # # #             if df.empty:
# # # #                 return pd.DataFrame() # Retourner un DataFrame vide si tout est supprimé
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de convertir l'index en DatetimeIndex pour daily_stats: {e}")
# # # #             return pd.DataFrame() # Retourner un DataFrame vide en cas d'échec critique

# # # #     # Sélectionner uniquement les colonnes numériques, en excluant 'Station' et 'Is_Daylight'
# # # #     numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Is_Daylight']]

# # # #     if not numerical_cols:
# # # #         return pd.DataFrame() # Retourner un DataFrame vide si aucune colonne numérique n'est trouvée

# # # #     # Grouper par station et par jour, puis calculer les statistiques
# # # #     agg_dict = {col: ['mean', 'min', 'max'] for col in numerical_cols if col != 'Rain_mm'}
# # # #     if 'Rain_mm' in numerical_cols:
# # # #         agg_dict['Rain_mm'] = ['mean', 'min', 'max', 'sum'] # Somme de la pluie


# # # #     daily_stats_df = df.groupby(['Station', df.index.date]).agg(agg_dict)
    
# # # #     # Apllatir les MultiIndex des colonnes après l'agrégation
# # # #     daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

# # # #     # Renommer l'index de date
# # # #     daily_stats_df.index.names = ['Station', 'Date']
    
# # # #     return daily_stats_df.reset_index()


# # # #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
# # # # import pandas as pd
# # # # from pyproj import CRS, Transformer
# # # # import pytz
# # # # from astral.location import LocationInfo
# # # # from astral import sun
# # # # import numpy as np
# # # # import warnings
# # # # import os # <-- Importation du module os
# # # # import gdown # <-- AJOUTÉ : Importation du module gdown pour le téléchargement

# # # # # Fonction utilitaire pour créer Rain_mm si nécessaire (dépend de votre implémentation)
# # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
# # # #     Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
# # # #     """
# # # #     df_copy = df.copy()
# # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
# # # #     elif 'Rain_01_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm']
# # # #     elif 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_02_mm']
# # # #     else:
# # # #         df_copy['Rain_mm'] = np.nan # Crée la colonne même si aucune source n'est disponible
# # # #         warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents pour créer 'Rain_mm'. 'Rain_mm' est rempli de NaN.")
# # # #     return df_copy


# # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # #     """
# # # #     df_copy = df.copy() # Travailler sur une copie

# # # #     # Prioriser la colonne 'Date' si elle existe pour VEA_SISSILI ou si pas de colonnes Y/M/D/H/M
# # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not any(col in df_copy.columns for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'])):
# # # #         try:
# # # #             # Tenter de convertir 'Date' en datetime, avec gestion des erreurs
# # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime pour le bassin {bassin}: {e}")
# # # #             df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
# # # #     else:
# # # #         # Logique pour les colonnes Year, Month, Day, Hour, Minute
# # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # #         # Convertir les colonnes en numérique, forçant les erreurs en NaN
# # # #         for col in date_cols:
# # # #             if col in df_copy.columns:
# # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # #         # Créer une série de strings de date/heure et la convertir
# # # #         try:
# # # #             # Filtrer les colonnes de date qui existent réellement dans le DataFrame
# # # #             existing_date_components = [col for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'] if col in df_copy.columns]
            
# # # #             if not existing_date_components:
# # # #                 raise ValueError("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute) trouvée.")

# # # #             # Créer une chaîne de caractères à partir des composantes existantes, avec un remplissage par défaut
# # # #             # pour les colonnes manquantes afin que to_datetime puisse les interpréter.
# # # #             # Par exemple, si 'Minute' est manquante, utiliser '00'
# # # #             date_strings = df_copy.apply(
# # # #                 lambda row: f"{int(row.get('Year', 2000))}-"
# # # #                             f"{int(row.get('Month', 1)):02d}-"
# # # #                             f"{int(row.get('Day', 1)):02d} "
# # # #                             f"{int(row.get('Hour', 0)):02d}:"
# # # #                             f"{int(row.get('Minute', 0)):02d}",
# # # #                 axis=1
# # # #             )
# # # #             df_copy['Datetime'] = pd.to_datetime(date_strings, errors='coerce')
            
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
# # # #             df_copy['Datetime'] = pd.NaT # Assigner NaT en cas d'erreur
            
# # # #     # Extraire/recréer les composantes de date/heure si Datetime a été créé et n'est pas vide
# # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # #         # Recréer la colonne 'Date' comme objet date (sans heure) si elle n'est pas déjà présente ou si elle est de type non-date
# # # #         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
# # # #              df_copy['Date'] = df_copy['Datetime'].dt.date
# # # #     else:
# # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

# # # #     return df_copy

# # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Effectue toutes les interpolations météorologiques en une seule passe.
# # # #     Son index DOIT ÊTRE un DatetimeIndex.
# # # #     Il doit également contenir une colonne 'Station'.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée. Son index DOIT ÊTRE un DatetimeIndex.
# # # #                            Il doit également contenir une colonne 'Station'.
# # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame original avec les données interpolées et
# # # #                       la colonne 'Is_Daylight' calculée, ainsi que la durée du jour.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     # --- Vérifications d'intégrité de l'index au début de la fonction ---
# # # #     print(f"DEBUG (interpolation): Index du DataFrame initial avant nettoyage des NaT: {type(df_processed.index)}")
# # # #     print(f"DEBUG (interpolation): Dtypes du DataFrame initial:\n{df_processed.dtypes}")
    
# # # #     # Supprime les lignes où l'index Datetime est NaT (valeur manquante)
# # # #     initial_rows = len(df_processed)
# # # #     df_processed = df_processed[df_processed.index.notna()]
# # # #     if len(df_processed) == 0:
# # # #         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
# # # #     if initial_rows - len(df_processed) > 0:
# # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")

# # # #     print(f"DEBUG (interpolation): Index du DataFrame après nettoyage des NaT: {type(df_processed.index)}")
# # # #     print(f"DEBUG (interpolation): Premières 5 valeurs de l'index après nettoyage des NaT: {df_processed.index[:5].tolist() if not df_processed.empty else 'DataFrame vide'}")

# # # #     # --- Pré-vérification et préparation des données GPS (inchangée) ---
# # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # #         raise ValueError(
# # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # #         )

# # # #     if not df_gps['Station'].is_unique:
# # # #         print("Avertissement: La colonne 'Station' dans df_gps contient des noms de station dupliqués.")
# # # #         print("Ceci peut entraîner des comportements inattendus ou des stations non reconnues.")
# # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # #         print(f"Suppression de {len(df_gps) - len(df_gps_unique)} doublons dans df_gps (en gardant la première occurrence).")
# # # #     else:
# # # #         df_gps_unique = df_gps.copy()

# # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # #     for col in numerical_cols:
# # # #         if col in df_processed.columns:
# # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # #     df_processed_parts = []
# # # #     stations_fallback = []

# # # #     for station_name, group in df_processed.groupby('Station'):
# # # #         group_copy = group.copy()

# # # #         print(f"DEBUG (interpolation/groupby): Début du traitement du groupe '{station_name}'.")
# # # #         print(f"DEBUG (interpolation/groupby): Type de l'index du groupe: {type(group_copy.index)}")
# # # #         print(f"DEBUG (interpolation/groupby): Dtypes du groupe '{station_name}':\n{group_copy.dtypes}")
# # # #         print(f"DEBUG (interpolation/groupby): Index du groupe '{station_name}' a des NaT? {group_copy.index.isna().any()}")
# # # #         print(f"DEBUG (interpolation/groupby): Premières 5 valeurs de l'index du groupe '{station_name}': {group_copy.index[:5].tolist() if not group_copy.empty else 'DataFrame vide'}")


# # # #         group_copy = group_copy.sort_index()

# # # #         gps_data = gps_info_dict.get(station_name)
# # # #         apply_fixed_daylight = True # Par défaut, utiliser le fallback

# # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # #             lat = gps_data['Lat']
# # # #             long = gps_data['Long']
# # # #             timezone_str = gps_data['Timezone']

# # # #             try:
# # # #                 tz = pytz.timezone(timezone_str)
# # # #                 # Localiser l'index si nécessaire
# # # #                 if group_copy.index.tz is None:
# # # #                     group_copy.index = group_copy.index.tz_localize(tz, ambiguous='NaT', nonexistent='NaT')
# # # #                 else:
# # # #                     group_copy.index = group_copy.index.tz_convert(tz)
# # # #                 print(f"Index Datetime localisé au fuseau horaire : {tz}")


# # # #                 daily_sun_info = {}
# # # #                 for date_only in group_copy.index.normalize().unique():
# # # #                     loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
# # # #                     s = sun.sun(loc.observer, date=date_only, tzinfo=loc.tzinfo)
# # # #                     daily_sun_info[date_only.date()] = {
# # # #                         'sunrise': s['sunrise'],
# # # #                         'sunset': s['sunset']
# # # #                     }

# # # #                 group_copy.loc[:, 'sunrise_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunrise'))
# # # #                 group_copy.loc[:, 'sunset_time_local'] = group_copy.index.map(lambda x: daily_sun_info.get(x.date(), {}).get('sunset'))

# # # #                 if group_copy.index.tz is not None:
# # # #                      if not group_copy['sunrise_time_local'].isnull().all():
# # # #                          group_copy.loc[:, 'sunrise_time_local'] = group_copy['sunrise_time_local'].dt.tz_convert(group_copy.index.tz)
# # # #                      if not group_copy['sunset_time_local'].isnull().all():
# # # #                          group_copy.loc[:, 'sunset_time_local'] = group_copy['sunset_time_local'].dt.tz_convert(group_copy.index.tz)


# # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index >= group_copy['sunrise_time_local']) & \
# # # #                                                   (group_copy.index < group_copy['sunset_time_local'])

# # # #                 daylight_timedelta = group_copy['sunset_time_local'] - group_copy['sunrise_time_local']
                
# # # #                 group_copy.loc[:, 'Daylight_Duration'] = pd.Series(dtype='object', index=group_copy.index)
                
# # # #                 valid_times = daylight_timedelta.notna()
# # # #                 group_copy.loc[valid_times, 'Daylight_Duration'] = daylight_timedelta[valid_times].apply(
# # # #                     lambda td: (pd.to_datetime('00:00:00') + td).strftime('%H:%M:%S') if pd.notna(td) else np.nan
# # # #                 )
# # # #                 print(f"Lever et coucher du soleil calculés pour {station_name}.")
# # # #                 apply_fixed_daylight = False

# # # #             except Exception as e:
# # # #                 print(f"Erreur lors du calcul du lever/coucher du soleil avec Astral pour {station_name}: {e}.")
# # # #                 stations_fallback.append(station_name)
# # # #         else:
# # # #             print(f"Avertissement: Coordonnées ou Fuseau horaire manquants/invalides pour le site '{station_name}' dans df_gps. Index localisé à UTC.")
# # # #             stations_fallback.append(station_name)

# # # #         if apply_fixed_daylight:
# # # #             if group_copy.index.tz is None:
# # # #                 group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # #             print(f"Utilisation de l'indicateur jour/nuit fixe (7h-18h) pour {station_name}.")


# # # #         df_processed_parts.append(group_copy)

# # # #     if not df_processed_parts:
# # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # #     df_final = pd.concat(df_processed_parts)

# # # #     # --- RÉINTÉGRATION DE LA LOGIQUE DE COLAB POUR LA VÉRIFICATION DE L'INDEX APRÈS CONCATÉNATION ---
# # # #     print(f"DEBUG (interpolation/concat): Index du DataFrame final après concaténation: {type(df_final.index)}")
# # # #     if not isinstance(df_final.index, pd.DatetimeIndex):
# # # #         print("Avertissement (interpolation/concat): L'index n'est pas un DatetimeIndex après concaténation. Tentative de correction...")
# # # #         if 'Datetime' in df_final.columns: # Vérifier si la colonne 'Datetime' est dispo pour recréer l'index
# # # #             df_final = df_final.set_index('Datetime', drop=True)
# # # #             df_final.index.name = 'Datetime'
# # # #             print("DEBUG (interpolation/concat): Index 'Datetime' recréé et trié avec succès.")
# # # #         else:
# # # #             print("Erreur (interpolation/concat): Colonne 'Datetime' manquante pour définir l'index après concaténation. Impossible d'interpoler avec la méthode 'time'.")
# # # #             # Si Datetime est vraiment absent ici, l'interpolation 'time' échouera.
# # # #             # Ne pas retourner df_final tout de suite pour permettre les print suivants, mais l'erreur se produira.
    
# # # #     df_final = df_final.sort_index()
# # # #     if df_final.index.name is None: # Assurer que le nom de l'index est défini
# # # #         df_final.index.name = 'Datetime'
# # # #     # --- FIN RÉINTÉGRATION COLAB ---


# # # #     # Nettoyage des colonnes temporaires
# # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date',
# # # #                                   'sunrise_time_local', 'sunset_time_local']
# # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # #     # Gestion intelligente de Rain_mm
# # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # #             df_final = create_rain_mm(df_final)
# # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # #         else:
# # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # #             if 'Rain_mm' not in df_final.columns:
# # # #                 df_final['Rain_mm'] = np.nan


# # # #     # Interpolation standard et bornage pour les variables numériques
# # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                      'Wind_Dir_Deg']

# # # #     for var in standard_vars:
# # # #         if var in df_final.columns:
# # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # #             if var in limits:
# # # #                 min_val = limits[var]['min']
# # # #                 max_val = limits[var]['max']
# # # #                 initial_nan_count = df_final[var].isna().sum()
# # # #                 df_final.loc[(df_final[var] < min_val) | (df_final[var] > max_val), var] = np.nan
# # # #                 if df_final[var].isna().sum() > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {df_final[var].isna().sum() - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # #             # --- DEBUG : Avant interpolation ---
# # # #             print(f"DEBUG (interpolation/variable): Interpolation de '{var}'. Type de l'index de df_final: {type(df_final.index)}")
# # # #             print(f"DEBUG (interpolation/variable): Index de df_final a des NaT? {df_final.index.isna().any()}")
# # # #             print(f"DEBUG (interpolation/variable): Premières 5 valeurs de l'index de df_final: {df_final.index[:5].tolist() if not df_final.empty else 'DataFrame vide'}")
            
# # # #             # --- RÉINTÉGRATION DE LA LOGIQUE DE COLAB POUR L'INTERPOLATION CONDITIONNELLE ---
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/variable): L'index n'est pas un DatetimeIndex pour l'interpolation de '{var}'. Utilisation de la méthode 'linear'.")
# # # #                 df_final[var] = df_final[var].interpolate(method='linear', limit_direction='both')
# # # #             # --- FIN RÉINTÉGRATION COLAB ---
# # # #             df_final[var] = df_final[var].bfill().ffill() # Pour les NaNs aux extrémités

# # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # #         if 'Solar_R_W/m^2' in limits:
# # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # #         if 'Is_Daylight' in df_final.columns:
# # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # #             if 'Rain_mm' in df_final.columns:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # #             else:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # #             # --- DEBUG : Avant interpolation solaire conditionnelle ---
# # # #             print(f"DEBUG (interpolation/solaire): Interpolation de 'Solar_R_W/m^2' (conditionnel). Type de l'index de df_final: {type(df_final.index)}")
# # # #             print(f"DEBUG (interpolation/solaire): Index de df_final a des NaT? {df_final.index.isna().any()}")
# # # #             print(f"DEBUG (interpolation/solaire): Premières 5 valeurs de l'index de df_final: {df_final.index[:5].tolist() if not df_final.empty else 'DataFrame vide'}")

# # # #             # --- RÉINTÉGRATION DE LA LOGIQUE DE COLAB POUR L'INTERPOLATION CONDITIONNELLE ---
# # # #             is_day = df_final['Is_Daylight'] # Définir is_day ici pour être sûr
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/solaire): L'index n'est pas un DatetimeIndex pour l'interpolation de 'Solar_R_W/m^2'. Utilisation de la méthode 'linear'.")
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both')
# # # #             # --- FIN RÉINTÉGRATION COLAB ---

# # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # #         else:
# # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # #             # --- RÉINTÉGRATION DE LA LOGIQUE DE COLAB POUR L'INTERPOLATION CONDITIONNELLE ---
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()
# # # #             else:
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both').bfill().ffill()
# # # #             # --- FIN RÉINTÉGRATION COLAB ---

# # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # #     missing_after_interp = df_final.isna().sum()
# # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # #     if not columns_with_missing.empty:
# # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # #     else:
# # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # #     return df_final


# # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # #         raise ValueError(
# # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # #         )

# # # #     def convert_row(row):
# # # #         try:
# # # #             zone = int(row['zone'])
# # # #             hemisphere = str(row['hemisphere']).upper()
# # # #             is_northern = hemisphere == 'N'

# # # #             proj_utm = CRS.from_proj4(
# # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # #             )
# # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # #         except Exception as e:
# # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # #     return df_copy

# # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # #     """
# # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
# # # #     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

# # # #     Returns:
# # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # #     """
# # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # #     data_dir = 'data'
# # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # #     files_info = [
# # # #         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
# # # #         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
# # # #         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
# # # #     ]

# # # #     loaded_dfs = []

# # # #     for file_info in files_info:
# # # #         output_file_path = os.path.join(data_dir, file_info['name'])
        
# # # #         if not os.path.exists(output_file_path):
# # # #             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
# # # #             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
# # # #             print(f"Téléchargement de {file_info['bassin']} terminé.")
# # # #         else:
# # # #             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
# # # #         loaded_dfs.append(pd.read_excel(output_file_path))

# # # #     vea_sissili_bassin = loaded_dfs[0]
# # # #     dano_bassin = loaded_dfs[1]
# # # #     dassari_bassin = loaded_dfs[2]

# # # #     # 4. Prétraitement des DataFrames (votre code original)
# # # #     print("Début du prétraitement des données de stations...")
    
# # # #     # Vea Sissili
# # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # #     # Dassari
# # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # #     dano_bassin['zone'] = 30
# # # #     dano_bassin['hemisphere'] = 'N'
# # # #     dassari_bassin['zone'] = 31
# # # #     dassari_bassin['hemisphere'] = 'N'

# # # #     # Application de la fonction de conversion UTM vers GPS
# # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # #     # Ajout des fuseaux horaires
# # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # #     # Fusion de tous les bassins
# # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # #     # Renommer 'Name' en 'Station'
# # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # #     initial_rows_count = len(bassins)
# # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # #     if len(bassins) < initial_rows_count:
# # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # #     if 'Station' in bassins.columns:
# # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # #     return bassins


# # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
# # # #     Conserve la première occurrence en cas de doublon.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée.

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame sans doublons.
# # # #     """
# # # #     if 'Station' in df.columns and 'Datetime' in df.columns:
# # # #         initial_rows = len(df)
# # # #         df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
# # # #         if len(df_cleaned) < initial_rows:
# # # #             warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
# # # #         return df_cleaned
# # # #     else:
# # # #         warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
# # # #         return df

# # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # #     """
# # # #     Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     # S'assurer que l'index est bien un DatetimeIndex pour éviter les problèmes
# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
# # # #         try:
# # # #             df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
# # # #             df_processed = df_processed[df_processed.index.notna()] # Supprimer les NaT dans l'index
# # # #             if df_processed.empty:
# # # #                 raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
# # # #         except Exception as e:
# # # #             raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

# # # #     for var, vals in limits.items():
# # # #         if var in df_processed.columns:
# # # #             min_val = vals.get('min')
# # # #             max_val = vals.get('max')
# # # #             if min_val is not None or max_val is not None:
# # # #                 initial_nan_count = df_processed[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_processed.loc[df_processed[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
# # # #                 # Compter les valeurs remplacées
# # # #                 new_nan_count = df_processed[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs aberrantes dans '{var}' par NaN.")
# # # #     return df_processed

# # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return None

# # # #     # Agrégation des données
# # # #     if periode == 'Journalière':
# # # #         resampled_df = filtered_df[variable].resample('D').mean()
# # # #     elif periode == 'Hebdomadaire':
# # # #         resampled_df = filtered_df[variable].resample('W').mean()
# # # #     elif periode == 'Mensuelle':
# # # #         resampled_df = filtered_df[variable].resample('M').mean()
# # # #     elif periode == 'Annuelle':
# # # #         resampled_df = filtered_df[variable].resample('Y').mean()
# # # #     else:
# # # #         resampled_df = filtered_df[variable] # Pas de resample si 'periode' inconnue

# # # #     # Supprimer les lignes où la variable est NaN après le resample
# # # #     resampled_df = resampled_df.dropna()

# # # #     if resampled_df.empty:
# # # #         return None

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     color = colors.get(station, '#1f77b4') # Couleur par défaut si non trouvée

# # # #     import plotly.graph_objects as go # Importation locale
# # # #     import plotly.io as pio # Importation locale

# # # #     fig = go.Figure()
# # # #     fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                              mode='lines', name=f'{variable_meta["Nom"]} - {station}',
# # # #                              line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)

# # # # def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

# # # #     import plotly.graph_objects as go # Importation locale
# # # #     import plotly.io as pio # Importation locale

# # # #     fig = go.Figure()
    
# # # #     all_stations = df['Station'].unique()
# # # #     if len(all_stations) < 2:
# # # #         warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
# # # #         return None

# # # #     for station in all_stations:
# # # #         filtered_df = df[df['Station'] == station].copy()
# # # #         if filtered_df.empty:
# # # #             continue

# # # #         # Agrégation des données
# # # #         if periode == 'Journalière':
# # # #             resampled_df = filtered_df[variable].resample('D').mean()
# # # #         elif periode == 'Hebdomadaire':
# # # #             resampled_df = filtered_df[variable].resample('W').mean()
# # # #         elif periode == 'Mensuelle':
# # # #             resampled_df = filtered_df[variable].resample('M').mean()
# # # #         elif periode == 'Annuelle':
# # # #             resampled_df = filtered_df[variable].resample('Y').mean()
# # # #         else:
# # # #             resampled_df = filtered_df[variable]

# # # #         resampled_df = resampled_df.dropna()
# # # #         if resampled_df.empty:
# # # #             continue
        
# # # #         color = colors.get(station, '#1f77b4') # Couleur par défaut
# # # #         fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                                  mode='lines', name=station,
# # # #                                  line=dict(color=color)))

# # # #     if not fig.data: # Si aucune trace n'a été ajoutée
# # # #         return None

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     fig.update_layout(
# # # #         title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)


# # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return None

# # # #     # Sélectionne les variables numériques à normaliser
# # # #     # Exclure 'Station' et 'Is_Daylight' qui ne sont pas des variables à tracer comme ça
# # # #     numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight']]

# # # #     if not numerical_vars:
# # # #         warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
# # # #         return None

# # # #     # Normalisation Min-Max
# # # #     normalized_df = filtered_df[numerical_vars].copy()
# # # #     for col in normalized_df.columns:
# # # #         min_val = normalized_df[col].min()
# # # #         max_val = normalized_df[col].max()
# # # #         if max_val != min_val:
# # # #             normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
# # # #         else: # Si toutes les valeurs sont identiques, normaliser à 0.5 ou laisser NaN
# # # #             normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

# # # #     # Supprimer les lignes où toutes les variables normalisées sont NaN
# # # #     normalized_df = normalized_df.dropna(how='all')

# # # #     if normalized_df.empty:
# # # #         return None
    
# # # #     import plotly.graph_objects as go # Importation locale
# # # #     import plotly.io as pio # Importation locale

# # # #     fig = go.Figure()
# # # #     for var in normalized_df.columns:
# # # #         var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
# # # #         color = colors.get(var, None) # Utiliser la couleur spécifique à la variable

# # # #         fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
# # # #                                  mode='lines', name=var_meta['Nom'],
# # # #                                  line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
# # # #         xaxis_title="Date",
# # # #         yaxis_title="Valeur Normalisée (0-1)",
# # # #         hovermode="x unified",
# # # #         legend_title="Variables"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)

# # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # # #     groupées par station.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         warnings.warn("L'index n'est pas un DatetimeIndex dans daily_stats. Tentative de conversion.")
# # # #         try:
# # # #             df.index = pd.to_datetime(df.index, errors='coerce')
# # # #             df = df[df.index.notna()]
# # # #             if df.empty:
# # # #                 return pd.DataFrame() # Retourner un DataFrame vide si tout est supprimé
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de convertir l'index en DatetimeIndex pour daily_stats: {e}")
# # # #             return pd.DataFrame() # Retourner un DataFrame vide en cas d'échec critique

# # # #     # Sélectionner uniquement les colonnes numériques, en excluant 'Station' et 'Is_Daylight'
# # # #     numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Is_Daylight']]

# # # #     if not numerical_cols:
# # # #         return pd.DataFrame() # Retourner un DataFrame vide si aucune colonne numérique n'est trouvée

# # # #     # Grouper par station et par jour, puis calculer les statistiques
# # # #     agg_dict = {col: ['mean', 'min', 'max'] for col in numerical_cols if col != 'Rain_mm'}
# # # #     if 'Rain_mm' in numerical_cols:
# # # #         agg_dict['Rain_mm'] = ['mean', 'min', 'max', 'sum'] # Somme de la pluie


# # # #     daily_stats_df = df.groupby(['Station', df.index.date]).agg(agg_dict)
    
# # # #     # Apllatir les MultiIndex des colonnes après l'agrégation
# # # #     daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

# # # #     # Renommer l'index de date
# # # #     daily_stats_df.index.names = ['Station', 'Date']
    
# # # #     return daily_stats_df.reset_index()

# # # #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@



# # # # import pandas as pd
# # # # from pyproj import CRS, Transformer
# # # # import pytz
# # # # from astral.location import LocationInfo
# # # # from astral import sun
# # # # import numpy as np
# # # # import warnings
# # # # import os
# # # # import gdown # Importation du module gdown pour le téléchargement

# # # # # Fonction utilitaire pour créer Rain_mm si nécessaire
# # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
# # # #     Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
# # # #     """
# # # #     df_copy = df.copy()
# # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
# # # #     elif 'Rain_01_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm']
# # # #     elif 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_02_mm']
# # # #     else:
# # # #         df_copy['Rain_mm'] = np.nan # Crée la colonne même si aucune source n'est disponible
# # # #         warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents pour créer 'Rain_mm'. 'Rain_mm' est rempli de NaN.")
# # # #     return df_copy


# # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     # Prioriser la colonne 'Date' si elle existe pour VEA_SISSILI ou si pas de colonnes Y/M/D/H/M
# # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not any(col in df_copy.columns for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'])):
# # # #         try:
# # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime pour le bassin {bassin}: {e}")
# # # #             df_copy['Datetime'] = pd.NaT
# # # #     else:
# # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # #         for col in date_cols:
# # # #             if col in df_copy.columns:
# # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # #         try:
# # # #             existing_date_components = [col for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'] if col in df_copy.columns]
            
# # # #             if not existing_date_components:
# # # #                 raise ValueError("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute) trouvée.")

# # # #             date_strings = df_copy.apply(
# # # #                 lambda row: f"{int(row.get('Year', 2000))}-"
# # # #                             f"{int(row.get('Month', 1)):02d}-"
# # # #                             f"{int(row.get('Day', 1)):02d} "
# # # #                             f"{int(row.get('Hour', 0)):02d}:"
# # # #                             f"{int(row.get('Minute', 0)):02d}",
# # # #                 axis=1
# # # #             )
# # # #             df_copy['Datetime'] = pd.to_datetime(date_strings, errors='coerce')
            
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
# # # #             df_copy['Datetime'] = pd.NaT
            
# # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # #         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
# # # #              df_copy['Date'] = df_copy['Datetime'].dt.date
# # # #     else:
# # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

# # # #     return df_copy

# # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Effectue toutes les interpolations météorologiques en une seule passe.
# # # #     Cette fonction DOIT recevoir un DataFrame avec une colonne 'Datetime' (et non un DatetimeIndex).
# # # #     Il doit également contenir une colonne 'Station'.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée avec la colonne 'Datetime'.
# # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame original avec les données interpolées,
# # # #                       la colonne 'Is_Daylight' calculée, la durée du jour, et un DatetimeIndex.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     # Assurez-vous que 'Datetime' est une colonne de type datetime et nettoyez les NaT
# # # #     if 'Datetime' not in df_processed.columns:
# # # #         raise ValueError("La colonne 'Datetime' est manquante dans le DataFrame d'entrée pour l'interpolation.")
    
# # # #     df_processed['Datetime'] = pd.to_datetime(df_processed['Datetime'], errors='coerce')
# # # #     df_processed.dropna(subset=['Datetime'], inplace=True)
# # # #     if df_processed.empty:
# # # #         raise ValueError("Après nettoyage des dates invalides, le DataFrame est vide. Impossible de procéder à l'interpolation.")
    
# # # #     print(f"DEBUG (interpolation): Type de la colonne 'Datetime' du DataFrame initial: {df_processed['Datetime'].dtype}")
# # # #     print(f"DEBUG (interpolation): Premières 5 valeurs de la colonne 'Datetime' après nettoyage des NaT: {df_processed['Datetime'].head(5).tolist() if not df_processed.empty else 'DataFrame vide'}")

# # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # #         raise ValueError(
# # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # #         )

# # # #     if not df_gps['Station'].is_unique:
# # # #         print("Avertissement: La colonne 'Station' dans df_gps contient des noms de station dupliqués.")
# # # #         print("Ceci peut entraîner des comportements inattendus ou des stations non reconnues.")
# # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # #         print(f"Suppression de {len(df_gps) - len(df_gps_unique)} doublons dans df_gps (en gardant la première occurrence).")
# # # #     else:
# # # #         df_gps_unique = df_gps.copy()

# # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # #     for col in numerical_cols:
# # # #         if col in df_processed.columns:
# # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # #     df_processed_parts = []

# # # #     for station_name, group in df_processed.groupby('Station'):
# # # #         group_copy = group.copy()
# # # #         print(f"DEBUG (interpolation/groupby): Début du traitement du groupe '{station_name}'.")
        
# # # #         # Force localization of 'Datetime' column to UTC for consistency within the group
# # # #         if group_copy['Datetime'].dt.tz is None:
# # # #             group_copy['Datetime'] = group_copy['Datetime'].dt.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #         else:
# # # #             group_copy['Datetime'] = group_copy['Datetime'].dt.tz_convert('UTC')
# # # #         print(f"DEBUG (interpolation/groupby): Colonne Datetime pour '{station_name}' localisée à UTC. Dtype: {group_copy['Datetime'].dtype}")

# # # #         # --- Astral calculations using local timezone ---
# # # #         apply_fixed_daylight = True
# # # #         gps_data = gps_info_dict.get(station_name)

# # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # #             lat = gps_data['Lat']
# # # #             long = gps_data['Long']
# # # #             timezone_str = gps_data['Timezone']

# # # #             try:
# # # #                 # Convert Datetime to local timezone for astral calculations ONLY
# # # #                 local_tz = pytz.timezone(timezone_str)
# # # #                 datetime_local = group_copy['Datetime'].dt.tz_convert(local_tz)

# # # #                 daily_sun_info = {}
# # # #                 for date_only in datetime_local.dt.normalize().unique():
# # # #                     loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
# # # #                     s = sun.sun(loc.observer, date=date_only.date(), tzinfo=loc.tzinfo)
# # # #                     daily_sun_info[date_only.date()] = {
# # # #                         'sunrise': s['sunrise'],
# # # #                         'sunset': s['sunset']
# # # #                     }

# # # #                 group_copy.loc[:, 'sunrise_time_local'] = datetime_local.dt.date.map(lambda x: daily_sun_info.get(x, {}).get('sunrise'))
# # # #                 group_copy.loc[:, 'sunset_time_local'] = datetime_local.dt.date.map(lambda x: daily_sun_info.get(x, {}).get('sunset'))

# # # #                 # Convert sunrise/sunset times back to UTC for consistent comparison with group_copy['Datetime']
# # # #                 if not group_copy['sunrise_time_local'].isnull().all():
# # # #                      group_copy.loc[:, 'sunrise_time_local'] = group_copy['sunrise_time_local'].dt.tz_convert('UTC')
# # # #                 if not group_copy['sunset_time_local'].isnull().all():
# # # #                      group_copy.loc[:, 'sunset_time_local'] = group_copy['sunset_time_local'].dt.tz_convert('UTC')


# # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy['Datetime'] >= group_copy['sunrise_time_local']) & \
# # # #                                                   (group_copy['Datetime'] < group_copy['sunset_time_local'])

# # # #                 daylight_timedelta = group_copy['sunset_time_local'] - group_copy['sunrise_time_local']
# # # #                 group_copy.loc[:, 'Daylight_Duration'] = daylight_timedelta.astype(str).replace('NaT', np.nan) # Store as string
# # # #                 print(f"Lever et coucher du soleil calculés pour {station_name}.")
# # # #                 apply_fixed_daylight = False

# # # #             except Exception as e:
# # # #                 print(f"Erreur lors du calcul du lever/coucher du soleil avec Astral pour {station_name}: {e}.")
# # # #                 warnings.warn(f"Calcul Astral impossible pour '{station_name}'. Utilisation de l'indicateur jour/nuit fixe.")
# # # #                 apply_fixed_daylight = True
# # # #         else:
# # # #             print(f"Avertissement: Coordonnées ou Fuseau horaire manquants/invalides pour le site '{station_name}' dans df_gps. Utilisation de l'indicateur jour/nuit fixe.")
# # # #             apply_fixed_daylight = True

# # # #         if apply_fixed_daylight:
# # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy['Datetime'].dt.hour >= 7) & (group_copy['Datetime'].dt.hour <= 18)
# # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # #             print(f"Utilisation de l'indicateur jour/nuit fixe (7h-18h) pour {station_name}.")

# # # #         df_processed_parts.append(group_copy)

# # # #     if not df_processed_parts:
# # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # #     # Concatenate all parts. 'Datetime' is a column, so ignore_index=True is appropriate here.
# # # #     df_final = pd.concat(df_processed_parts, ignore_index=True) 
# # # #     print(f"DEBUG (interpolation/concat): Index du DataFrame final après concaténation (avant set_index): {type(df_final.index)}")
# # # #     print(f"DEBUG (interpolation/concat): Colonnes du DataFrame final après concaténation: {df_final.columns.tolist()}")

# # # #     # Set 'Datetime' as the index at the very end of interpolation function
# # # #     if 'Datetime' in df_final.columns:
# # # #         df_final['Datetime'] = pd.to_datetime(df_final['Datetime'], errors='coerce')
# # # #         df_final.dropna(subset=['Datetime'], inplace=True)
# # # #         if df_final.empty:
# # # #             raise ValueError("Le DataFrame est vide après le nettoyage des dates dans la concaténation finale de l'interpolation.")
# # # #         df_final = df_final.set_index('Datetime').sort_index()
# # # #         df_final.index.name = 'Datetime' # Ensure index name is set
# # # #         print("DEBUG (interpolation/concat): Index 'Datetime' défini et trié avec succès sur df_final.")
# # # #     else:
# # # #         raise ValueError("La colonne 'Datetime' est manquante après concaténation. Impossible de définir DatetimeIndex.")

# # # #     # Nettoyage des colonnes temporaires
# # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date',
# # # #                                   'sunrise_time_local', 'sunset_time_local']
# # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # #     # Gestion intelligente de Rain_mm
# # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # #             df_final = create_rain_mm(df_final)
# # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # #         else:
# # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # #             if 'Rain_mm' not in df_final.columns:
# # # #                 df_final['Rain_mm'] = np.nan


# # # #     # Interpolation standard et bornage pour les variables numériques
# # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                      'Wind_Dir_Deg']

# # # #     for var in standard_vars:
# # # #         if var in df_final.columns:
# # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # #             if var in limits:
# # # #                 min_val = limits[var]['min']
# # # #                 max_val = limits[var]['max']
# # # #                 initial_nan_count = df_final[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_final.loc[df_final[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_final.loc[df_final[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_final[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # #             print(f"DEBUG (interpolation/variable): Interpolation de '{var}'. Type de l'index de df_final: {type(df_final.index)}")
            
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/variable): L'index n'est pas un DatetimeIndex pour l'interpolation de '{var}'. Utilisation de la méthode 'linear'.")
# # # #                 df_final[var] = df_final[var].interpolate(method='linear', limit_direction='both')
# # # #             df_final[var] = df_final[var].bfill().ffill()

# # # #     # Conditional interpolation for Solar Radiation
# # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # #         if 'Solar_R_W/m^2' in limits:
# # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # #         if 'Is_Daylight' in df_final.columns:
# # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # #             if 'Rain_mm' in df_final.columns:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # #             else:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # #             print(f"DEBUG (interpolation/solaire): Interpolation de 'Solar_R_W/m^2' (conditionnel). Type de l'index de df_final: {type(df_final.index)}")

# # # #             is_day = df_final['Is_Daylight']
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/solaire): L'index n'est pas un DatetimeIndex pour l'interpolation de 'Solar_R_W/m^2'. Utilisation de la méthode 'linear'.")
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both')

# # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # #         else:
# # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()
# # # #             else:
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both').bfill().ffill()

# # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # #     missing_after_interp = df_final.isna().sum()
# # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # #     if not columns_with_missing.empty:
# # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # #     else:
# # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # #     return df_final


# # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # #         raise ValueError(
# # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # #         )

# # # #     def convert_row(row):
# # # #         try:
# # # #             zone = int(row['zone'])
# # # #             hemisphere = str(row['hemisphere']).upper()
# # # #             is_northern = hemisphere == 'N'

# # # #             proj_utm = CRS.from_proj4(
# # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # #             )
# # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # #         except Exception as e:
# # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # #     return df_copy

# # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # #     """
# # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
# # # #     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

# # # #     Returns:
# # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # #     """
# # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # #     data_dir = 'data'
# # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # #     files_info = [
# # # #         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
# # # #         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
# # # #         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
# # # #     ]

# # # #     loaded_dfs = []

# # # #     for file_info in files_info:
# # # #         output_file_path = os.path.join(data_dir, file_info['name'])
        
# # # #         if not os.path.exists(output_file_path):
# # # #             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
# # # #             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
# # # #             print(f"Téléchargement de {file_info['bassin']} terminé.")
# # # #         else:
# # # #             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
# # # #         loaded_dfs.append(pd.read_excel(output_file_path))

# # # #     vea_sissili_bassin = loaded_dfs[0]
# # # #     dano_bassin = loaded_dfs[1]
# # # #     dassari_bassin = loaded_dfs[2]

# # # #     # Prétraitement des DataFrames (votre code original)
# # # #     print("Début du prétraitement des données de stations...")
    
# # # #     # Vea Sissili
# # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # #     # Dassari
# # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # #     dano_bassin['zone'] = 30
# # # #     dano_bassin['hemisphere'] = 'N'
# # # #     dassari_bassin['zone'] = 31
# # # #     dassari_bassin['hemisphere'] = 'N'

# # # #     # Application de la fonction de conversion UTM vers GPS
# # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # #     # Ajout des fuseaux horaires
# # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # #     # Fusion de tous les bassins
# # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # #     # Renommer 'Name' en 'Station'
# # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # #     initial_rows_count = len(bassins)
# # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # #     if len(bassins) < initial_rows_count:
# # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # #     if 'Station' in bassins.columns:
# # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # #     return bassins

# # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
# # # #     Conserve la première occurrence en cas de doublon.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée.

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame sans doublons.
# # # #     """
# # # #     if 'Station' in df.columns and 'Datetime' in df.columns:
# # # #         initial_rows = len(df)
# # # #         df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
# # # #         if len(df_cleaned) < initial_rows:
# # # #             warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
# # # #         return df_cleaned
# # # #     else:
# # # #         warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
# # # #         return df

# # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # #     """
# # # #     Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
# # # #         try:
# # # #             df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
# # # #             df_processed = df_processed[df_processed.index.notna()]
# # # #             if df_processed.empty:
# # # #                 raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
# # # #         except Exception as e:
# # # #             raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

# # # #     for var, vals in limits.items():
# # # #         if var in df_processed.columns:
# # # #             min_val = vals.get('min')
# # # #             max_val = vals.get('max')
# # # #             if min_val is not None or max_val is not None:
# # # #                 initial_nan_count = df_processed[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_processed.loc[df_processed[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_processed[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs aberrantes dans '{var}' par NaN.")
# # # #     return df_processed

# # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return None

# # # #     if periode == 'Journalière':
# # # #         resampled_df = filtered_df[variable].resample('D').mean()
# # # #     elif periode == 'Hebdomadaire':
# # # #         resampled_df = filtered_df[variable].resample('W').mean()
# # # #     elif periode == 'Mensuelle':
# # # #         resampled_df = filtered_df[variable].resample('M').mean()
# # # #     elif periode == 'Annuelle':
# # # #         resampled_df = filtered_df[variable].resample('Y').mean()
# # # #     else:
# # # #         resampled_df = filtered_df[variable]

# # # #     resampled_df = resampled_df.dropna()

# # # #     if resampled_df.empty:
# # # #         return None

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     color = colors.get(station, '#1f77b4')

# # # #     import plotly.graph_objects as go
# # # #     import plotly.io as pio

# # # #     fig = go.Figure()
# # # #     fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                              mode='lines', name=f'{variable_meta["Nom"]} - {station}',
# # # #                              line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)

# # # # def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

# # # #     import plotly.graph_objects as go
# # # #     import plotly.io as pio

# # # #     fig = go.Figure()
    
# # # #     all_stations = df['Station'].unique()
# # # #     if len(all_stations) < 2:
# # # #         warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
# # # #         return None

# # # #     for station in all_stations:
# # # #         filtered_df = df[df['Station'] == station].copy()
# # # #         if filtered_df.empty:
# # # #             continue

# # # #         if periode == 'Journalière':
# # # #             resampled_df = filtered_df[variable].resample('D').mean()
# # # #         elif periode == 'Hebdomadaire':
# # # #             resampled_df = filtered_df[variable].resample('W').mean()
# # # #         elif periode == 'Mensuelle':
# # # #             resampled_df = filtered_df[variable].resample('M').mean()
# # # #         elif periode == 'Annuelle':
# # # #             resampled_df = filtered_df[variable].resample('Y').mean()
# # # #         else:
# # # #             resampled_df = filtered_df[variable]

# # # #         resampled_df = resampled_df.dropna()
# # # #         if resampled_df.empty:
# # # #             continue
        
# # # #         color = colors.get(station, '#1f77b4')
# # # #         fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                                  mode='lines', name=station,
# # # #                                  line=dict(color=color)))

# # # #     if not fig.data:
# # # #         return None

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     fig.update_layout(
# # # #         title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)


# # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return None

# # # #     numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight']]

# # # #     if not numerical_vars:
# # # #         warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
# # # #         return None

# # # #     normalized_df = filtered_df[numerical_vars].copy()
# # # #     for col in normalized_df.columns:
# # # #         min_val = normalized_df[col].min()
# # # #         max_val = normalized_df[col].max()
# # # #         if max_val != min_val:
# # # #             normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
# # # #         else:
# # # #             normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

# # # #     normalized_df = normalized_df.dropna(how='all')

# # # #     if normalized_df.empty:
# # # #         return None
    
# # # #     import plotly.graph_objects as go
# # # #     import plotly.io as pio

# # # #     fig = go.Figure()
# # # #     for var in normalized_df.columns:
# # # #         var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
# # # #         color = colors.get(var, None)

# # # #         fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
# # # #                                  mode='lines', name=var_meta['Nom'],
# # # #                                  line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
# # # #         xaxis_title="Date",
# # # #         yaxis_title="Valeur Normalisée (0-1)",
# # # #         hovermode="x unified",
# # # #         legend_title="Variables"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)

# # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # # #     groupées par station.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # # #     """
# # # #     df = df.copy()

# # # #     # Assurez-vous que 'Datetime' est une colonne et de type datetime
# # # #     if isinstance(df.index, pd.DatetimeIndex):
# # # #         df = df.reset_index()

# # # #     df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')
# # # #     df = df.dropna(subset=['Datetime', 'Station'])

# # # #     if df.empty:
# # # #         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans daily_stats.")
# # # #         return pd.DataFrame()

# # # #     if 'Is_Daylight' not in df.columns:
# # # #         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
# # # #         df['Is_Daylight'] = (df['Datetime'].dt.hour >= 7) & (df['Datetime'].dt.hour <= 18)

# # # #     numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Datetime', 'Is_Daylight']]
    
# # # #     if not numerical_cols:
# # # #         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
# # # #         return pd.DataFrame()

# # # #     agg_funcs = {col: ['mean', 'min', 'max'] for col in numerical_cols}
    
# # # #     if 'Rain_mm' in numerical_cols:
# # # #         agg_funcs['Rain_mm'].append('sum')

# # # #     daily_stats_df = df.groupby(['Station', df['Datetime'].dt.date]).agg(agg_funcs)

# # # #     daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

# # # #     daily_stats_df = daily_stats_df.rename_axis(index={'Datetime': 'Date'})
# # # #     daily_stats_df = daily_stats_df.reset_index()

# # # #     return daily_stats_df

# # # # ####################


# # # # import pandas as pd
# # # # from pyproj import CRS, Transformer
# # # # import pytz
# # # # from astral.location import LocationInfo
# # # # from astral import sun
# # # # import numpy as np
# # # # import warnings
# # # # import os
# # # # import gdown # Importation du module gdown pour le téléchargement

# # # # # Fonction utilitaire pour créer Rain_mm si nécessaire
# # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
# # # #     Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
# # # #     """
# # # #     df_copy = df.copy()
# # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
# # # #     elif 'Rain_01_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm']
# # # #     elif 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_02_mm']
# # # #     else:
# # # #         df_copy['Rain_mm'] = np.nan # Crée la colonne même si aucune source n'est disponible
# # # #         warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents pour créer 'Rain_mm'. 'Rain_mm' est rempli de NaN.")
# # # #     return df_copy


# # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     # Prioriser la colonne 'Date' si elle existe pour VEA_SISSILI ou si pas de colonnes Y/M/D/H/M
# # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not any(col in df_copy.columns for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'])):
# # # #         try:
# # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime pour le bassin {bassin}: {e}")
# # # #             df_copy['Datetime'] = pd.NaT
# # # #     else:
# # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # #         for col in date_cols:
# # # #             if col in df_copy.columns:
# # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # #         try:
# # # #             existing_date_components = [col for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'] if col in df_copy.columns]
            
# # # #             if not existing_date_components:
# # # #                 raise ValueError("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute) trouvée.")

# # # #             date_strings = df_copy.apply(
# # # #                 lambda row: f"{int(row.get('Year', 2000))}-"
# # # #                             f"{int(row.get('Month', 1)):02d}-"
# # # #                             f"{int(row.get('Day', 1)):02d} "
# # # #                             f"{int(row.get('Hour', 0)):02d}:"
# # # #                             f"{int(row.get('Minute', 0)):02d}",
# # # #                 axis=1
# # # #             )
# # # #             df_copy['Datetime'] = pd.to_datetime(date_strings, errors='coerce')
            
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
# # # #             df_copy['Datetime'] = pd.NaT
            
# # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # #         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
# # # #              df_copy['Date'] = df_copy['Datetime'].dt.date
# # # #     else:
# # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

# # # #     return df_copy

# # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Effectue toutes les interpolations météorologiques en une seule passe.
# # # #     Cette fonction DOIT recevoir un DataFrame avec une colonne 'Datetime' (et non un DatetimeIndex).
# # # #     Il doit également contenir une colonne 'Station'.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée avec la colonne 'Datetime'.
# # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame original avec les données interpolées,
# # # #                       la colonne 'Is_Daylight' calculée, la durée du jour, et un DatetimeIndex.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     # Assurez-vous que 'Datetime' est une colonne de type datetime et nettoyez les NaT
# # # #     if 'Datetime' not in df_processed.columns:
# # # #         raise ValueError("La colonne 'Datetime' est manquante dans le DataFrame d'entrée pour l'interpolation.")
    
# # # #     df_processed['Datetime'] = pd.to_datetime(df_processed['Datetime'], errors='coerce')
# # # #     df_processed.dropna(subset=['Datetime'], inplace=True)
# # # #     if df_processed.empty:
# # # #         raise ValueError("Après nettoyage des dates invalides, le DataFrame est vide. Impossible de procéder à l'interpolation.")
    
# # # #     print(f"DEBUG (interpolation): Type de la colonne 'Datetime' du DataFrame initial: {df_processed['Datetime'].dtype}")
# # # #     print(f"DEBUG (interpolation): Premières 5 valeurs de la colonne 'Datetime' après nettoyage des NaT: {df_processed['Datetime'].head(5).tolist() if not df_processed.empty else 'DataFrame vide'}")

# # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # #         raise ValueError(
# # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # #         )

# # # #     if not df_gps['Station'].is_unique:
# # # #         print("Avertissement: La colonne 'Station' dans df_gps contient des noms de station dupliqués.")
# # # #         print("Ceci peut entraîner des comportements inattendus ou des stations non reconnues.")
# # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # #         print(f"Suppression de {len(df_gps) - len(df_gps_unique)} doublons dans df_gps (en gardant la première occurrence).")
# # # #     else:
# # # #         df_gps_unique = df_gps.copy()

# # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # #     for col in numerical_cols:
# # # #         if col in df_processed.columns:
# # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # #     df_processed_parts = []

# # # #     for station_name, group in df_processed.groupby('Station'):
# # # #         group_copy = group.copy()
# # # #         print(f"DEBUG (interpolation/groupby): Début du traitement du groupe '{station_name}'.")
        
# # # #         # Force localization of 'Datetime' column to UTC for consistency within the group
# # # #         if group_copy['Datetime'].dt.tz is None:
# # # #             group_copy['Datetime'] = group_copy['Datetime'].dt.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #         else:
# # # #             group_copy['Datetime'] = group_copy['Datetime'].dt.tz_convert('UTC')
# # # #         print(f"DEBUG (interpolation/groupby): Colonne Datetime pour '{station_name}' localisée à UTC. Dtype: {group_copy['Datetime'].dtype}")

# # # #         # --- Astral calculations using local timezone ---
# # # #         apply_fixed_daylight = True
# # # #         gps_data = gps_info_dict.get(station_name)

# # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # #             lat = gps_data['Lat']
# # # #             long = gps_data['Long']
# # # #             timezone_str = gps_data['Timezone']

# # # #             try:
# # # #                 # Convert Datetime to local timezone for astral calculations ONLY
# # # #                 local_tz = pytz.timezone(timezone_str)
# # # #                 datetime_local = group_copy['Datetime'].dt.tz_convert(local_tz)

# # # #                 daily_sun_info = {}
# # # #                 for date_only in datetime_local.dt.normalize().unique():
# # # #                     loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
# # # #                     s = sun.sun(loc.observer, date=date_only.date(), tzinfo=loc.tzinfo)
# # # #                     daily_sun_info[date_only.date()] = {
# # # #                         'sunrise': s['sunrise'],
# # # #                         'sunset': s['sunset']
# # # #                     }

# # # #                 group_copy.loc[:, 'sunrise_time_local'] = datetime_local.dt.date.map(lambda x: daily_sun_info.get(x, {}).get('sunrise'))
# # # #                 group_copy.loc[:, 'sunset_time_local'] = datetime_local.dt.date.map(lambda x: daily_sun_info.get(x, {}).get('sunset'))

# # # #                 # Convert sunrise/sunset times back to UTC for consistent comparison with group_copy['Datetime']
# # # #                 if not group_copy['sunrise_time_local'].isnull().all():
# # # #                      group_copy.loc[:, 'sunrise_time_local'] = group_copy['sunrise_time_local'].dt.tz_convert('UTC')
# # # #                 if not group_copy['sunset_time_local'].isnull().all():
# # # #                      group_copy.loc[:, 'sunset_time_local'] = group_copy['sunset_time_local'].dt.tz_convert('UTC')


# # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy['Datetime'] >= group_copy['sunrise_time_local']) & \
# # # #                                                   (group_copy['Datetime'] < group_copy['sunset_time_local'])

# # # #                 daylight_timedelta = group_copy['sunset_time_local'] - group_copy['sunrise_time_local']
# # # #                 group_copy.loc[:, 'Daylight_Duration'] = daylight_timedelta.astype(str).replace('NaT', np.nan) # Store as string
# # # #                 print(f"Lever et coucher du soleil calculés pour {station_name}.")
# # # #                 apply_fixed_daylight = False

# # # #             except Exception as e:
# # # #                 print(f"Erreur lors du calcul du lever/coucher du soleil avec Astral pour {station_name}: {e}.")
# # # #                 warnings.warn(f"Calcul Astral impossible pour '{station_name}'. Utilisation de l'indicateur jour/nuit fixe.")
# # # #                 apply_fixed_daylight = True
# # # #         else:
# # # #             print(f"Avertissement: Coordonnées ou Fuseau horaire manquants/invalides pour le site '{station_name}' dans df_gps. Utilisation de l'indicateur jour/nuit fixe.")
# # # #             apply_fixed_daylight = True

# # # #         if apply_fixed_daylight:
# # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy['Datetime'].dt.hour >= 7) & (group_copy['Datetime'].dt.hour <= 18)
# # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # #             print(f"Utilisation de l'indicateur jour/nuit fixe (7h-18h) pour {station_name}.")

# # # #         df_processed_parts.append(group_copy)

# # # #     if not df_processed_parts:
# # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # #     # Concatenate all parts. 'Datetime' is a column, so ignore_index=True is appropriate here.
# # # #     df_final = pd.concat(df_processed_parts, ignore_index=True) 
# # # #     print(f"DEBUG (interpolation/concat): Index du DataFrame final après concaténation (avant set_index): {type(df_final.index)}")
# # # #     print(f"DEBUG (interpolation/concat): Colonnes du DataFrame final après concaténation: {df_final.columns.tolist()}")

# # # #     # Set 'Datetime' as the index at the very end of interpolation function
# # # #     if 'Datetime' in df_final.columns:
# # # #         df_final['Datetime'] = pd.to_datetime(df_final['Datetime'], errors='coerce')
# # # #         df_final.dropna(subset=['Datetime'], inplace=True)
# # # #         if df_final.empty:
# # # #             raise ValueError("Le DataFrame est vide après le nettoyage des dates dans la concaténation finale de l'interpolation.")
# # # #         df_final = df_final.set_index('Datetime').sort_index()
# # # #         df_final.index.name = 'Datetime' # Ensure index name is set
# # # #         print("DEBUG (interpolation/concat): Index 'Datetime' défini et trié avec succès sur df_final.")
# # # #     else:
# # # #         raise ValueError("La colonne 'Datetime' est manquante après concaténation. Impossible de définir DatetimeIndex.")

# # # #     # Nettoyage des colonnes temporaires
# # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date',
# # # #                                   'sunrise_time_local', 'sunset_time_local']
# # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # #     # Gestion intelligente de Rain_mm
# # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # #             df_final = create_rain_mm(df_final)
# # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # #         else:
# # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # #             if 'Rain_mm' not in df_final.columns:
# # # #                 df_final['Rain_mm'] = np.nan


# # # #     # Interpolation standard et bornage pour les variables numériques
# # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                      'Wind_Dir_Deg']

# # # #     for var in standard_vars:
# # # #         if var in df_final.columns:
# # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # #             if var in limits:
# # # #                 min_val = limits[var]['min']
# # # #                 max_val = limits[var]['max']
# # # #                 initial_nan_count = df_final[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_final.loc[df_final[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_final.loc[df_final[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_final[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # #             print(f"DEBUG (interpolation/variable): Interpolation de '{var}'. Type de l'index de df_final: {type(df_final.index)}")
            
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/variable): L'index n'est pas un DatetimeIndex pour l'interpolation de '{var}'. Utilisation de la méthode 'linear'.")
# # # #                 df_final[var] = df_final[var].interpolate(method='linear', limit_direction='both')
# # # #             df_final[var] = df_final[var].bfill().ffill()

# # # #     # Conditional interpolation for Solar Radiation
# # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # #         if 'Solar_R_W/m^2' in limits:
# # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # #         if 'Is_Daylight' in df_final.columns:
# # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # #             if 'Rain_mm' in df_final.columns:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # #             else:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # #             print(f"DEBUG (interpolation/solaire): Interpolation de 'Solar_R_W/m^2' (conditionnel). Type de l'index de df_final: {type(df_final.index)}")

# # # #             is_day = df_final['Is_Daylight']
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/solaire): L'index n'est pas un DatetimeIndex pour l'interpolation de 'Solar_R_W/m^2'. Utilisation de la méthode 'linear'.")
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both')

# # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # #         else:
# # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()
# # # #             else:
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both').bfill().ffill()

# # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # #     missing_after_interp = df_final.isna().sum()
# # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # #     if not columns_with_missing.empty:
# # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # #     else:
# # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # #     return df_final


# # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # #         raise ValueError(
# # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # #         )

# # # #     def convert_row(row):
# # # #         try:
# # # #             zone = int(row['zone'])
# # # #             hemisphere = str(row['hemisphere']).upper()
# # # #             is_northern = hemisphere == 'N'

# # # #             proj_utm = CRS.from_proj4(
# # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # #             )
# # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # #         except Exception as e:
# # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # #     return df_copy

# # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # #     """
# # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
# # # #     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

# # # #     Returns:
# # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # #     """
# # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # #     data_dir = 'data'
# # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # #     files_info = [
# # # #         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
# # # #         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
# # # #         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
# # # #     ]

# # # #     loaded_dfs = []

# # # #     for file_info in files_info:
# # # #         output_file_path = os.path.join(data_dir, file_info['name'])
        
# # # #         if not os.path.exists(output_file_path):
# # # #             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
# # # #             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
# # # #             print(f"Téléchargement de {file_info['bassin']} terminé.")
# # # #         else:
# # # #             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
# # # #         loaded_dfs.append(pd.read_excel(output_file_path))

# # # #     vea_sissili_bassin = loaded_dfs[0]
# # # #     dano_bassin = loaded_dfs[1]
# # # #     dassari_bassin = loaded_dfs[2]

# # # #     # Prétraitement des DataFrames (votre code original)
# # # #     print("Début du prétraitement des données de stations...")
    
# # # #     # Vea Sissili
# # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # #     # Dassari
# # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # #     dano_bassin['zone'] = 30
# # # #     dano_bassin['hemisphere'] = 'N'
# # # #     dassari_bassin['zone'] = 31
# # # #     dassari_bassin['hemisphere'] = 'N'

# # # #     # Application de la fonction de conversion UTM vers GPS
# # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # #     # Ajout des fuseaux horaires
# # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # #     # Fusion de tous les bassins
# # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # #     # Renommer 'Name' en 'Station'
# # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # #     initial_rows_count = len(bassins)
# # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # #     if len(bassins) < initial_rows_count:
# # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # #     if 'Station' in bassins.columns:
# # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # #     return bassins

# # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
# # # #     Conserve la première occurrence en cas de doublon.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée.

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame sans doublons.
# # # #     """
# # # #     if 'Station' in df.columns and 'Datetime' in df.columns:
# # # #         initial_rows = len(df)
# # # #         df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
# # # #         if len(df_cleaned) < initial_rows:
# # # #             warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
# # # #         return df_cleaned
# # # #     else:
# # # #         warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
# # # #         return df

# # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # #     """
# # # #     Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
# # # #         try:
# # # #             df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
# # # #             df_processed = df_processed[df_processed.index.notna()]
# # # #             if df_processed.empty:
# # # #                 raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
# # # #         except Exception as e:
# # # #             raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

# # # #     for var, vals in limits.items():
# # # #         if var in df_processed.columns:
# # # #             min_val = vals.get('min')
# # # #             max_val = vals.get('max')
# # # #             if min_val is not None or max_val is not None:
# # # #                 initial_nan_count = df_processed[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_processed.loc[df_processed[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_processed[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs aberrantes dans '{var}' par NaN.")
# # # #     return df_processed

# # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return None

# # # #     if periode == 'Journalière':
# # # #         resampled_df = filtered_df[variable].resample('D').mean()
# # # #     elif periode == 'Hebdomadaire':
# # # #         resampled_df = filtered_df[variable].resample('W').mean()
# # # #     elif periode == 'Mensuelle':
# # # #         resampled_df = filtered_df[variable].resample('M').mean()
# # # #     elif periode == 'Annuelle':
# # # #         resampled_df = filtered_df[variable].resample('Y').mean()
# # # #     else:
# # # #         resampled_df = filtered_df[variable]

# # # #     resampled_df = resampled_df.dropna()

# # # #     if resampled_df.empty:
# # # #         return None

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     color = colors.get(station, '#1f77b4')

# # # #     import plotly.graph_objects as go
# # # #     import plotly.io as pio

# # # #     fig = go.Figure()
# # # #     fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                              mode='lines', name=f'{variable_meta["Nom"]} - {station}',
# # # #                              line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)

# # # # def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

# # # #     import plotly.graph_objects as go
# # # #     import plotly.io as pio

# # # #     fig = go.Figure()
    
# # # #     all_stations = df['Station'].unique()
# # # #     if len(all_stations) < 2:
# # # #         warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
# # # #         return None

# # # #     for station in all_stations:
# # # #         filtered_df = df[df['Station'] == station].copy()
# # # #         if filtered_df.empty:
# # # #             continue

# # # #         if periode == 'Journalière':
# # # #             resampled_df = filtered_df[variable].resample('D').mean()
# # # #         elif periode == 'Hebdomadaire':
# # # #             resampled_df = filtered_df[variable].resample('W').mean()
# # # #         elif periode == 'Mensuelle':
# # # #             resampled_df = filtered_df[variable].resample('M').mean()
# # # #         elif periode == 'Annuelle':
# # # #             resampled_df = filtered_df[variable].resample('Y').mean()
# # # #         else:
# # # #             resampled_df = filtered_df[variable]

# # # #         resampled_df = resampled_df.dropna()
# # # #         if resampled_df.empty:
# # # #             continue
        
# # # #         color = colors.get(station, '#1f77b4')
# # # #         fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                                  mode='lines', name=station,
# # # #                                  line=dict(color=color)))

# # # #     if not fig.data:
# # # #         return None

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     fig.update_layout(
# # # #         title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)


# # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return None

# # # #     numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight']]

# # # #     if not numerical_vars:
# # # #         warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
# # # #         return None

# # # #     normalized_df = filtered_df[numerical_vars].copy()
# # # #     for col in normalized_df.columns:
# # # #         min_val = normalized_df[col].min()
# # # #         max_val = normalized_df[col].max()
# # # #         if max_val != min_val:
# # # #             normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
# # # #         else:
# # # #             normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

# # # #     normalized_df = normalized_df.dropna(how='all')

# # # #     if normalized_df.empty:
# # # #         return None
    
# # # #     import plotly.graph_objects as go
# # # #     import plotly.io as pio

# # # #     fig = go.Figure()
# # # #     for var in normalized_df.columns:
# # # #         var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
# # # #         color = colors.get(var, None)

# # # #         fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
# # # #                                  mode='lines', name=var_meta['Nom'],
# # # #                                  line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
# # # #         xaxis_title="Date",
# # # #         yaxis_title="Valeur Normalisée (0-1)",
# # # #         hovermode="x unified",
# # # #         legend_title="Variables"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)

# # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # # #     groupées par station.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # # #     """
# # # #     df = df.copy()

# # # #     # Assurez-vous que 'Datetime' est une colonne et de type datetime
# # # #     if isinstance(df.index, pd.DatetimeIndex):
# # # #         df = df.reset_index()

# # # #     df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')
# # # #     df = df.dropna(subset=['Datetime', 'Station'])

# # # #     if df.empty:
# # # #         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans daily_stats.")
# # # #         return pd.DataFrame()

# # # #     if 'Is_Daylight' not in df.columns:
# # # #         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
# # # #         df['Is_Daylight'] = (df['Datetime'].dt.hour >= 7) & (df['Datetime'].dt.hour <= 18)

# # # #     numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Datetime', 'Is_Daylight']]
    
# # # #     if not numerical_cols:
# # # #         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
# # # #         return pd.DataFrame()

# # # #     agg_funcs = {col: ['mean', 'min', 'max'] for col in numerical_cols}
    
# # # #     if 'Rain_mm' in numerical_cols:
# # # #         agg_funcs['Rain_mm'].append('sum')

# # # #     daily_stats_df = df.groupby(['Station', df['Datetime'].dt.date]).agg(agg_funcs)

# # # #     daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

# # # #     daily_stats_df = daily_stats_df.rename_axis(index={'Datetime': 'Date'})
# # # #     daily_stats_df = daily_stats_df.reset_index()

# # # #     return daily_stats_df

# # # # #######################



# # # #&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&
# # # # import pandas as pd
# # # # from pyproj import CRS, Transformer
# # # # import pytz
# # # # from astral.location import LocationInfo
# # # # from astral import sun
# # # # import numpy as np
# # # # import warnings
# # # # import os
# # # # import gdown # Importation du module gdown pour le téléchargement

# # # # # Fonction utilitaire pour créer Rain_mm si nécessaire
# # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
# # # #     Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
# # # #     """
# # # #     df_copy = df.copy()
# # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
# # # #     elif 'Rain_01_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm']
# # # #     elif 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_02_mm']
# # # #     else:
# # # #         df_copy['Rain_mm'] = np.nan # Crée la colonne même si aucune source n'est disponible
# # # #         warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents pour créer 'Rain_mm'. 'Rain_mm' est rempli de NaN.")
# # # #     return df_copy


# # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     # Prioriser la colonne 'Date' si elle existe pour VEA_SISSILI ou si pas de colonnes Y/M/D/H/M
# # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not any(col in df_copy.columns for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'])):
# # # #         try:
# # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime pour le bassin {bassin}: {e}")
# # # #             df_copy['Datetime'] = pd.NaT
# # # #     else:
# # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # #         for col in date_cols:
# # # #             if col in df_copy.columns:
# # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # #         try:
# # # #             existing_date_components = [col for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'] if col in df_copy.columns]
            
# # # #             if not existing_date_components:
# # # #                 raise ValueError("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute) trouvée.")

# # # #             date_strings = df_copy.apply(
# # # #                 lambda row: f"{int(row.get('Year', 2000))}-"
# # # #                             f"{int(row.get('Month', 1)):02d}-"
# # # #                             f"{int(row.get('Day', 1)):02d} "
# # # #                             f"{int(row.get('Hour', 0)):02d}:"
# # # #                             f"{int(row.get('Minute', 0)):02d}",
# # # #                 axis=1
# # # #             )
# # # #             df_copy['Datetime'] = pd.to_datetime(date_strings, errors='coerce')
            
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
# # # #             df_copy['Datetime'] = pd.NaT
            
# # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # #         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
# # # #              df_copy['Date'] = df_copy['Datetime'].dt.date
# # # #     else:
# # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

# # # #     return df_copy

# # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Effectue toutes les interpolations météorologiques en une seule passe.
# # # #     Cette fonction DOIT recevoir un DataFrame avec un DatetimeIndex.
# # # #     Il doit également contenir une colonne 'Station'.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame original avec les données interpolées,
# # # #                       la colonne 'Is_Daylight' calculée, la durée du jour, et un DatetimeIndex.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     # Assurez-vous que l'index est bien DatetimeIndex et nettoyez les NaT
# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame d'entrée pour l'interpolation DOIT avoir un DatetimeIndex.")
    
# # # #     initial_rows = len(df_processed)
# # # #     df_processed = df_processed[df_processed.index.notna()]
# # # #     if len(df_processed) == 0:
# # # #         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
# # # #     if initial_rows - len(df_processed) > 0:
# # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")
    
# # # #     print(f"DEBUG (interpolation): Type de l'index du DataFrame initial: {type(df_processed.index)}")
# # # #     # Correction de la ligne de débogage pour éviter l'erreur .head() sur DatetimeIndex
# # # #     print(f"DEBUG (interpolation): Premières 5 valeurs de l'index après nettoyage des NaT: {df_processed.index[:5].tolist() if not df_processed.empty else 'DataFrame vide'}")

# # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # #         raise ValueError(
# # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # #         )

# # # #     if not df_gps['Station'].is_unique:
# # # #         print("Avertissement: La colonne 'Station' dans df_gps contient des noms de station dupliqués.")
# # # #         print("Ceci peut entraîner des comportements inattendus ou des stations non reconnues.")
# # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # #         print(f"Suppression de {len(df_gps) - len(df_gps_unique)} doublons dans df_gps (en gardant la première occurrence).")
# # # #     else:
# # # #         df_gps_unique = df_gps.copy()

# # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # #     for col in numerical_cols:
# # # #         if col in df_processed.columns:
# # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # #     df_processed_parts = []

# # # #     for station_name, group in df_processed.groupby('Station'):
# # # #         group_copy = group.copy()
# # # #         print(f"DEBUG (interpolation/groupby): Début du traitement du groupe '{station_name}'.")
        
# # # #         # Localisation de l'index temporel du groupe
# # # #         gps_data = gps_info_dict.get(station_name)
# # # #         if gps_data and pd.notna(gps_data.get('Timezone')):
# # # #             try:
# # # #                 tz = pytz.timezone(gps_data['Timezone'])
# # # #                 # Convertir l'index en UTC si non localisé ou s'il est dans un autre TZ
# # # #                 if group_copy.index.tz is None:
# # # #                     group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #                 else: # Convert if already localized to a different TZ than target for consistency
# # # #                     group_copy.index = group_copy.index.tz_convert('UTC')
# # # #                 print(f"DEBUG (interpolation/groupby): Index Datetime pour '{station_name}' localisé à UTC. Dtype: {group_copy.index.dtype}")
# # # #             except Exception as e:
# # # #                 warnings.warn(f"Impossible de localiser l'index pour '{station_name}' avec le fuseau horaire '{gps_data['Timezone']}': {e}. Utilisation de UTC par défaut.")
# # # #                 if group_copy.index.tz is None:
# # # #                     group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #                 else:
# # # #                     group_copy.index = group_copy.index.tz_convert('UTC')
# # # #         else:
# # # #             warnings.warn(f"Fuseau horaire manquant pour '{station_name}'. Localisation de l'index à UTC par défaut.")
# # # #             if group_copy.index.tz is None:
# # # #                 group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #             else:
# # # #                 group_copy.index = group_copy.index.tz_convert('UTC')
        
# # # #         # S'assurer que l'index n'a pas de NaT après localisation
# # # #         group_copy = group_copy[group_copy.index.notna()]
# # # #         if group_copy.empty:
# # # #             warnings.warn(f"Le groupe '{station_name}' est vide après nettoyage de l'index Datetime. Il sera ignoré.")
# # # #             continue


# # # #         # --- Calculs Astral en utilisant le fuseau horaire local ---
# # # #         apply_fixed_daylight = True
# # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # #             lat = gps_data['Lat']
# # # #             long = gps_data['Long']
# # # #             timezone_str = gps_data['Timezone']

# # # #             try:
# # # #                 local_tz = pytz.timezone(timezone_str)
# # # #                 # Utiliser l'index converti localement pour les calculs Astral
# # # #                 index_local = group_copy.index.tz_convert(local_tz)

# # # #                 daily_sun_info = {}
# # # #                 for date_only in index_local.normalize().unique():
# # # #                     loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
# # # #                     s = sun.sun(loc.observer, date=date_only.date(), tzinfo=loc.tzinfo)
# # # #                     daily_sun_info[date_only.date()] = {
# # # #                         'sunrise': s['sunrise'],
# # # #                         'sunset': s['sunset']
# # # #                     }

# # # #                 # Mapper les heures de lever/coucher du soleil (en fuseau horaire local, puis convertir en UTC)
# # # #                 sunrise_times = index_local.date.map(lambda x: daily_sun_info.get(x, {}).get('sunrise'))
# # # #                 sunset_times = index_local.date.map(lambda x: daily_sun_info.get(x, {}).get('sunset'))

# # # #                 # Convertir les series sunrise_times et sunset_times en Timestamps si ce n'est pas déjà le cas
# # # #                 # avant de les localiser à UTC pour comparaison avec group_copy.index
# # # #                 sunrise_times = pd.to_datetime(sunrise_times, errors='coerce').dt.tz_localize(local_tz, ambiguous='NaT', nonexistent='NaT')
# # # #                 sunset_times = pd.to_datetime(sunset_times, errors='coerce').dt.tz_localize(local_tz, ambiguous='NaT', nonexistent='NaT')
                
# # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index >= sunrise_times.dt.tz_convert('UTC')) & \
# # # #                                                   (group_copy.index < sunset_times.dt.tz_convert('UTC'))

# # # #                 daylight_timedelta = sunset_times - sunrise_times
# # # #                 group_copy.loc[:, 'Daylight_Duration'] = daylight_timedelta.astype(str).replace('NaT', np.nan)
# # # #                 print(f"Lever et coucher du soleil calculés pour {station_name}.")
# # # #                 apply_fixed_daylight = False

# # # #             except Exception as e:
# # # #                 print(f"Erreur lors du calcul du lever/coucher du soleil avec Astral pour {station_name}: {e}.")
# # # #                 warnings.warn(f"Calcul Astral impossible pour '{station_name}'. Utilisation de l'indicateur jour/nuit fixe.")
# # # #                 apply_fixed_daylight = True
# # # #         else:
# # # #             print(f"Avertissement: Coordonnées ou Fuseau horaire manquants/invalides pour le site '{station_name}' dans df_gps. Utilisation de l'indicateur jour/nuit fixe.")
# # # #             apply_fixed_daylight = True

# # # #         if apply_fixed_daylight:
# # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # #             print(f"Utilisation de l'indicateur jour/nuit fixe (7h-18h) pour {station_name}.")

# # # #         df_processed_parts.append(group_copy)

# # # #     if not df_processed_parts:
# # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # #     # Concaténation de toutes les parties. Puisque chaque partie a un DatetimeIndex,
# # # #     # nous pouvons concaténer directement sans ignore_index=True
# # # #     df_final = pd.concat(df_processed_parts)
# # # #     # Assurez-vous que l'index est trié et qu'il est bien nommé 'Datetime'
# # # #     df_final = df_final.sort_index()
# # # #     df_final.index.name = 'Datetime' 
# # # #     print(f"DEBUG (interpolation/concat): Index du DataFrame final après concaténation et tri: {type(df_final.index)}")
# # # #     print(f"DEBUG (interpolation/concat): Colonnes du DataFrame final après concaténation: {df_final.columns.tolist()}")


# # # #     # Nettoyage des colonnes temporaires (elles ne sont plus nécessaires ou n'existent plus)
# # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date']
# # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # #     # Gestion intelligente de Rain_mm
# # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # #             df_final = create_rain_mm(df_final)
# # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # #         else:
# # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # #             if 'Rain_mm' not in df_final.columns:
# # # #                 df_final['Rain_mm'] = np.nan


# # # #     # Interpolation standard et bornage pour les variables numériques
# # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                      'Wind_Dir_Deg']

# # # #     for var in standard_vars:
# # # #         if var in df_final.columns:
# # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # #             if var in limits:
# # # #                 min_val = limits[var]['min']
# # # #                 max_val = limits[var]['max']
# # # #                 initial_nan_count = df_final[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_final.loc[df_final[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_final.loc[df_final[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_final[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # #             print(f"DEBUG (interpolation/variable): Interpolation de '{var}'. Type de l'index de df_final: {type(df_final.index)}")
            
# # # #             # Interpolation seulement si l'index est bien DatetimeIndex
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/variable): L'index n'est pas un DatetimeIndex pour l'interpolation de '{var}'. Utilisation de la méthode 'linear'.")
# # # #                 df_final[var] = df_final[var].interpolate(method='linear', limit_direction='both')
# # # #             df_final[var] = df_final[var].bfill().ffill()

# # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # #         if 'Solar_R_W/m^2' in limits:
# # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # #         if 'Is_Daylight' in df_final.columns:
# # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # #             if 'Rain_mm' in df_final.columns:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # #             else:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # #             print(f"DEBUG (interpolation/solaire): Interpolation de 'Solar_R_W/m^2' (conditionnel). Type de l'index de df_final: {type(df_final.index)}")

# # # #             is_day = df_final['Is_Daylight']
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/solaire): L'index n'est pas un DatetimeIndex pour l'interpolation de 'Solar_R_W/m^2'. Utilisation de la méthode 'linear'.")
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both')

# # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # #         else:
# # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()
# # # #             else:
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both').bfill().ffill()

# # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # #     missing_after_interp = df_final.isna().sum()
# # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # #     if not columns_with_missing.empty:
# # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # #     else:
# # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # #     return df_final


# # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # #         raise ValueError(
# # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # #         )

# # # #     def convert_row(row):
# # # #         try:
# # # #             zone = int(row['zone'])
# # # #             hemisphere = str(row['hemisphere']).upper()
# # # #             is_northern = hemisphere == 'N'

# # # #             proj_utm = CRS.from_proj4(
# # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # #             )
# # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # #         except Exception as e:
# # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # #     return df_copy

# # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # #     """
# # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
# # # #     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

# # # #     Returns:
# # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # #     """
# # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # #     data_dir = 'data'
# # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # #     files_info = [
# # # #         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
# # # #         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
# # # #         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
# # # #     ]

# # # #     loaded_dfs = []

# # # #     for file_info in files_info:
# # # #         output_file_path = os.path.join(data_dir, file_info['name'])
        
# # # #         if not os.path.exists(output_file_path):
# # # #             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
# # # #             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
# # # #             print(f"Téléchargement de {file_info['bassin']} terminé.")
# # # #         else:
# # # #             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
# # # #         loaded_dfs.append(pd.read_excel(output_file_path))

# # # #     vea_sissili_bassin = loaded_dfs[0]
# # # #     dano_bassin = loaded_dfs[1]
# # # #     dassari_bassin = loaded_dfs[2]

# # # #     # Prétraitement des DataFrames (votre code original)
# # # #     print("Début du prétraitement des données de stations...")
    
# # # #     # Vea Sissili
# # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # #     # Dassari
# # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # #     dano_bassin['zone'] = 30
# # # #     dano_bassin['hemisphere'] = 'N'
# # # #     dassari_bassin['zone'] = 31
# # # #     dassari_bassin['hemisphere'] = 'N'

# # # #     # Application de la fonction de conversion UTM vers GPS
# # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # #     # Ajout des fuseaux horaires
# # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # #     # Fusion de tous les bassins
# # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # #     # Renommer 'Name' en 'Station'
# # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # #     initial_rows_count = len(bassins)
# # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # #     if len(bassins) < initial_rows_count:
# # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # #     if 'Station' in bassins.columns:
# # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # #     return bassins

# # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
# # # #     Conserve la première occurrence en cas de doublon.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée.

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame sans doublons.
# # # #     """
# # # #     if 'Station' in df.columns and 'Datetime' in df.columns:
# # # #         initial_rows = len(df)
# # # #         df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
# # # #         if len(df_cleaned) < initial_rows:
# # # #             warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
# # # #         return df_cleaned
# # # #     else:
# # # #         warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
# # # #         return df

# # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # #     """
# # # #     Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
# # # #         try:
# # # #             df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
# # # #             df_processed = df_processed[df_processed.index.notna()]
# # # #             if df_processed.empty:
# # # #                 raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
# # # #         except Exception as e:
# # # #             raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

# # # #     for var, vals in limits.items():
# # # #         if var in df_processed.columns:
# # # #             min_val = vals.get('min')
# # # #             max_val = vals.get('max')
# # # #             if min_val is not None or max_val is not None:
# # # #                 initial_nan_count = df_processed[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_processed.loc[df_processed[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_processed[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs aberrantes dans '{var}' par NaN.")
# # # #     return df_processed

# # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return None

# # # #     if periode == 'Journalière':
# # # #         resampled_df = filtered_df[variable].resample('D').mean()
# # # #     elif periode == 'Hebdomadaire':
# # # #         resampled_df = filtered_df[variable].resample('W').mean()
# # # #     elif periode == 'Mensuelle':
# # # #         resampled_df = filtered_df[variable].resample('M').mean()
# # # #     elif periode == 'Annuelle':
# # # #         resampled_df = filtered_df[variable].resample('Y').mean()
# # # #     else:
# # # #         resampled_df = filtered_df[variable]

# # # #     resampled_df = resampled_df.dropna()

# # # #     if resampled_df.empty:
# # # #         return None

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     color = colors.get(station, '#1f77b4')

# # # #     import plotly.graph_objects as go
# # # #     import plotly.io as pio

# # # #     fig = go.Figure()
# # # #     fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                              mode='lines', name=f'{variable_meta["Nom"]} - {station}',
# # # #                              line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)

# # # # def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

# # # #     import plotly.graph_objects as go
# # # #     import plotly.io as pio

# # # #     fig = go.Figure()
    
# # # #     all_stations = df['Station'].unique()
# # # #     if len(all_stations) < 2:
# # # #         warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
# # # #         return None

# # # #     for station in all_stations:
# # # #         filtered_df = df[df['Station'] == station].copy()
# # # #         if filtered_df.empty:
# # # #             continue

# # # #         if periode == 'Journalière':
# # # #             resampled_df = filtered_df[variable].resample('D').mean()
# # # #         elif periode == 'Hebdomadaire':
# # # #             resampled_df = filtered_df[variable].resample('W').mean()
# # # #         elif periode == 'Mensuelle':
# # # #             resampled_df = filtered_df[variable].resample('M').mean()
# # # #         elif periode == 'Annuelle':
# # # #             resampled_df = filtered_df[variable].resample('Y').mean()
# # # #         else:
# # # #             resampled_df = filtered_df[variable]

# # # #         resampled_df = resampled_df.dropna()
# # # #         if resampled_df.empty:
# # # #             continue
        
# # # #         color = colors.get(station, '#1f77b4')
# # # #         fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                                  mode='lines', name=station,
# # # #                                  line=dict(color=color)))

# # # #     if not fig.data:
# # # #         return None

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     fig.update_layout(
# # # #         title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)


# # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return None

# # # #     numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight']]

# # # #     if not numerical_vars:
# # # #         warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
# # # #         return None

# # # #     normalized_df = filtered_df[numerical_vars].copy()
# # # #     for col in normalized_df.columns:
# # # #         min_val = normalized_df[col].min()
# # # #         max_val = normalized_df[col].max()
# # # #         if max_val != min_val:
# # # #             normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
# # # #         else:
# # # #             normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

# # # #     normalized_df = normalized_df.dropna(how='all')

# # # #     if normalized_df.empty:
# # # #         return None
    
# # # #     import plotly.graph_objects as go
# # # #     import plotly.io as pio

# # # #     fig = go.Figure()
# # # #     for var in normalized_df.columns:
# # # #         var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
# # # #         color = colors.get(var, None)

# # # #         fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
# # # #                                  mode='lines', name=var_meta['Nom'],
# # # #                                  line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
# # # #         xaxis_title="Date",
# # # #         yaxis_title="Valeur Normalisée (0-1)",
# # # #         hovermode="x unified",
# # # #         legend_title="Variables"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)

# # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # # #     groupées par station.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # # #     """
# # # #     df = df.copy()

# # # #     # Assurez-vous que 'Datetime' est une colonne et de type datetime
# # # #     if isinstance(df.index, pd.DatetimeIndex):
# # # #         df = df.reset_index()

# # # #     df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')
# # # #     df = df.dropna(subset=['Datetime', 'Station'])

# # # #     if df.empty:
# # # #         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans daily_stats.")
# # # #         return pd.DataFrame()

# # # #     if 'Is_Daylight' not in df.columns:
# # # #         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
# # # #         df['Is_Daylight'] = (df['Datetime'].dt.hour >= 7) & (df['Datetime'].dt.hour <= 18)

# # # #     numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Datetime', 'Is_Daylight']]
    
# # # #     if not numerical_cols:
# # # #         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
# # # #         return pd.DataFrame()

# # # #     agg_funcs = {col: ['mean', 'min', 'max'] for col in numerical_cols}
    
# # # #     if 'Rain_mm' in numerical_cols:
# # # #         agg_funcs['Rain_mm'].append('sum')

# # # #     daily_stats_df = df.groupby(['Station', df['Datetime'].dt.date]).agg(agg_funcs)

# # # #     daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

# # # #     daily_stats_df = daily_stats_df.rename_axis(index={'Datetime': 'Date'})
# # # #     daily_stats_df = daily_stats_df.reset_index()

# # # #     return daily_stats_df

# # # #&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

# # # # # ################
# # # # import pandas as pd
# # # # from pyproj import CRS, Transformer
# # # # import pytz
# # # # from astral.location import LocationInfo
# # # # from astral import sun
# # # # import numpy as np
# # # # import warnings
# # # # import os
# # # # import gdown # Importation du module gdown pour le téléchargement
# # # # import plotly.graph_objects as go # Garder l'importation de go pour les figures

# # # # # Fonction utilitaire pour créer Rain_mm si nécessaire
# # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
# # # #     Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
# # # #     """
# # # #     df_copy = df.copy()
# # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
# # # #     elif 'Rain_01_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm']
# # # #     elif 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_02_mm']
# # # #     else:
# # # #         df_copy['Rain_mm'] = np.nan # Crée la colonne même si aucune source n'est disponible
# # # #         warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents pour créer 'Rain_mm'. 'Rain_mm' est rempli de NaN.")
# # # #     return df_copy


# # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     # Prioriser la colonne 'Date' si elle existe pour VEA_SISSILI ou si pas de colonnes Y/M/D/H/M
# # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not any(col in df_copy.columns for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'])):
# # # #         try:
# # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime pour le bassin {bassin}: {e}")
# # # #             df_copy['Datetime'] = pd.NaT
# # # #     else:
# # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # #         for col in date_cols:
# # # #             if col in df_copy.columns:
# # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # #         try:
# # # #             existing_date_components = [col for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'] if col in df_copy.columns]
            
# # # #             if not existing_date_components:
# # # #                 raise ValueError("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute) trouvée.")

# # # #             date_strings = df_copy.apply(
# # # #                 lambda row: f"{int(row.get('Year', 2000))}-"
# # # #                             f"{int(row.get('Month', 1)):02d}-"
# # # #                             f"{int(row.get('Day', 1)):02d} "
# # # #                             f"{int(row.get('Hour', 0)):02d}:"
# # # #                             f"{int(row.get('Minute', 0)):02d}",
# # # #                 axis=1
# # # #             )
# # # #             df_copy['Datetime'] = pd.to_datetime(date_strings, errors='coerce')
            
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
# # # #             df_copy['Datetime'] = pd.NaT
            
# # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # #         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
# # # #              df_copy['Date'] = df_copy['Datetime'].dt.date
# # # #     else:
# # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

# # # #     return df_copy

# # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Effectue toutes les interpolations météorologiques en une seule passe.
# # # #     Cette fonction DOIT recevoir un DataFrame avec un DatetimeIndex.
# # # #     Il doit également contenir une colonne 'Station'.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame original avec les données interpolées,
# # # #                       la colonne 'Is_Daylight' calculée, la durée du jour, et un DatetimeIndex.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     # Assurez-vous que l'index est bien DatetimeIndex et nettoyez les NaT
# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame d'entrée pour l'interpolation DOIT avoir un DatetimeIndex.")
    
# # # #     initial_rows = len(df_processed)
# # # #     df_processed = df_processed[df_processed.index.notna()]
# # # #     if len(df_processed) == 0:
# # # #         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
# # # #     if initial_rows - len(df_processed) > 0:
# # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")
    
# # # #     print(f"DEBUG (interpolation): Type de l'index du DataFrame initial: {type(df_processed.index)}")
# # # #     print(f"DEBUG (interpolation): Premières 5 valeurs de l'index après nettoyage des NaT: {df_processed.index[:5].tolist() if not df_processed.empty else 'DataFrame vide'}")

# # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # #         raise ValueError(
# # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # #         )

# # # #     if not df_gps['Station'].is_unique:
# # # #         print("Avertissement: La colonne 'Station' dans df_gps contient des noms de station dupliqués.")
# # # #         print("Ceci peut entraîner des comportements inattendus ou des stations non reconnues.")
# # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # #         print(f"Suppression de {len(df_gps) - len(df_gps_unique)} doublons dans df_gps (en gardant la première occurrence).")
# # # #     else:
# # # #         df_gps_unique = df_gps.copy()

# # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # #     for col in numerical_cols:
# # # #         if col in df_processed.columns:
# # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # #     df_processed_parts = []

# # # #     for station_name, group in df_processed.groupby('Station'):
# # # #         group_copy = group.copy()
# # # #         print(f"DEBUG (interpolation/groupby): Début du traitement du groupe '{station_name}'.")
        
# # # #         # Localisation de l'index temporel du groupe
# # # #         gps_data = gps_info_dict.get(station_name)
# # # #         if gps_data and pd.notna(gps_data.get('Timezone')):
# # # #             try:
# # # #                 tz = pytz.timezone(gps_data['Timezone'])
# # # #                 # Convertir l'index en UTC si non localisé ou s'il est dans un autre TZ
# # # #                 if group_copy.index.tz is None:
# # # #                     group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #                 else: # Convert if already localized to a different TZ than target for consistency
# # # #                     group_copy.index = group_copy.index.tz_convert('UTC')
# # # #                 print(f"DEBUG (interpolation/groupby): Index Datetime pour '{station_name}' localisé à UTC. Dtype: {group_copy.index.dtype}")
# # # #             except Exception as e:
# # # #                 warnings.warn(f"Impossible de localiser l'index pour '{station_name}' avec le fuseau horaire '{gps_data['Timezone']}': {e}. Utilisation de UTC par défaut.")
# # # #                 if group_copy.index.tz is None:
# # # #                     group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #                 else:
# # # #                     group_copy.index = group_copy.index.tz_convert('UTC')
# # # #         else:
# # # #             warnings.warn(f"Fuseau horaire manquant pour '{station_name}'. Localisation de l'index à UTC par défaut.")
# # # #             if group_copy.index.tz is None:
# # # #                 group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #             else:
# # # #                 group_copy.index = group_copy.index.tz_convert('UTC')
        
# # # #         # S'assurer que l'index n'a pas de NaT après localisation
# # # #         group_copy = group_copy[group_copy.index.notna()]
# # # #         if group_copy.empty:
# # # #             warnings.warn(f"Le groupe '{station_name}' est vide après nettoyage de l'index Datetime. Il sera ignoré.")
# # # #             continue


# # # #         # --- Calculs Astral en utilisant le fuseau horaire local ---
# # # #         apply_fixed_daylight = True
# # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # #             lat = gps_data['Lat']
# # # #             long = gps_data['Long']
# # # #             timezone_str = gps_data['Timezone']

# # # #             try:
# # # #                 local_tz = pytz.timezone(timezone_str)
# # # #                 # Utiliser l'index converti localement pour les calculs Astral
# # # #                 index_local = group_copy.index.tz_convert(local_tz)

# # # #                 daily_sun_info = {}
# # # #                 for date_only in index_local.normalize().unique():
# # # #                     loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
# # # #                     s = sun.sun(loc.observer, date=date_only.date(), tzinfo=loc.tzinfo)
# # # #                     daily_sun_info[date_only.date()] = {
# # # #                         'sunrise': s['sunrise'],
# # # #                         'sunset': s['sunset']
# # # #                     }

# # # #                 # Mapper les heures de lever/coucher du soleil (en fuseau horaire local, puis convertir en UTC)
# # # #                 sunrise_times = index_local.date.map(lambda x: daily_sun_info.get(x, {}).get('sunrise'))
# # # #                 sunset_times = index_local.date.map(lambda x: daily_sun_info.get(x, {}).get('sunset'))

# # # #                 # Convertir les series sunrise_times et sunset_times en Timestamps si ce n'est pas déjà le cas
# # # #                 # avant de les localiser à UTC pour comparaison avec group_copy.index
# # # #                 sunrise_times = pd.to_datetime(sunrise_times, errors='coerce').dt.tz_localize(local_tz, ambiguous='NaT', nonexistent='NaT')
# # # #                 sunset_times = pd.to_datetime(sunset_times, errors='coerce').dt.tz_localize(local_tz, ambiguous='NaT', nonexistent='NaT')
                
# # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index >= sunrise_times.dt.tz_convert('UTC')) & \
# # # #                                                   (group_copy.index < sunset_times.dt.tz_convert('UTC'))

# # # #                 daylight_timedelta = sunset_times - sunrise_times
# # # #                 group_copy.loc[:, 'Daylight_Duration'] = daylight_timedelta.astype(str).replace('NaT', np.nan)
# # # #                 print(f"Lever et coucher du soleil calculés pour {station_name}.")
# # # #                 apply_fixed_daylight = False

# # # #             except Exception as e:
# # # #                 print(f"Erreur lors du calcul du lever/coucher du soleil avec Astral pour {station_name}: {e}.")
# # # #                 warnings.warn(f"Calcul Astral impossible pour '{station_name}'. Utilisation de l'indicateur jour/nuit fixe.")
# # # #                 apply_fixed_daylight = True
# # # #         else:
# # # #             print(f"Avertissement: Coordonnées ou Fuseau horaire manquants/invalides pour le site '{station_name}' dans df_gps. Utilisation de l'indicateur jour/nuit fixe.")
# # # #             apply_fixed_daylight = True

# # # #         if apply_fixed_daylight:
# # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # #             print(f"Utilisation de l'indicateur jour/nuit fixe (7h-18h) pour {station_name}.")

# # # #         df_processed_parts.append(group_copy)

# # # #     if not df_processed_parts:
# # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # #     # Concaténation de toutes les parties. Puisque chaque partie a un DatetimeIndex,
# # # #     # nous pouvons concaténer directement sans ignore_index=True
# # # #     df_final = pd.concat(df_processed_parts)
# # # #     # Assurez-vous que l'index est trié et qu'il est bien nommé 'Datetime'
# # # #     df_final = df_final.sort_index()
# # # #     df_final.index.name = 'Datetime' 
# # # #     print(f"DEBUG (interpolation/concat): Index du DataFrame final après concaténation et tri: {type(df_final.index)}")
# # # #     print(f"DEBUG (interpolation/concat): Colonnes du DataFrame final après concaténation: {df_final.columns.tolist()}")


# # # #     # Nettoyage des colonnes temporaires (elles ne sont plus nécessaires ou n'existent plus)
# # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date']
# # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # #     # Gestion intelligente de Rain_mm
# # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # #             df_final = create_rain_mm(df_final)
# # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # #         else:
# # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # #             if 'Rain_mm' not in df_final.columns:
# # # #                 df_final['Rain_mm'] = np.nan


# # # #     # Interpolation standard et bornage pour les variables numériques
# # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                      'Wind_Dir_Deg']

# # # #     for var in standard_vars:
# # # #         if var in df_final.columns:
# # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # #             if var in limits:
# # # #                 min_val = limits[var]['min']
# # # #                 max_val = limits[var]['max']
# # # #                 initial_nan_count = df_final[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_final.loc[df_final[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_final.loc[df_final[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_final[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # #             print(f"DEBUG (interpolation/variable): Interpolation de '{var}'. Type de l'index de df_final: {type(df_final.index)}")
            
# # # #             # Interpolation seulement si l'index est bien DatetimeIndex
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/variable): L'index n'est pas un DatetimeIndex pour l'interpolation de '{var}'. Utilisation de la méthode 'linear'.")
# # # #                 df_final[var] = df_final[var].interpolate(method='linear', limit_direction='both')
# # # #             df_final[var] = df_final[var].bfill().ffill()

# # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # #         if 'Solar_R_W/m^2' in limits:
# # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # #         if 'Is_Daylight' in df_final.columns:
# # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # #             if 'Rain_mm' in df_final.columns:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # #             else:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # #             print(f"DEBUG (interpolation/solaire): Interpolation de 'Solar_R_W/m^2' (conditionnel). Type de l'index de df_final: {type(df_final.index)}")

# # # #             is_day = df_final['Is_Daylight']
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/solaire): L'index n'est pas un DatetimeIndex pour l'interpolation de 'Solar_R_W/m^2'. Utilisation de la méthode 'linear'.")
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both')

# # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # #         else:
# # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()
# # # #             else:
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both').bfill().ffill()

# # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # #     missing_after_interp = df_final.isna().sum()
# # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # #     if not columns_with_missing.empty:
# # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # #     else:
# # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # #     return df_final


# # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # #         raise ValueError(
# # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # #         )

# # # #     def convert_row(row):
# # # #         try:
# # # #             zone = int(row['zone'])
# # # #             hemisphere = str(row['hemisphere']).upper()
# # # #             is_northern = hemisphere == 'N'

# # # #             proj_utm = CRS.from_proj4(
# # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # #             )
# # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # #         except Exception as e:
# # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # #     return df_copy

# # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # #     """
# # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
# # # #     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

# # # #     Returns:
# # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # #     """
# # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # #     data_dir = 'data'
# # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # #     files_info = [
# # # #         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
# # # #         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
# # # #         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
# # # #     ]

# # # #     loaded_dfs = []

# # # #     for file_info in files_info:
# # # #         output_file_path = os.path.join(data_dir, file_info['name'])
        
# # # #         if not os.path.exists(output_file_path):
# # # #             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
# # # #             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
# # # #             print(f"Téléchargement de {file_info['bassin']} terminé.")
# # # #         else:
# # # #             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
# # # #         loaded_dfs.append(pd.read_excel(output_file_path))

# # # #     vea_sissili_bassin = loaded_dfs[0]
# # # #     dano_bassin = loaded_dfs[1]
# # # #     dassari_bassin = loaded_dfs[2]

# # # #     # Prétraitement des DataFrames (votre code original)
# # # #     print("Début du prétraitement des données de stations...")
    
# # # #     # Vea Sissili
# # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # #     # Dassari
# # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # #     dano_bassin['zone'] = 30
# # # #     dano_bassin['hemisphere'] = 'N'
# # # #     dassari_bassin['zone'] = 31
# # # #     dassari_bassin['hemisphere'] = 'N'

# # # #     # Application de la fonction de conversion UTM vers GPS
# # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # #     # Ajout des fuseaux horaires
# # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # #     # Fusion de tous les bassins
# # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # #     # Renommer 'Name' en 'Station'
# # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # #     initial_rows_count = len(bassins)
# # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # #     if len(bassins) < initial_rows_count:
# # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # #     if 'Station' in bassins.columns:
# # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # #     return bassins

# # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
# # # #     Conserve la première occurrence en cas de doublon.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée.

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame sans doublons.
# # # #     """
# # # #     if 'Station' in df.columns and 'Datetime' in df.columns:
# # # #         initial_rows = len(df)
# # # #         df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
# # # #         if len(df_cleaned) < initial_rows:
# # # #             warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
# # # #         return df_cleaned
# # # #     else:
# # # #         warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
# # # #         return df

# # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # #     """
# # # #     Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
# # # #         try:
# # # #             df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
# # # #             df_processed = df_processed[df_processed.index.notna()]
# # # #             if df_processed.empty:
# # # #                 raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
# # # #         except Exception as e:
# # # #             raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

# # # #     for var, vals in limits.items():
# # # #         if var in df_processed.columns:
# # # #             min_val = vals.get('min')
# # # #             max_val = vals.get('max')
# # # #             if min_val is not None or max_val is not None:
# # # #                 initial_nan_count = df_processed[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_processed.loc[df_processed[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_processed[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs aberrantes dans '{var}' par NaN.")
# # # #     return df_processed

# # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
# # # #     Retourne l'objet Figure Plotly.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return go.Figure() # Retourne une figure vide si pas de données

# # # #     if periode == 'Journalière':
# # # #         resampled_df = filtered_df[variable].resample('D').mean()
# # # #     elif periode == 'Hebdomadaire':
# # # #         resampled_df = filtered_df[variable].resample('W').mean()
# # # #     elif periode == 'Mensuelle':
# # # #         resampled_df = filtered_df[variable].resample('M').mean()
# # # #     elif periode == 'Annuelle':
# # # #         resampled_df = filtered_df[variable].resample('Y').mean()
# # # #     else:
# # # #         resampled_df = filtered_df[variable]

# # # #     resampled_df = resampled_df.dropna()

# # # #     if resampled_df.empty:
# # # #         return go.Figure() # Retourne une figure vide si pas de données après resample/dropna

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     color = colors.get(station, '#1f77b4')

# # # #     fig = go.Figure()
# # # #     fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                              mode='lines', name=f'{variable_meta["Nom"]} - {station}',
# # # #                              line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return fig

# # # # def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
# # # #     """
# # # #     Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
# # # #     Retourne l'objet Figure Plotly.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

# # # #     fig = go.Figure()
    
# # # #     all_stations = df['Station'].unique()
# # # #     if len(all_stations) < 2:
# # # #         warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
# # # #         return go.Figure() # Retourne une figure vide si moins de 2 stations

# # # #     for station in all_stations:
# # # #         filtered_df = df[df['Station'] == station].copy()
# # # #         if filtered_df.empty:
# # # #             continue

# # # #         if periode == 'Journalière':
# # # #             resampled_df = filtered_df[variable].resample('D').mean()
# # # #         elif periode == 'Hebdomadaire':
# # # #             resampled_df = filtered_df[variable].resample('W').mean()
# # # #         elif periode == 'Mensuelle':
# # # #             resampled_df = filtered_df[variable].resample('M').mean()
# # # #         elif periode == 'Annuelle':
# # # #             resampled_df = filtered_df[variable].resample('Y').mean()
# # # #         else:
# # # #             resampled_df = filtered_df[variable]

# # # #         resampled_df = resampled_df.dropna()
# # # #         if resampled_df.empty:
# # # #             continue
        
# # # #         color = colors.get(station, '#1f77b4')
# # # #         fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                                  mode='lines', name=station,
# # # #                                  line=dict(color=color)))

# # # #     if not fig.data:
# # # #         return go.Figure() # Retourne une figure vide si aucune trace n'a été ajoutée

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     fig.update_layout(
# # # #         title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return fig


# # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> go.Figure:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
# # # #     Retourne l'objet Figure Plotly.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return go.Figure() # Retourne une figure vide si pas de données

# # # #     numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight', 'Daylight_Duration']]

# # # #     if not numerical_vars:
# # # #         warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
# # # #         return go.Figure() # Retourne une figure vide si pas de variables numériques

# # # #     normalized_df = filtered_df[numerical_vars].copy()
# # # #     for col in normalized_df.columns:
# # # #         min_val = normalized_df[col].min()
# # # #         max_val = normalized_df[col].max()
# # # #         if max_val != min_val:
# # # #             normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
# # # #         else:
# # # #             normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

# # # #     normalized_df = normalized_df.dropna(how='all')

# # # #     if normalized_df.empty:
# # # #         return go.Figure() # Retourne une figure vide si pas de données après normalisation/dropna
    
# # # #     fig = go.Figure()
# # # #     for var in normalized_df.columns:
# # # #         var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
# # # #         color = colors.get(var, None)

# # # #         fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
# # # #                                  mode='lines', name=var_meta['Nom'],
# # # #                                  line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
# # # #         xaxis_title="Date",
# # # #         yaxis_title="Valeur Normalisée (0-1)",
# # # #         hovermode="x unified",
# # # #         legend_title="Variables"
# # # #     )
# # # #     return fig

# # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # # #     groupées par station.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # # #     """
# # # #     df = df.copy()

# # # #     # Assurez-vous que 'Datetime' est une colonne et de type datetime
# # # #     if isinstance(df.index, pd.DatetimeIndex):
# # # #         df = df.reset_index()

# # # #     df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')
# # # #     df = df.dropna(subset=['Datetime', 'Station'])

# # # #     if df.empty:
# # # #         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans daily_stats.")
# # # #         return pd.DataFrame()

# # # #     if 'Is_Daylight' not in df.columns:
# # # #         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
# # # #         df['Is_Daylight'] = (df['Datetime'].dt.hour >= 7) & (df['Datetime'].dt.hour <= 18)

# # # #     numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Datetime', 'Is_Daylight']]
    
# # # #     if not numerical_cols:
# # # #         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
# # # #         return pd.DataFrame()

# # # #     agg_funcs = {col: ['mean', 'min', 'max'] for col in numerical_cols}
    
# # # #     if 'Rain_mm' in numerical_cols:
# # # #         agg_funcs['Rain_mm'].append('sum')

# # # #     daily_stats_df = df.groupby(['Station', df['Datetime'].dt.date]).agg(agg_funcs)

# # # #     daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

# # # #     daily_stats_df = daily_stats_df.rename_axis(index={'Datetime': 'Date'})
# # # #     daily_stats_df = daily_stats_df.reset_index()

# # # #     return daily_stats_df

# # # ############
# # # # import pandas as pd
# # # # from pyproj import CRS, Transformer
# # # # import pytz
# # # # from astral.location import LocationInfo
# # # # from astral import sun
# # # # import numpy as np
# # # # import warnings
# # # # import os
# # # # import gdown # Importation du module gdown pour le téléchargement

# # # # # Fonction utilitaire pour créer Rain_mm si nécessaire
# # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
# # # #     Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
# # # #     """
# # # #     df_copy = df.copy()
# # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
# # # #     elif 'Rain_01_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm']
# # # #     elif 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_02_mm']
# # # #     else:
# # # #         df_copy['Rain_mm'] = np.nan # Crée la colonne même si aucune source n'est disponible
# # # #         warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents pour créer 'Rain_mm'. 'Rain_mm' est rempli de NaN.")
# # # #     return df_copy


# # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     # Prioriser la colonne 'Date' si elle existe pour VEA_SISSILI ou si pas de colonnes Y/M/D/H/M
# # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not any(col in df_copy.columns for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'])):
# # # #         try:
# # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime pour le bassin {bassin}: {e}")
# # # #             df_copy['Datetime'] = pd.NaT
# # # #     else:
# # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # #         for col in date_cols:
# # # #             if col in df_copy.columns:
# # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # #         try:
# # # #             existing_date_components = [col for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'] if col in df_copy.columns]
            
# # # #             if not existing_date_components:
# # # #                 raise ValueError("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute) trouvée.")

# # # #             date_strings = df_copy.apply(
# # # #                 lambda row: f"{int(row.get('Year', 2000))}-"
# # # #                             f"{int(row.get('Month', 1)):02d}-"
# # # #                             f"{int(row.get('Day', 1)):02d} "
# # # #                             f"{int(row.get('Hour', 0)):02d}:"
# # # #                             f"{int(row.get('Minute', 0)):02d}",
# # # #                 axis=1
# # # #             )
# # # #             df_copy['Datetime'] = pd.to_datetime(date_strings, errors='coerce')
            
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
# # # #             df_copy['Datetime'] = pd.NaT
            
# # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # #         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
# # # #              df_copy['Date'] = df_copy['Datetime'].dt.date
# # # #     else:
# # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

# # # #     return df_copy

# # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Effectue toutes les interpolations météorologiques en une seule passe.
# # # #     Cette fonction DOIT recevoir un DataFrame avec un DatetimeIndex.
# # # #     Il doit également contenir une colonne 'Station'.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame original avec les données interpolées,
# # # #                       la colonne 'Is_Daylight' calculée, la durée du jour, et un DatetimeIndex.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     # Assurez-vous que l'index est bien DatetimeIndex et nettoyez les NaT
# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame d'entrée pour l'interpolation DOIT avoir un DatetimeIndex.")
    
# # # #     initial_rows = len(df_processed)
# # # #     df_processed = df_processed[df_processed.index.notna()]
# # # #     if len(df_processed) == 0:
# # # #         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
# # # #     if initial_rows - len(df_processed) > 0:
# # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")
    
# # # #     print(f"DEBUG (interpolation): Type de l'index du DataFrame initial: {type(df_processed.index)}")
# # # #     # Correction de la ligne de débogage pour éviter l'erreur .head() sur DatetimeIndex
# # # #     print(f"DEBUG (interpolation): Premières 5 valeurs de l'index après nettoyage des NaT: {df_processed.index[:5].tolist() if not df_processed.empty else 'DataFrame vide'}")

# # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # #         raise ValueError(
# # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # #         )

# # # #     if not df_gps['Station'].is_unique:
# # # #         print("Avertissement: La colonne 'Station' dans df_gps contient des noms de station dupliqués.")
# # # #         print("Ceci peut entraîner des comportements inattendus ou des stations non reconnues.")
# # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # #         print(f"Suppression de {len(df_gps) - len(df_gps_unique)} doublons dans df_gps (en gardant la première occurrence).")
# # # #     else:
# # # #         df_gps_unique = df_gps.copy()

# # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # #     for col in numerical_cols:
# # # #         if col in df_processed.columns:
# # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # #     df_processed_parts = []

# # # #     for station_name, group in df_processed.groupby('Station'):
# # # #         group_copy = group.copy()
# # # #         print(f"DEBUG (interpolation/groupby): Début du traitement du groupe '{station_name}'.")
        
# # # #         # Localisation de l'index temporel du groupe
# # # #         gps_data = gps_info_dict.get(station_name)
# # # #         if gps_data and pd.notna(gps_data.get('Timezone')):
# # # #             try:
# # # #                 tz = pytz.timezone(gps_data['Timezone'])
# # # #                 # Convertir l'index en UTC si non localisé ou s'il est dans un autre TZ
# # # #                 if group_copy.index.tz is None:
# # # #                     group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #                 else: # Convert if already localized to a different TZ than target for consistency
# # # #                     group_copy.index = group_copy.index.tz_convert('UTC')
# # # #                 print(f"DEBUG (interpolation/groupby): Index Datetime pour '{station_name}' localisé à UTC. Dtype: {group_copy.index.dtype}")
# # # #             except Exception as e:
# # # #                 warnings.warn(f"Impossible de localiser l'index pour '{station_name}' avec le fuseau horaire '{gps_data['Timezone']}': {e}. Utilisation de UTC par défaut.")
# # # #                 if group_copy.index.tz is None:
# # # #                     group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #                 else:
# # # #                     group_copy.index = group_copy.index.tz_convert('UTC')
# # # #         else:
# # # #             warnings.warn(f"Fuseau horaire manquant pour '{station_name}'. Localisation de l'index à UTC par défaut.")
# # # #             if group_copy.index.tz is None:
# # # #                 group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #             else:
# # # #                 group_copy.index = group_copy.index.tz_convert('UTC')
        
# # # #         # S'assurer que l'index n'a pas de NaT après localisation
# # # #         group_copy = group_copy[group_copy.index.notna()]
# # # #         if group_copy.empty:
# # # #             warnings.warn(f"Le groupe '{station_name}' est vide après nettoyage de l'index Datetime. Il sera ignoré.")
# # # #             continue


# # # #         # --- Calculs Astral en utilisant le fuseau horaire local ---
# # # #         apply_fixed_daylight = True
# # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # #             lat = gps_data['Lat']
# # # #             long = gps_data['Long']
# # # #             timezone_str = gps_data['Timezone']

# # # #             try:
# # # #                 local_tz = pytz.timezone(timezone_str)
# # # #                 # Utiliser l'index converti localement pour les calculs Astral
# # # #                 index_local = group_copy.index.tz_convert(local_tz)

# # # #                 daily_sun_info = {}
# # # #                 for date_only in index_local.normalize().unique():
# # # #                     loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
# # # #                     s = sun.sun(loc.observer, date=date_only.date(), tzinfo=loc.tzinfo)
# # # #                     daily_sun_info[date_only.date()] = {
# # # #                         'sunrise': s['sunrise'],
# # # #                         'sunset': s['sunset']
# # # #                     }

# # # #                 # Mapper les heures de lever/coucher du soleil (en fuseau horaire local, puis convertir en UTC)
# # # #                 sunrise_times = index_local.date.map(lambda x: daily_sun_info.get(x, {}).get('sunrise'))
# # # #                 sunset_times = index_local.date.map(lambda x: daily_sun_info.get(x, {}).get('sunset'))

# # # #                 # Convertir les series sunrise_times et sunset_times en Timestamps si ce n'est pas déjà le cas
# # # #                 # avant de les localiser à UTC pour comparaison avec group_copy.index
# # # #                 sunrise_times = pd.to_datetime(sunrise_times, errors='coerce').dt.tz_localize(local_tz, ambiguous='NaT', nonexistent='NaT')
# # # #                 sunset_times = pd.to_datetime(sunset_times, errors='coerce').dt.tz_localize(local_tz, ambiguous='NaT', nonexistent='NaT')
                
# # # #                 group_copy.loc[:, 'Is_Daylight'] = (group_copy.index >= sunrise_times.dt.tz_convert('UTC')) & \
# # # #                                                   (group_copy.index < sunset_times.dt.tz_convert('UTC'))

# # # #                 daylight_timedelta = sunset_times - sunrise_times
# # # #                 group_copy.loc[:, 'Daylight_Duration'] = daylight_timedelta.astype(str).replace('NaT', np.nan)
# # # #                 print(f"Lever et coucher du soleil calculés pour {station_name}.")
# # # #                 apply_fixed_daylight = False

# # # #             except Exception as e:
# # # #                 print(f"Erreur lors du calcul du lever/coucher du soleil avec Astral pour {station_name}: {e}.")
# # # #                 warnings.warn(f"Calcul Astral impossible pour '{station_name}'. Utilisation de l'indicateur jour/nuit fixe.")
# # # #                 apply_fixed_daylight = True
# # # #         else:
# # # #             print(f"Avertissement: Coordonnées ou Fuseau horaire manquants/invalides pour le site '{station_name}' dans df_gps. Utilisation de l'indicateur jour/nuit fixe.")
# # # #             apply_fixed_daylight = True

# # # #         if apply_fixed_daylight:
# # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # #             print(f"Utilisation de l'indicateur jour/nuit fixe (7h-18h) pour {station_name}.")

# # # #         df_processed_parts.append(group_copy)

# # # #     if not df_processed_parts:
# # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # #     # Concaténation de toutes les parties. Puisque chaque partie a un DatetimeIndex,
# # # #     # nous pouvons concaténer directement sans ignore_index=True
# # # #     df_final = pd.concat(df_processed_parts)
# # # #     # Assurez-vous que l'index est trié et qu'il est bien nommé 'Datetime'
# # # #     df_final = df_final.sort_index()
# # # #     df_final.index.name = 'Datetime' 
# # # #     print(f"DEBUG (interpolation/concat): Index du DataFrame final après concaténation et tri: {type(df_final.index)}")
# # # #     print(f"DEBUG (interpolation/concat): Colonnes du DataFrame final après concaténation: {df_final.columns.tolist()}")


# # # #     # Nettoyage des colonnes temporaires (elles ne sont plus nécessaires ou n'existent plus)
# # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date']
# # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # #     # Gestion intelligente de Rain_mm
# # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # #             df_final = create_rain_mm(df_final)
# # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # #         else:
# # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # #             if 'Rain_mm' not in df_final.columns:
# # # #                 df_final['Rain_mm'] = np.nan


# # # #     # Interpolation standard et bornage pour les variables numériques
# # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                      'Wind_Dir_Deg']

# # # #     for var in standard_vars:
# # # #         if var in df_final.columns:
# # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # #             if var in limits:
# # # #                 min_val = limits[var]['min']
# # # #                 max_val = limits[var]['max']
# # # #                 initial_nan_count = df_final[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_final.loc[df_final[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_final.loc[df_final[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_final[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # #             print(f"DEBUG (interpolation/variable): Interpolation de '{var}'. Type de l'index de df_final: {type(df_final.index)}")
            
# # # #             # Interpolation seulement si l'index est bien DatetimeIndex
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/variable): L'index n'est pas un DatetimeIndex pour l'interpolation de '{var}'. Utilisation de la méthode 'linear'.")
# # # #                 df_final[var] = df_final[var].interpolate(method='linear', limit_direction='both')
# # # #             df_final[var] = df_final[var].bfill().ffill()

# # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # #         if 'Solar_R_W/m^2' in limits:
# # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # #         if 'Is_Daylight' in df_final.columns:
# # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # #             if 'Rain_mm' in df_final.columns:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # #             else:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # #             print(f"DEBUG (interpolation/solaire): Interpolation de 'Solar_R_W/m^2' (conditionnel). Type de l'index de df_final: {type(df_final.index)}")

# # # #             is_day = df_final['Is_Daylight']
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/solaire): L'index n'est pas un DatetimeIndex pour l'interpolation de 'Solar_R_W/m^2'. Utilisation de la méthode 'linear'.")
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both')

# # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # #         else:
# # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()
# # # #             else:
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both').bfill().ffill()

# # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # #     missing_after_interp = df_final.isna().sum()
# # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # #     if not columns_with_missing.empty:
# # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # #     else:
# # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # #     return df_final


# # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # #         raise ValueError(
# # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # #         )

# # # #     def convert_row(row):
# # # #         try:
# # # #             zone = int(row['zone'])
# # # #             hemisphere = str(row['hemisphere']).upper()
# # # #             is_northern = hemisphere == 'N'

# # # #             proj_utm = CRS.from_proj4(
# # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # #             )
# # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # #         except Exception as e:
# # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # #     return df_copy

# # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # #     """
# # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
# # # #     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

# # # #     Returns:
# # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # #     """
# # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # #     data_dir = 'data'
# # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # #     files_info = [
# # # #         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
# # # #         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
# # # #         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
# # # #     ]

# # # #     loaded_dfs = []

# # # #     for file_info in files_info:
# # # #         output_file_path = os.path.join(data_dir, file_info['name'])
        
# # # #         if not os.path.exists(output_file_path):
# # # #             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
# # # #             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
# # # #             print(f"Téléchargement de {file_info['bassin']} terminé.")
# # # #         else:
# # # #             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
# # # #         loaded_dfs.append(pd.read_excel(output_file_path))

# # # #     vea_sissili_bassin = loaded_dfs[0]
# # # #     dano_bassin = loaded_dfs[1]
# # # #     dassari_bassin = loaded_dfs[2]

# # # #     # Prétraitement des DataFrames (votre code original)
# # # #     print("Début du prétraitement des données de stations...")
    
# # # #     # Vea Sissili
# # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # #     # Dassari
# # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # #     dano_bassin['zone'] = 30
# # # #     dano_bassin['hemisphere'] = 'N'
# # # #     dassari_bassin['zone'] = 31
# # # #     dassari_bassin['hemisphere'] = 'N'

# # # #     # Application de la fonction de conversion UTM vers GPS
# # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # #     # Ajout des fuseaux horaires
# # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # #     # Fusion de tous les bassins
# # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # #     # Renommer 'Name' en 'Station'
# # # #     bassins = bassins.rename(columns={'Name': 'Station'})
    
# # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # #     initial_rows_count = len(bassins)
# # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # #     if len(bassins) < initial_rows_count:
# # # #         warnings.warn(f"{initial_rows_count - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # #     # Assurez-vous que la colonne Station est de type string pour un regroupement fiable
# # # #     if 'Station' in bassins.columns:
# # # #         bassins['Station'] = bassins['Station'].astype(str)

# # # #     print("Préparation des données de coordonnées des stations terminée.")
# # # #     return bassins

# # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
# # # #     Conserve la première occurrence en cas de doublon.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée.

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame sans doublons.
# # # #     """
# # # #     if 'Station' in df.columns and 'Datetime' in df.columns:
# # # #         initial_rows = len(df)
# # # #         df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
# # # #         if len(df_cleaned) < initial_rows:
# # # #             warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
# # # #         return df_cleaned
# # # #     else:
# # # #         warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
# # # #         return df

# # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # #     """
# # # #     Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
# # # #         try:
# # # #             df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
# # # #             df_processed = df_processed[df_processed.index.notna()]
# # # #             if df_processed.empty:
# # # #                 raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
# # # #         except Exception as e:
# # # #             raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

# # # #     for var, vals in limits.items():
# # # #         if var in df_processed.columns:
# # # #             min_val = vals.get('min')
# # # #             max_val = vals.get('max')
# # # #             if min_val is not None or max_val is not None:
# # # #                 initial_nan_count = df_processed[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_processed.loc[df_processed[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_processed[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs aberrantes dans '{var}' par NaN.")
# # # #     return df_processed

# # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return None

# # # #     if periode == 'Journalière':
# # # #         resampled_df = filtered_df[variable].resample('D').mean()
# # # #     elif periode == 'Hebdomadaire':
# # # #         resampled_df = filtered_df[variable].resample('W').mean()
# # # #     elif periode == 'Mensuelle':
# # # #         resampled_df = filtered_df[variable].resample('M').mean()
# # # #     elif periode == 'Annuelle':
# # # #         resampled_df = filtered_df[variable].resample('Y').mean()
# # # #     else:
# # # #         resampled_df = filtered_df[variable]

# # # #     resampled_df = resampled_df.dropna()

# # # #     if resampled_df.empty:
# # # #         return None

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     color = colors.get(station, '#1f77b4')

# # # #     import plotly.graph_objects as go
# # # #     import plotly.io as pio

# # # #     fig = go.Figure()
# # # #     fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                              mode='lines', name=f'{variable_meta["Nom"]} - {station}',
# # # #                              line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)

# # # # def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

# # # #     import plotly.graph_objects as go
# # # #     import plotly.io as pio

# # # #     fig = go.Figure()
    
# # # #     all_stations = df['Station'].unique()
# # # #     if len(all_stations) < 2:
# # # #         warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
# # # #         return None

# # # #     for station in all_stations:
# # # #         filtered_df = df[df['Station'] == station].copy()
# # # #         if filtered_df.empty:
# # # #             continue

# # # #         if periode == 'Journalière':
# # # #             resampled_df = filtered_df[variable].resample('D').mean()
# # # #         elif periode == 'Hebdomadaire':
# # # #             resampled_df = filtered_df[variable].resample('W').mean()
# # # #         elif periode == 'Mensuelle':
# # # #             resampled_df = filtered_df[variable].resample('M').mean()
# # # #         elif periode == 'Annuelle':
# # # #             resampled_df = filtered_df[variable].resample('Y').mean()
# # # #         else:
# # # #             resampled_df = filtered_df[variable]

# # # #         resampled_df = resampled_df.dropna()
# # # #         if resampled_df.empty:
# # # #             continue
        
# # # #         color = colors.get(station, '#1f77b4')
# # # #         fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                                  mode='lines', name=station,
# # # #                                  line=dict(color=color)))

# # # #     if not fig.data:
# # # #         return None

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     fig.update_layout(
# # # #         title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)


# # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> str:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return None

# # # #     numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight']]

# # # #     if not numerical_vars:
# # # #         warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
# # # #         return None

# # # #     normalized_df = filtered_df[numerical_vars].copy()
# # # #     for col in normalized_df.columns:
# # # #         min_val = normalized_df[col].min()
# # # #         max_val = normalized_df[col].max()
# # # #         if max_val != min_val:
# # # #             normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
# # # #         else:
# # # #             normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

# # # #     normalized_df = normalized_df.dropna(how='all')

# # # #     if normalized_df.empty:
# # # #         return None
    
# # # #     import plotly.graph_objects as go
# # # #     import plotly.io as pio

# # # #     fig = go.Figure()
# # # #     for var in normalized_df.columns:
# # # #         var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
# # # #         color = colors.get(var, None)

# # # #         fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
# # # #                                  mode='lines', name=var_meta['Nom'],
# # # #                                  line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
# # # #         xaxis_title="Date",
# # # #         yaxis_title="Valeur Normalisée (0-1)",
# # # #         hovermode="x unified",
# # # #         legend_title="Variables"
# # # #     )
# # # #     return pio.to_html(fig, full_html=False)

# # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # # #     groupées par station.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # # #     """
# # # #     df = df.copy()

# # # #     # Assurez-vous que 'Datetime' est une colonne et de type datetime
# # # #     if isinstance(df.index, pd.DatetimeIndex):
# # # #         df = df.reset_index()

# # # #     df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')
# # # #     df = df.dropna(subset=['Datetime', 'Station'])

# # # #     if df.empty:
# # # #         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans daily_stats.")
# # # #         return pd.DataFrame()

# # # #     if 'Is_Daylight' not in df.columns:
# # # #         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
# # # #         df['Is_Daylight'] = (df['Datetime'].dt.hour >= 7) & (df['Datetime'].dt.hour <= 18)

# # # #     numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Datetime', 'Is_Daylight']]
    
# # # #     if not numerical_cols:
# # # #         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
# # # #         return pd.DataFrame()

# # # #     agg_funcs = {col: ['mean', 'min', 'max'] for col in numerical_cols}
    
# # # #     if 'Rain_mm' in numerical_cols:
# # # #         agg_funcs['Rain_mm'].append('sum')

# # # #     daily_stats_df = df.groupby(['Station', df['Datetime'].dt.date]).agg(agg_funcs)

# # # #     daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

# # # #     daily_stats_df = daily_stats_df.rename_axis(index={'Datetime': 'Date'})
# # # #     daily_stats_df = daily_stats_df.reset_index()

# # # #     return daily_stats_df

# # # # import pandas as pd
# # # # from pyproj import CRS, Transformer
# # # # import pytz
# # # # from astral.location import LocationInfo
# # # # from astral import sun
# # # # import numpy as np
# # # # import warnings
# # # # import os
# # # # import gdown # Importation du module gdown pour le téléchargement
# # # # import plotly.graph_objects as go # Importation pour gérer les objets Figure Plotly
# # # # import matplotlib.pyplot as plt # NOUVEAUX IMPORTS pour Matplotlib
# # # # import seaborn as sns
# # # # import traceback # Importation de traceback pour les messages d'erreur détaillés
# # # # import math # Ajouté pour math.ceil
# # # # from datetime import timedelta # Ajouté pour timedelta

# # # # # Importation des configurations et métadonnées depuis config.py
# # # # from config import METADATA_VARIABLES, PALETTE_DEFAUT, DATA_LIMITS

# # # # # Fonction utilitaire pour créer Rain_mm si nécessaire
# # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
# # # #     Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
# # # #     """
# # # #     df_copy = df.copy()
# # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
# # # #     elif 'Rain_01_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm']
# # # #     elif 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_02_mm']
# # # #     else:
# # # #         df_copy['Rain_mm'] = np.nan # Crée la colonne même si aucune source n'est disponible
# # # #         warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents pour créer 'Rain_mm'. 'Rain_mm' est rempli de NaN.")
# # # #     return df_copy


# # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     # Prioriser la colonne 'Date' si elle existe pour VEA_SISSILI ou si pas de colonnes Y/M/D/H/M
# # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not any(col in df_copy.columns for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'])):
# # # #         try:
# # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime pour le bassin {bassin}: {e}")
# # # #             df_copy['Datetime'] = pd.NaT
# # # #     else:
# # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # #         for col in date_cols:
# # # #             if col in df_copy.columns:
# # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # #         try:
# # # #             existing_date_components = [col for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'] if col in df_copy.columns]
            
# # # #             if not existing_date_components:
# # # #                 raise ValueError("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute) trouvée.")

# # # #             date_strings = df_copy.apply(
# # # #                 lambda row: f"{int(row.get('Year', 2000))}-"
# # # #                             f"{int(row.get('Month', 1)):02d}-"
# # # #                             f"{int(row.get('Day', 1)):02d} "
# # # #                             f"{int(row.get('Hour', 0)):02d}:"
# # # #                             f"{int(row.get('Minute', 0)):02d}",
# # # #                 axis=1
# # # #             )
# # # #             df_copy['Datetime'] = pd.to_datetime(date_strings, errors='coerce')
            
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
# # # #             df_copy['Datetime'] = pd.NaT
            
# # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # #         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
# # # #              df_copy['Date'] = df_copy['Datetime'].dt.date
# # # #     else:
# # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

# # # #     return df_copy

# # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Effectue toutes les interpolations météorologiques en une seule passe.
# # # #     Cette fonction DOIT recevoir un DataFrame avec un DatetimeIndex.
# # # #     Il doit également contenir une colonne 'Station'.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame original avec les données interpolées,
# # # #                       la colonne 'Is_Daylight' calculée, la durée du jour, et un DatetimeIndex.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     # Assurez-vous que l'index est bien DatetimeIndex et nettoyez les NaT
# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame d'entrée pour l'interpolation DOIT avoir un DatetimeIndex.")
    
# # # #     initial_rows = len(df_processed)
# # # #     df_processed = df_processed[df_processed.index.notna()]
# # # #     if len(df_processed) == 0:
# # # #         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
# # # #     if initial_rows - len(df_processed) > 0:
# # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")
    
# # # #     print(f"DEBUG (interpolation): Type de l'index du DataFrame initial: {type(df_processed.index)}")
# # # #     print(f"DEBUG (interpolation): Premières 5 valeurs de l'index après nettoyage des NaT: {df_processed.index[:5].tolist() if not df_processed.empty else 'DataFrame vide'}")

# # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # #         raise ValueError(
# # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # #         )

# # # #     if not df_gps['Station'].is_unique:
# # # #         print("Avertissement: La colonne 'Station' dans df_gps contient des noms de station dupliqués.")
# # # #         print("Ceci peut entraîner des comportements inattendus ou des stations non reconnues.")
# # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # #         print(f"Suppression de {len(df_gps) - len(df_gps_unique)} doublons dans df_gps (en gardant la première occurrence).")
# # # #     else:
# # # #         df_gps_unique = df_gps.copy()

# # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # #     for col in numerical_cols:
# # # #         if col in df_processed.columns:
# # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # #     df_processed_parts = []

# # # #     for station_name, group in df_processed.groupby('Station'):
# # # #         group_copy = group.copy()
# # # #         print(f"DEBUG (interpolation/groupby): Début du traitement du groupe '{station_name}'.")
        
# # # #         # Standardize group_copy.index to UTC first
# # # #         # This block ensures the index is UTC-aware before proceeding
# # # #         if group_copy.index.tz is None:
# # # #             group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #         elif group_copy.index.tz != pytz.utc: # If it's already tz-aware but not UTC, convert to UTC
# # # #             group_copy.index = group_copy.index.tz_convert('UTC')
# # # #         print(f"DEBUG (interpolation/groupby): Index Datetime pour '{station_name}' STANDARDISÉ à UTC. Dtype: {group_copy.index.dtype}")
        
# # # #         # S'assurer que l'index n'a pas de NaT après localisation
# # # #         group_copy = group_copy[group_copy.index.notna()]
# # # #         if group_copy.empty:
# # # #             warnings.warn(f"Le groupe '{station_name}' est vide après nettoyage de l'index Datetime. Il sera ignoré.")
# # # #             continue


# # # #         # --- Calculs Astral en utilisant le fuseau horaire local ---
# # # #         apply_fixed_daylight = True
# # # #         gps_data = gps_info_dict.get(station_name)
# # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # #             lat = gps_data['Lat']
# # # #             long = gps_data['Long']
# # # #             timezone_str = gps_data['Timezone']

# # # #             try:
# # # #                 local_tz = pytz.timezone(timezone_str)
# # # #                 # Create a local timezone-aware version of the index for Astral calculations
# # # #                 index_for_astral_local = group_copy.index.tz_convert(local_tz)

# # # #                 daily_sun_info = {}
# # # #                 # Get unique dates from the local-time index (these are timezone-aware Timestamps)
# # # #                 # Utilisation de .drop_duplicates() au lieu de .unique() pour garantir un objet Pandas
# # # #                 unique_dates_ts_local = index_for_astral_local.normalize().drop_duplicates()

# # # #                 # Ensure unique_dates is not empty before processing
# # # #                 if unique_dates_ts_local.empty: # Utilisez .empty pour les objets Pandas
# # # #                     raise ValueError("No unique dates found for Astral calculation.")
                
# # # #                 for ts_local_aware in unique_dates_ts_local: # Iterate over timezone-aware Timestamps
# # # #                     loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
                    
# # # #                     # Convert the timezone-aware Timestamp to a naive datetime.date object
# # # #                     # This is what Astral's sun.sun function expects for its 'date' parameter
# # # #                     # This conversion helps avoid potential re-localization warnings from Astral/pytz.
# # # #                     naive_date_for_astral = ts_local_aware.to_pydatetime().date()
                    
# # # #                     # Pass the NAIVE date object to Astral. Astral's observer handles the timezone internally.
# # # #                     s = sun.sun(loc.observer, date=naive_date_for_astral) 
# # # #                     daily_sun_info[naive_date_for_astral] = {
# # # #                         'sunrise': s['sunrise'],
# # # #                         'sunset': s['sunset']
# # # #                     }

# # # #                 # Explicitly create a list of naive dates for the DataFrame index
# # # #                 naive_unique_dates_for_index = [ts.date() for ts in unique_dates_ts_local] # Use .date() here as keys are naive
# # # #                 temp_df_sun_index = pd.Index(naive_unique_dates_for_index, name='Date_Local_Naive')
# # # #                 temp_df_sun = pd.DataFrame(index=temp_df_sun_index)
                
# # # #                 # NOUVEAUX DÉBOGAGES pour comprendre le type juste avant l'opération
# # # #                 print(f"DEBUG (astral_calc): unique_dates_ts_local type: {type(unique_dates_ts_local)}")
# # # #                 print(f"DEBUG (astral_calc): naive_unique_dates_for_index type: {type(naive_unique_dates_for_index)}")
# # # #                 print(f"DEBUG (astral_calc): temp_df_sun_index type: {type(temp_df_sun_index)}")
# # # #                 if not temp_df_sun.empty:
# # # #                     print(f"DEBUG (astral_calc): First element of temp_df_sun.index: {temp_df_sun.index[0]}")
# # # #                     print(f"DEBUG (astral_calc): Type of first element of temp_df_sun.index: {type(temp_df_sun.index[0])}")

# # # #                 # Correction: Utilisation de la compréhension de liste pour éviter le problème de .map()
# # # #                 temp_df_sun['sunrise_time_local'] = [daily_sun_info.get(date, {}).get('sunrise') for date in temp_df_sun.index]
# # # #                 temp_df_sun['sunset_time_local'] = [daily_sun_info.get(date, {}).get('sunset') for date in temp_df_sun.index]

# # # #                 # Merge with group_copy (which has UTC index)
# # # #                 # To merge, create a normalized local date column (naive) in group_copy
# # # #                 group_copy_reset = group_copy.reset_index()
# # # #                 group_copy_reset['Date_Local_Naive'] = group_copy_reset['Datetime'].dt.tz_convert(local_tz).dt.date

# # # #                 group_copy_reset = pd.merge(group_copy_reset, temp_df_sun, on='Date_Local_Naive', how='left')

# # # #                 # Convert merged local times back to UTC for comparison with original 'Datetime' (which is UTC)
# # # #                 group_copy_reset['sunrise_time_utc'] = group_copy_reset['sunrise_time_local'].dt.tz_convert('UTC')
# # # #                 group_copy_reset['sunset_time_utc'] = group_copy_reset['sunset_time_local'].dt.tz_convert('UTC')

# # # #                 group_copy_reset.loc[:, 'Is_Daylight'] = (group_copy_reset['Datetime'] >= group_copy_reset['sunrise_time_utc']) & \
# # # #                                                           (group_copy_reset['Datetime'] < group_copy_reset['sunset_time_utc'])

# # # #                 daylight_timedelta_local = group_copy_reset['sunset_time_local'] - group_copy_reset['sunrise_time_local']
# # # #                 group_copy_reset.loc[:, 'Daylight_Duration'] = daylight_timedelta_local.astype(str).replace('NaT', np.nan)

# # # #                 group_copy = group_copy_reset.set_index('Datetime')
# # # #                 group_copy = group_copy.drop(columns=['Date_Local_Naive', 'sunrise_time_local', 'sunset_time_local', 'sunrise_time_utc', 'sunset_time_utc'], errors='ignore')

# # # #                 print(f"Lever et coucher du soleil calculés pour {station_name}.")
# # # #                 apply_fixed_daylight = False

# # # #             except Exception as e:
# # # #                 print(f"Erreur lors du calcul du lever/coucher du soleil avec Astral pour {station_name}: {e}.")
# # # #                 traceback.print_exc() # Print full traceback for this specific error
# # # #                 warnings.warn(f"Calcul Astral impossible pour '{station_name}'. Utilisation de l'indicateur jour/nuit fixe.")
# # # #                 apply_fixed_daylight = True
# # # #         else:
# # # #             print(f"Avertissement: Coordonnées ou Fuseau horaire manquants/invalides pour le site '{station_name}' dans df_gps. Utilisation de l'indicateur jour/nuit fixe.")
# # # #             apply_fixed_daylight = True

# # # #         if apply_fixed_daylight:
# # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
# # # #             print(f"Utilisation de l'indicateur jour/nuit fixe (7h-18h) pour {station_name}.")

# # # #         df_processed_parts.append(group_copy)

# # # #     if not df_processed_parts:
# # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # #     # Concaténation de toutes les parties. Puisque chaque partie a un DatetimeIndex,
# # # #     # nous pouvons concaténer directement sans ignore_index=True
# # # #     df_final = pd.concat(df_processed_parts)
# # # #     # Assurez-vous que l'index est trié et qu'il est bien nommé 'Datetime'
# # # #     df_final = pd.concat(df_processed_parts).sort_index()
# # # #     df_final.index.name = 'Datetime' 
# # # #     print(f"DEBUG (interpolation/concat): Index du DataFrame final après concaténation et tri: {type(df_final.index)}")
# # # #     print(f"DEBUG (interpolation/concat): Colonnes du DataFrame final après concaténation: {df_final.columns.tolist()}")


# # # #     # Nettoyage des colonnes temporaires (elles ne sont plus nécessaires ou n'existent plus)
# # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date']
# # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # #     # Gestion intelligente de Rain_mm
# # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # #             df_final = create_rain_mm(df_final)
# # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # #         else:
# # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # #             if 'Rain_mm' not in df_final.columns:
# # # #                 df_final['Rain_mm'] = np.nan


# # # #     # Interpolation standard et bornage pour les variables numériques
# # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']

# # # #     for var in standard_vars:
# # # #         if var in df_final.columns:
# # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # #             if var in limits:
# # # #                 min_val = limits[var]['min']
# # # #                 max_val = limits[var]['max']
# # # #                 initial_nan_count = df_final[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_final.loc[df_final[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_final.loc[df_final[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_final[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # #             print(f"DEBUG (interpolation/variable): Interpolation de '{var}'. Type de l'index de df_final: {type(df_final.index)}")
            
# # # #             # Interpolation seulement si l'index est bien DatetimeIndex
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/variable): L'index n'est pas un DatetimeIndex pour l'interpolation de '{var}'. Utilisation de la méthode 'linear'.")
# # # #                 df_final[var] = df_final[var].interpolate(method='linear', limit_direction='both')
# # # #             df_final[var] = df_final[var].bfill().ffill()

# # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # #         if 'Solar_R_W/m^2' in limits:
# # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # #         if 'Is_Daylight' in df_final.columns:
# # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # #             if 'Rain_mm' in df_final.columns:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # #             else:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # #             print(f"DEBUG (interpolation/solaire): Interpolation de 'Solar_R_W/m^2' (conditionnel). Type de l'index de df_final: {type(df_final.index)}")

# # # #             is_day = df_final['Is_Daylight']
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/solaire): L'index n'est pas un DatetimeIndex pour l'interpolation de 'Solar_R_W/m^2'. Utilisation de la méthode 'linear'.")
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both')

# # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # #         else:
# # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()
# # # #             else:
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both').bfill().ffill()

# # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # #     missing_after_interp = df_final.isna().sum()
# # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # #     if not columns_with_missing.empty:
# # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # #     else:
# # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # #     return df_final


# # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # #         raise ValueError(
# # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # #         )

# # # #     def convert_row(row):
# # # #         try:
# # # #             zone = int(row['zone'])
# # # #             hemisphere = str(row['hemisphere']).upper()
# # # #             is_northern = hemisphere == 'N'

# # # #             proj_utm = CRS.from_proj4(
# # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # #             )
# # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # #         except Exception as e:
# # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # #     return df_copy

# # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # #     """
# # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
# # # #     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

# # # #     Returns:
# # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # #     """
# # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # #     data_dir = 'data'
# # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # #     files_info = [
# # # #         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
# # # #         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
# # # #         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
# # # #     ]

# # # #     loaded_dfs = []

# # # #     for file_info in files_info:
# # # #         output_file_path = os.path.join(data_dir, file_info['name'])
        
# # # #         if not os.path.exists(output_file_path):
# # # #             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
# # # #             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
# # # #             print(f"Téléchargement de {file_info['bassin']} terminé.")
# # # #         else:
# # # #             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
# # # #         loaded_dfs.append(pd.read_excel(output_file_path))

# # # #     vea_sissili_bassin = loaded_dfs[0]
# # # #     dano_bassin = loaded_dfs[1]
# # # #     dassari_bassin = loaded_dfs[2]

# # # #     # Prétraitement des DataFrames (votre code original)
# # # #     print("Début du prétraitement des données de stations...")
    
# # # #     # Vea Sissili
# # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # #     # Dassari
# # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # #     dano_bassin['zone'] = 30
# # # #     dano_bassin['hemisphere'] = 'N'
# # # #     dassari_bassin['zone'] = 31
# # # #     dassari_bassin['hemisphere'] = 'N'

# # # #     # Application de la fonction de conversion UTM vers GPS
# # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # #     # Ajout des fuseaux horaires
# # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # #     # Fusion de tous les bassins
# # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # #     # Renommer 'Name' en 'Station'
# # # #     bassins = bassins.rename(columns={'Name': 'Station'})

# # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # #     initial_rows = len(bassins)
# # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # #     if len(bassins) < initial_rows:
# # # #         print(f"Attention: {initial_rows - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # #     # 5. Sauvegarde du DataFrame final en JSON
# # # #     output_json_path = os.path.join(data_dir, "station_coordinates.json")
# # # #     # Utiliser to_json avec orient='records' pour un format plus lisible et facile à charger
# # # #     bassins.to_json(output_json_path, orient='records', indent=4)
# # # #     print(f"\nPréparation des données terminée. Coordonnées des stations sauvegardées dans '{output_json_path}'.")
# # # #     print("Vous pouvez maintenant lancer votre application Flask.")

# # # #     return bassins # Retourner le DataFrame des données GPS


# # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
# # # #     Conserve la première occurrence en cas de doublon.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée.

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame sans doublons.
# # # #     """
# # # #     if 'Station' in df.columns and 'Datetime' in df.columns:
# # # #         initial_rows = len(df)
# # # #         df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
# # # #         if len(df_cleaned) < initial_rows:
# # # #             warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
# # # #         return df_cleaned
# # # #     else:
# # # #         warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
# # # #         return df

# # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # #     """
# # # #     Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
# # # #         try:
# # # #             df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
# # # #             df_processed = df_processed[df_processed.index.notna()]
# # # #             if df_processed.empty:
# # # #                 raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
# # # #         except Exception as e:
# # # #             raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

# # # #     for var, vals in limits.items():
# # # #         if var in df_processed.columns:
# # # #             min_val = vals.get('min')
# # # #             max_val = vals.get('max')
# # # #             if min_val is not None or max_val is not None:
# # # #                 initial_nan_count = df_processed[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_processed.loc[df_processed[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_processed[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
# # # #     return df_processed

# # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
# # # #     Retourne l'objet Figure Plotly.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return go.Figure() # Retourne une figure vide si pas de données

# # # #     if periode == 'Journalière':
# # # #         resampled_df = filtered_df[variable].resample('D').mean()
# # # #     elif periode == 'Hebdomadaire':
# # # #         resampled_df = filtered_df[variable].resample('W').mean()
# # # #     elif periode == 'Mensuelle':
# # # #         resampled_df = filtered_df[variable].resample('M').mean()
# # # #     elif periode == 'Annuelle':
# # # #         resampled_df = filtered_df[variable].resample('Y').mean()
# # # #     else: # Données Brutes
# # # #         resampled_df = filtered_df[variable]

# # # #     resampled_df = resampled_df.dropna()

# # # #     if resampled_df.empty:
# # # #         return go.Figure() # Retourne une figure vide si pas de données après resample/dropna

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     color = colors.get(station, '#1f77b4')

# # # #     fig = go.Figure()
# # # #     fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                              mode='lines', name=f'{variable_meta["Nom"]} - {station}',
# # # #                              line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified",
# # # #         template="plotly_white" # Utiliser un template Plotly plus clair
# # # #     )
# # # #     return fig

# # # # def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
# # # #     """
# # # #     Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
# # # #     Retourne l'objet Figure Plotly.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

# # # #     fig = go.Figure()
    
# # # #     all_stations = df['Station'].unique()
# # # #     if len(all_stations) < 2:
# # # #         warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
# # # #         return go.Figure() # Retourne une figure vide si moins de 2 stations

# # # #     for station in all_stations:
# # # #         filtered_df = df[df['Station'] == station].copy()
# # # #         if filtered_df.empty:
# # # #             continue

# # # #         if periode == 'Journalière':
# # # #             resampled_df = filtered_df[variable].resample('D').mean()
# # # #         elif periode == 'Hebdomadaire':
# # # #             resampled_df = filtered_df[variable].resample('W').mean()
# # # #         elif periode == 'Mensuelle':
# # # #             resampled_df = filtered_df[variable].resample('M').mean()
# # # #         elif periode == 'Annuelle':
# # # #             resampled_df = filtered_df[variable].resample('Y').mean()
# # # #         else: # Données Brutes
# # # #             resampled_df = filtered_df[variable]

# # # #         resampled_df = resampled_df.dropna()
# # # #         if resampled_df.empty:
# # # #             continue
        
# # # #         color = colors.get(station, '#1f77b4')
# # # #         fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                                  mode='lines', name=station,
# # # #                                  line=dict(color=color)))

# # # #     if not fig.data:
# # # #         return go.Figure() # Retourne une figure vide si aucune trace n'a été ajoutée

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     fig.update_layout(
# # # #         title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified",
# # # #         legend_title="Variables",
# # # #         template="plotly_white"
# # # #     )
# # # #     return fig


# # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> go.Figure:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
# # # #     Retourne l'objet Figure Plotly.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return go.Figure() # Retourne une figure vide si pas de données

# # # #     numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight', 'Daylight_Duration']]

# # # #     if not numerical_vars:
# # # #         warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
# # # #         return go.Figure() # Retourne une figure vide si pas de variables numériques

# # # #     normalized_df = filtered_df[numerical_vars].copy()
# # # #     for col in normalized_df.columns:
# # # #         min_val = normalized_df[col].min()
# # # #         max_val = normalized_df[col].max()
# # # #         if max_val != min_val:
# # # #             normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
# # # #         else:
# # # #             normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

# # # #     normalized_df = normalized_df.dropna(how='all')

# # # #     if normalized_df.empty:
# # # #         return go.Figure() # Retourne une figure vide si pas de données après normalisation/dropna
    
# # # #     fig = go.Figure()
# # # #     for var in normalized_df.columns:
# # # #         var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
# # # #         color = colors.get(var, None)

# # # #         fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
# # # #                                  mode='lines', name=var_meta['Nom'],
# # # #                                  line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
# # # #         xaxis_title="Date",
# # # #         yaxis_title="Valeur Normalisée (0-1)",
# # # #         hovermode="x unified",
# # # #         legend_title="Variables",
# # # #         template="plotly_white"
# # # #     )
# # # #     return fig

# # # # def calculate_daily_summary_table(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # # #     groupées par station. Cette fonction renvoie un DataFrame de statistiques, non un graphique.
# # # #     C'est la fonction qui remplace l'ancienne 'daily_stats'.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     # Assurez-vous que 'Datetime' est une colonne et de type datetime
# # # #     if isinstance(df_copy.index, pd.DatetimeIndex):
# # # #         df_copy = df_copy.reset_index()

# # # #     df_copy['Datetime'] = pd.to_datetime(df_copy['Datetime'], errors='coerce')
# # # #     df_copy = df_copy.dropna(subset=['Datetime', 'Station'])

# # # #     if df_copy.empty:
# # # #         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans calculate_daily_summary_table.")
# # # #         return pd.DataFrame()

# # # #     if 'Is_Daylight' not in df_copy.columns:
# # # #         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
# # # #         df_copy['Is_Daylight'] = (df_copy['Datetime'].dt.hour >= 7) & (df_copy['Datetime'].dt.hour <= 18)

# # # #     numerical_cols = [col for col in df_copy.columns if pd.api.types.is_numeric_dtype(df_copy[col]) and col not in ['Station', 'Datetime', 'Is_Daylight', 'Daylight_Duration']]
    
# # # #     if not numerical_cols:
# # # #         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
# # # #         return pd.DataFrame()

# # # #     # Calcul des statistiques de base par jour
# # # #     daily_aggregated_df = df_copy.groupby(['Station', df_copy['Datetime'].dt.date]).agg({
# # # #         col: ['mean', 'min', 'max'] for col in numerical_cols if METADATA_VARIABLES.get(col, {}).get('is_rain') == False
# # # #     })

# # # #     # Renommage des colonnes agrégées pour les non-pluies
# # # #     daily_aggregated_df.columns = ['_'.join(col).strip() for col in daily_aggregated_df.columns.values]


# # # #     # Traitement spécifique pour la pluie (Rain_mm)
# # # #     if 'Rain_mm' in df_copy.columns and METADATA_VARIABLES.get('Rain_mm', {}).get('is_rain'):
# # # #         df_daily_rain = df_copy.groupby(['Station', df_copy['Datetime'].dt.date])['Rain_mm'].sum().reset_index()
# # # #         df_daily_rain = df_daily_rain.rename(columns={'Rain_mm': 'Rain_mm_sum'})

# # # #         # Fusionner les statistiques de pluie avec les autres
# # # #         if not daily_aggregated_df.empty:
# # # #             daily_aggregated_df = daily_aggregated_df.reset_index()
# # # #             daily_stats_df = pd.merge(daily_aggregated_df, df_daily_rain, on=['Station', 'Datetime'], how='left')
# # # #             daily_stats_df = daily_stats_df.rename(columns={'Datetime': 'Date'})
# # # #         else:
# # # #             daily_stats_df = df_daily_rain.rename(columns={'Datetime': 'Date'})
# # # #     else:
# # # #         daily_stats_df = daily_aggregated_df.reset_index().rename(columns={'Datetime': 'Date'})


# # # #     # Calcul des statistiques de saison et de sécheresse pour la pluie
# # # #     if 'Rain_mm' in df_copy.columns and METADATA_VARIABLES.get('Rain_mm', {}).get('is_rain'):
# # # #         df_daily_rain_raw = df_copy.groupby(['Station', pd.Grouper(key='Datetime', freq='D')])['Rain_mm'].sum().reset_index()
        
# # # #         RAIN_SEASON_GAP_THRESHOLD = pd.Timedelta(days=60)
# # # #         season_stats = []
# # # #         for station_name, station_df_rain in df_daily_rain_raw.groupby('Station'):
# # # #             station_df_rain = station_df_rain.set_index('Datetime').sort_index()
# # # #             rain_events = station_df_rain[station_df_rain['Rain_mm'] > 0].index

# # # #             if rain_events.empty:
# # # #                 season_stats.append({'Station': station_name, 'Moyenne_Saison_Pluvieuse': np.nan, 'Debut_Saison_Pluvieuse': pd.NaT, 'Fin_Saison_Pluvieuse': pd.NaT, 'Duree_Saison_Pluvieuse_Jours': np.nan})
# # # #                 continue
            
# # # #             block_ids = (rain_events.to_series().diff() > RAIN_SEASON_GAP_THRESHOLD).cumsum()
# # # #             valid_blocks = {}
# # # #             for block_id, rain_dates_in_block in rain_events.to_series().groupby(block_ids):
# # # #                 if not rain_dates_in_block.empty:
# # # #                     block_start = rain_dates_in_block.min()
# # # #                     block_end = rain_dates_in_block.max()
# # # #                     full_block_df = station_df_rain.loc[block_start:block_end]
# # # #                     valid_blocks[block_id] = full_block_df

# # # #             if not valid_blocks:
# # # #                 season_stats.append({'Station': station_name, 'Moyenne_Saison_Pluvieuse': np.nan, 'Debut_Saison_Pluvieuse': pd.NaT, 'Fin_Saison_Pluvieuse': pd.NaT, 'Duree_Saison_Pluvieuse_Jours': np.nan})
# # # #                 continue

# # # #             main_block_id = max(valid_blocks, key=lambda k: (valid_blocks[k].index.max() - valid_blocks[k].index.min()).days)
# # # #             main_season_df = valid_blocks[main_block_id]

# # # #             debut_saison = main_season_df.index.min()
# # # #             fin_saison = main_season_df.index.max()
# # # #             total_days = (fin_saison - debut_saison).days + 1
# # # #             moyenne_saison = main_season_df['Rain_mm'].sum() / total_days if total_days > 0 else 0

# # # #             season_stats.append({
# # # #                 'Station': station_name,
# # # #                 'Moyenne_Saison_Pluvieuse': moyenne_saison,
# # # #                 'Debut_Saison_Pluvieuse': debut_saison,
# # # #                 'Fin_Saison_Pluvieuse': fin_saison,
# # # #                 'Duree_Saison_Pluvieuse_Jours': total_days
# # # #             })
# # # #         df_season_stats = pd.DataFrame(season_stats)
        
# # # #         # Merge season stats into main daily_stats_df
# # # #         if not df_season_stats.empty:
# # # #             daily_stats_df = pd.merge(daily_stats_df, df_season_stats, on='Station', how='left')


# # # #     # Calcul des statistiques globales (Max, Min, Moyenne, Médiane, etc.) par station
# # # #     # pour les variables numériques, à inclure dans le tableau récapitulatif.
# # # #     final_stats_per_station = pd.DataFrame()
# # # #     for station_name in df_copy['Station'].unique():
# # # #         station_df = df_copy[df_copy['Station'] == station_name].copy()
# # # #         station_summary = {'Station': station_name}

# # # #         for var in numerical_cols:
# # # #             if var in station_df.columns and pd.api.types.is_numeric_dtype(station_df[var]):
# # # #                 # Filtrage pour la radiation solaire (seulement pendant le jour)
# # # #                 if var == 'Solar_R_W/m^2':
# # # #                     var_data = station_df.loc[station_df['Is_Daylight'], var].dropna()
# # # #                 else:
# # # #                     var_data = station_df[var].dropna()
                
# # # #                 if not var_data.empty:
# # # #                     station_summary[f'{var}_Maximum'] = var_data.max()
# # # #                     station_summary[f'{var}_Minimum'] = var_data.min()
# # # #                     station_summary[f'{var}_Moyenne'] = var_data.mean()
# # # #                     station_summary[f'{var}_Mediane'] = var_data.median()
                    
# # # #                     # Pour Rain_mm uniquement
# # # #                     if var == 'Rain_mm':
# # # #                         station_summary[f'{var}_Cumul_Annuel'] = station_df['Rain_mm'].sum()
# # # #                         # Moyenne des jours pluvieux (seulement les jours où il a plu)
# # # #                         rainy_days_data = station_df[station_df['Rain_mm'] > 0]['Rain_mm'].dropna()
# # # #                         station_summary[f'{var}_Moyenne_Jours_Pluvieux'] = rainy_days_data.mean() if not rainy_days_data.empty else np.nan

# # # #                         # Ajouter la durée de la saison pluvieuse et de sécheresse du df_season_stats
# # # #                         if 'Duree_Saison_Pluvieuse_Jours' in daily_stats_df.columns:
# # # #                             s_data = daily_stats_df[daily_stats_df['Station'] == station_name]
# # # #                             if not s_data.empty:
# # # #                                 station_summary[f'{var}_Duree_Saison_Pluvieuse_Jours'] = s_data['Duree_Saison_Pluvieuse_Jours'].iloc[0]
# # # #                                 # Placeholder pour sécheresse si non calculé ailleurs
# # # #                                 station_summary[f'{var}_Duree_Secheresse_Definie_Jours'] = np.nan # Vous devrez calculer ceci plus tard

# # # #         final_stats_per_station = pd.concat([final_stats_per_station, pd.DataFrame([station_summary])], ignore_index=True)
        
# # # #     return final_stats_per_station # Retourne le DataFrame de statistiques agrégées par station


# # # # def generate_variable_summary_plots_for_web(df: pd.DataFrame, station: str, variable: str, metadata: dict, palette: dict) -> plt.Figure:
# # # #     """
# # # #     Génère un graphique Matplotlib/Seaborn pour les statistiques agrégées d'une variable spécifique
# # # #     pour une station donnée, en utilisant la logique fournie par l'utilisateur pour 'daily_stats'.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame global de données traitées avec DatetimeIndex et colonne 'Station'.
# # # #         station (str): Le nom de la station à visualiser.
# # # #         variable (str): La variable à visualiser (e.g., 'Air_Temp_Deg_C', 'Rain_mm').
# # # #         metadata (dict): Dictionnaire de métadonnées pour les variables (Nom, Unite, is_rain).
# # # #         palette (dict): Dictionnaire de couleurs pour les différentes métriques statistiques.

# # # #     Returns:
# # # #         plt.Figure: Un objet Figure Matplotlib contenant tous les sous-graphiques pour la variable sélectionnée.
# # # #     """
# # # #     df_station = df[df['Station'] == station].copy()

# # # #     if df_station.empty:
# # # #         fig, ax = plt.subplots(figsize=(10, 6))
# # # #         ax.text(0.5, 0.5, f"Aucune donnée pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# # # #         ax.axis('off')
# # # #         return fig

# # # #     # S'assurer que Datetime est un index datetime correct pour les données de la station
# # # #     if isinstance(df_station.index, pd.DatetimeIndex):
# # # #         df_station = df_station.reset_index() # Réinitialise l'index pour un accès facile aux colonnes
    
# # # #     df_station['Datetime'] = pd.to_datetime(df_station['Datetime'], errors='coerce')
# # # #     df_station = df_station.dropna(subset=['Datetime', 'Station'])
# # # #     df_station = df_station.set_index('Datetime').sort_index() # Remet l'index pour les opérations de séries temporelles

# # # #     if df_station.empty:
# # # #         fig, ax = plt.subplots(figsize=(10, 6))
# # # #         ax.text(0.5, 0.5, f"DataFrame vide après nettoyage des dates pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# # # #         ax.axis('off')
# # # #         return fig

# # # #     # Assurez-vous que 'Is_Daylight' existe (généralement géré par interpolation, mais au cas où)
# # # #     if 'Is_Daylight' not in df_station.columns:
# # # #         df_station['Is_Daylight'] = (df_station.index.hour >= 7) & (df_station.index.hour <= 18)


# # # #     var_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})

# # # #     # Dictionnaire pour stocker les statistiques pour la variable sélectionnée
# # # #     stats_for_plot = {}
# # # #     metrics_to_plot = []
    
# # # #     # --- LOGIQUE SPÉCIFIQUE À RAIN_MM (transférée de la fonction daily_stats de l'utilisateur) ---
# # # #     if var_meta.get('is_rain', False) and variable == 'Rain_mm':
# # # #         df_daily_rain = df_station.groupby(pd.Grouper(freq='D'))['Rain_mm'].sum().reset_index()
# # # #         df_daily_rain = df_daily_rain.rename(columns={'Rain_mm': 'Rain_mm_sum'})
# # # #         df_daily_rain['Datetime'] = pd.to_datetime(df_daily_rain['Datetime'])
# # # #         df_daily_rain = df_daily_rain.set_index('Datetime').sort_index()

# # # #         RAIN_SEASON_GAP_THRESHOLD = pd.Timedelta(days=60)
# # # #         rain_events = df_daily_rain[df_daily_rain['Rain_mm_sum'] > 0].index

# # # #         s_moyenne_saison = np.nan
# # # #         s_duree_saison = np.nan
# # # #         s_debut_saison = pd.NaT
# # # #         s_fin_saison = pd.NaT

# # # #         if not rain_events.empty:
# # # #             block_ids = (rain_events.to_series().diff() > RAIN_SEASON_GAP_THRESHOLD).cumsum()
# # # #             valid_blocks = {}
# # # #             for block_id, rain_dates_in_block in rain_events.to_series().groupby(block_ids):
# # # #                 if not rain_dates_in_block.empty:
# # # #                     block_start = rain_dates_in_block.min()
# # # #                     block_end = rain_dates_in_block.max()
# # # #                     full_block_df = df_daily_rain.loc[block_start:block_end]
# # # #                     valid_blocks[block_id] = full_block_df

# # # #             if valid_blocks:
# # # #                 main_block_id = max(valid_blocks, key=lambda k: (valid_blocks[k].index.max() - valid_blocks[k].index.min()).days)
# # # #                 main_season_df = valid_blocks[main_block_id]

# # # #                 s_debut_saison = main_season_df.index.min()
# # # #                 s_fin_saison = main_season_df.index.max()
# # # #                 total_days_season = (s_fin_saison - s_debut_saison).days + 1
# # # #                 s_moyenne_saison = main_season_df['Rain_mm_sum'].sum() / total_days_season if total_days_season > 0 else 0
# # # #                 s_duree_saison = total_days_season

# # # #         # Logique de détection de la sécheresse
# # # #         longest_dry_spell = np.nan
# # # #         debut_secheresse = pd.NaT
# # # #         fin_secheresse = pd.NaT

# # # #         full_daily_series_rain = df_daily_rain['Rain_mm_sum'].resample('D').sum().fillna(0)
# # # #         rainy_days_index = full_daily_series_rain[full_daily_series_rain > 0].index

# # # #         if not rainy_days_index.empty and pd.notna(s_moyenne_saison) and s_moyenne_saison > 0:
# # # #             temp_dry_spells = []
# # # #             for i in range(1, len(rainy_days_index)):
# # # #                 prev_rain_date = rainy_days_index[i-1]
# # # #                 current_rain_date = rainy_days_index[i]
# # # #                 dry_days_between_rains = (current_rain_date - prev_rain_date).days - 1

# # # #                 if dry_days_between_rains > 0:
# # # #                     rain_prev_day = full_daily_series_rain.loc[prev_rain_date]
# # # #                     temp_debut = pd.NaT
# # # #                     temp_duree = 0
# # # #                     for j in range(1, dry_days_between_rains + 1):
# # # #                         current_dry_date = prev_rain_date + timedelta(days=j)
# # # #                         current_ratio = rain_prev_day / j
# # # #                         if current_ratio < s_moyenne_saison: # Condition pour la sécheresse définie
# # # #                             temp_debut = current_dry_date
# # # #                             temp_duree = (current_rain_date - temp_debut).days
# # # #                             break
# # # #                     if pd.notna(temp_debut) and temp_duree > 0:
# # # #                         temp_dry_spells.append({
# # # #                             'Duree': temp_duree,
# # # #                             'Debut': temp_debut,
# # # #                             'Fin': current_rain_date - timedelta(days=1)
# # # #                         })
            
# # # #             if temp_dry_spells:
# # # #                 df_temp_dry = pd.DataFrame(temp_dry_spells)
# # # #                 idx_max_dry = df_temp_dry['Duree'].idxmax()
# # # #                 longest_dry_spell = df_temp_dry.loc[idx_max_dry, 'Duree']
# # # #                 debut_secheresse = df_temp_dry.loc[idx_max_dry, 'Debut']
# # # #                 fin_secheresse = df_temp_dry.loc[idx_max_dry, 'Fin']

# # # #         # Statistiques pour Rain_mm (tirées de la vue consolidée de daily_stats de l'utilisateur)
# # # #         rain_data_for_stats = df_station[variable].dropna()
        
# # # #         stats_for_plot['Maximum'] = rain_data_for_stats.max() if not rain_data_for_stats.empty else np.nan
# # # #         stats_for_plot['Minimum'] = rain_data_for_stats.min() if not rain_data_for_stats.empty else np.nan
# # # #         stats_for_plot['Mediane'] = rain_data_for_stats.median() if not rain_data_for_stats.empty else np.nan
# # # #         stats_for_plot['Cumul_Annuel'] = df_station[variable].sum()
        
# # # #         rainy_days_data = df_station[df_station[variable] > 0][variable].dropna()
# # # #         stats_for_plot['Moyenne_Jours_Pluvieux'] = rainy_days_data.mean() if not rainy_days_data.empty else np.nan
        
# # # #         stats_for_plot['Moyenne_Saison_Pluvieuse'] = s_moyenne_saison
# # # #         stats_for_plot['Duree_Saison_Pluvieuse_Jours'] = s_duree_saison
# # # #         stats_for_plot['Debut_Saison_Pluvieuse'] = s_debut_saison
# # # #         stats_for_plot['Fin_Saison_Pluvieuse'] = s_fin_saison
        
# # # #         stats_for_plot['Duree_Secheresse_Definie_Jours'] = longest_dry_spell
# # # #         stats_for_plot['Debut_Secheresse_Definie'] = debut_secheresse
# # # #         stats_for_plot['Fin_Secheresse_Definie'] = fin_secheresse

# # # #         # Récupérer les dates pour max/min pour Rain_mm
# # # #         max_date_idx = df_station[variable].idxmax() if not df_station[variable].empty else pd.NaT
# # # #         min_date_idx = df_station[variable].idxmin() if not df_station[variable].empty else pd.NaT
# # # #         stats_for_plot['Date_Maximum'] = max_date_idx if pd.notna(max_date_idx) else pd.NaT
# # # #         stats_for_plot['Date_Minimum'] = min_date_idx if pd.notna(min_date_idx) else pd.NaT
        
# # # #         metrics_to_plot = [
# # # #             'Maximum', 'Minimum', 'Cumul_Annuel', 'Mediane',
# # # #             'Moyenne_Jours_Pluvieux', 'Moyenne_Saison_Pluvieuse',
# # # #             'Duree_Saison_Pluvieuse_Jours', 'Duree_Secheresse_Definie_Jours'
# # # #         ]
# # # #         nrows, ncols = 4, 2
# # # #         figsize = (18, 16)
        
# # # #     # --- LOGIQUE POUR LES AUTRES VARIABLES (transférée de la fonction daily_stats de l'utilisateur) ---
# # # #     else:
# # # #         current_var_data = df_station[variable].dropna()
# # # #         if variable == 'Solar_R_W/m^2':
# # # #             current_var_data = df_station.loc[df_station['Is_Daylight'], variable].dropna()

# # # #         if current_var_data.empty:
# # # #             fig, ax = plt.subplots(figsize=(10, 6))
# # # #             ax.text(0.5, 0.5, f"Aucune donnée valide pour la variable {var_meta['Nom']} à {station}.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# # # #             ax.axis('off')
# # # #             return fig

# # # #         # Statistiques pour les autres variables (de daily_stats de l'utilisateur)
# # # #         stats_for_plot['Maximum'] = current_var_data.max()
# # # #         stats_for_plot['Minimum'] = current_var_data.min()
# # # #         stats_for_plot['Mediane'] = current_var_data.median()
# # # #         stats_for_plot['Moyenne'] = current_var_data.mean()

# # # #         # Récupérer les dates pour max/min
# # # #         max_idx = current_var_data.idxmax() if not current_var_data.empty else pd.NaT
# # # #         min_idx = current_var_data.idxmin() if not current_var_data.empty else pd.NaT

# # # #         stats_for_plot['Date_Maximum'] = max_idx if pd.notna(max_idx) else pd.NaT
# # # #         stats_for_plot['Date_Minimum'] = min_idx if pd.notna(min_idx) else pd.NaT

# # # #         metrics_to_plot = ['Maximum', 'Minimum', 'Moyenne', 'Mediane']
# # # #         nrows, ncols = 2, 2
# # # #         figsize = (18, 12)

# # # #     # Gérer le cas où stats_for_plot pourrait être vide (ex: toutes les données sont NaN)
# # # #     if not stats_for_plot:
# # # #         fig, ax = plt.subplots(figsize=(10, 6))
# # # #         ax.text(0.5, 0.5, f"Impossible de calculer des statistiques pour la variable '{variable}' à la station '{station}' (données manquantes ou non numériques).", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# # # #         ax.axis('off')
# # # #         return fig

# # # #     # --- PARTIE COMMUNE DE TRACÉ (adaptée de la fonction daily_stats de l'utilisateur) ---
# # # #     fig, axes = plt.subplots(nrows, ncols, figsize=figsize)
# # # #     plt.subplots_adjust(hspace=0.6, wspace=0.4) # Ajustement de l'espacement pour un meilleur ajustement
# # # #     axes = axes.flatten()

# # # #     fig.suptitle(f'Statistiques de {var_meta["Nom"]} pour la station {station}', fontsize=16, y=0.98)

# # # #     for i, metric in enumerate(metrics_to_plot):
# # # #         ax = axes[i]
# # # #         value = stats_for_plot.get(metric)
# # # #         if pd.isna(value):
# # # #             ax.text(0.5, 0.5, "Données non disponibles", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=12, color='gray')
# # # #             ax.axis('off')
# # # #             continue

# # # #         color = palette.get(metric.replace(' ', '_'), '#cccccc') # Utilisation de la `palette` passée en argument
        
# # # #         # Créer un DataFrame factice pour sns.barplot, car il attend un DataFrame
# # # #         plot_data_bar = pd.DataFrame({'Metric': [metric.replace('_', ' ')], 'Value': [value]})
# # # #         sns.barplot(x='Metric', y='Value', data=plot_data_bar, ax=ax, color=color, edgecolor='none')

# # # #         # Ajouter les annotations
# # # #         text = ""
# # # #         if metric in ['Duree_Saison_Pluvieuse_Jours', 'Duree_Secheresse_Definie_Jours']:
# # # #             start_date_key = f'Debut_{metric.replace("Jours", "")}'
# # # #             end_date_key = f'Fin_{metric.replace("Jours", "")}'
# # # #             start_date = stats_for_plot.get(start_date_key)
# # # #             end_date = stats_for_plot.get(end_date_key)
# # # #             date_info = ""
# # # #             if pd.notna(start_date) and pd.notna(end_date):
# # # #                 date_info = f"\ndu {start_date.strftime('%d/%m/%Y')} au {end_date.strftime('%d/%m/%Y')}"
# # # #             text = f"{int(value)} j{date_info}"
# # # #         elif metric in ['Maximum', 'Minimum', 'Cumul_Annuel', 'Moyenne_Jours_Pluvieux', 'Moyenne_Saison_Pluvieuse', 'Mediane', 'Moyenne']:
# # # #             unit = var_meta['Unite']
# # # #             date_str = ''
# # # #             # Vérifier les clés de date pour max/min si elles existent dans stats_for_plot
# # # #             if (metric == 'Maximum' and 'Date_Maximum' in stats_for_plot and pd.notna(stats_for_plot['Date_Maximum'])):
# # # #                 date_str = f"\n({stats_for_plot['Date_Maximum'].strftime('%d/%m/%Y')})"
# # # #             elif (metric == 'Minimum' and 'Date_Minimum' in stats_for_plot and pd.notna(stats_for_plot['Date_Minimum'])):
# # # #                 date_str = f"\n({stats_for_plot['Date_Minimum'].strftime('%d/%m/%Y')})"
            
# # # #             text = f"{value:.1f} {unit}{date_str}"
# # # #         else:
# # # #             text = f"{value:.1f} {var_meta['Unite']}"

# # # #         # Positionne le texte sur le graphique.
# # # #         ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
# # # #                 text, ha='center', va='bottom', fontsize=9, color='black',
# # # #                 bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))
        
# # # #         ax.set_title(f"{var_meta['Nom']} {metric.replace('_', ' ')}", fontsize=11)
# # # #         ax.set_xlabel("")
# # # #         ax.set_ylabel(f"Valeur ({var_meta['Unite']})", fontsize=10)
# # # #         ax.tick_params(axis='x', rotation=0)
# # # #         ax.set_xticklabels([])

# # # #     # Si moins de graphiques que de sous-graphiques, désactiver les axes inutilisés
# # # #     for j in range(i + 1, len(axes)):
# # # #         fig.delaxes(axes[j])

# # # #     plt.tight_layout(rect=[0, 0, 1, 0.96])

# # # #     return fig

# # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # # #     groupées par station.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # # #     """
# # # #     df = df.copy()

# # # #     # Assurez-vous que 'Datetime' est une colonne et de type datetime
# # # #     if isinstance(df.index, pd.DatetimeIndex):
# # # #         df = df.reset_index()

# # # #     df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')
# # # #     df = df.dropna(subset=['Datetime', 'Station'])

# # # #     if df.empty:
# # # #         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans daily_stats.")
# # # #         return pd.DataFrame()

# # # #     if 'Is_Daylight' not in df.columns:
# # # #         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
# # # #         df['Is_Daylight'] = (df['Datetime'].dt.hour >= 7) & (df['Datetime'].dt.hour <= 18)

# # # #     numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Datetime', 'Is_Daylight']]
    
# # # #     if not numerical_cols:
# # # #         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
# # # #         return pd.DataFrame()

# # # #     agg_funcs = {col: ['mean', 'min', 'max'] for col in numerical_cols}
    
# # # #     if 'Rain_mm' in numerical_cols:
# # # #         agg_funcs['Rain_mm'].append('sum')

# # # #     daily_stats_df = df.groupby(['Station', df['Datetime'].dt.date]).agg(agg_funcs)

# # # #     daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

# # # #     daily_stats_df = daily_stats_df.rename_axis(index={'Datetime': 'Date'})
# # # #     daily_stats_df = daily_stats_df.reset_index()

# # # #     return daily_stats_df


# # # # import pandas as pd
# # # # from pyproj import CRS, Transformer
# # # # import pytz
# # # # from astral.location import LocationInfo
# # # # from astral import sun
# # # # import numpy as np
# # # # import warnings
# # # # import os
# # # # import gdown # Importation du module gdown pour le téléchargement
# # # # import plotly.graph_objects as go # Importation pour gérer les objets Figure Plotly
# # # # import matplotlib.pyplot as plt # NOUVEAUX IMPORTS pour Matplotlib
# # # # import seaborn as sns
# # # # import traceback # Importation de traceback pour les messages d'erreur détaillés
# # # # import math # Ajouté pour math.ceil
# # # # from datetime import timedelta # Ajouté pour timedelta

# # # # # Importation des configurations et métadonnées depuis config.py
# # # # from config import METADATA_VARIABLES, PALETTE_DEFAUT, DATA_LIMITS

# # # # # Fonction utilitaire pour créer Rain_mm si nécessaire
# # # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
# # # #     Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
# # # #     """
# # # #     df_copy = df.copy()
# # # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
# # # #     elif 'Rain_01_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm']
# # # #     elif 'Rain_02_mm' in df_copy.columns:
# # # #         df_copy['Rain_mm'] = df_copy['Rain_02_mm']
# # # #     else:
# # # #         df_copy['Rain_mm'] = np.nan # Crée la colonne même si aucune source n'est disponible
# # # #         warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents pour créer 'Rain_mm'. 'Rain_mm' est rempli de NaN.")
# # # #     return df_copy


# # # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # # #     """
# # # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée.
# # # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     # Prioriser la colonne 'Date' si elle existe pour VEA_SISSILI ou si pas de colonnes Y/M/D/H/M
# # # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not any(col in df_copy.columns for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'])):
# # # #         try:
# # # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime pour le bassin {bassin}: {e}")
# # # #             df_copy['Datetime'] = pd.NaT
# # # #     else:
# # # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # # #         for col in date_cols:
# # # #             if col in df_copy.columns:
# # # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # # #         try:
# # # #             existing_date_components = [col for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'] if col in df_copy.columns]
            
# # # #             if not existing_date_components:
# # # #                 raise ValueError("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute) trouvée.")

# # # #             date_strings = df_copy.apply(
# # # #                 lambda row: f"{int(row.get('Year', 2000))}-"
# # # #                             f"{int(row.get('Month', 1)):02d}-"
# # # #                             f"{int(row.get('Day', 1)):02d} "
# # # #                             f"{int(row.get('Hour', 0)):02d}:"
# # # #                             f"{int(row.get('Minute', 0)):02d}",
# # # #                 axis=1
# # # #             )
# # # #             df_copy['Datetime'] = pd.to_datetime(date_strings, errors='coerce')
            
# # # #         except Exception as e:
# # # #             warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
# # # #             df_copy['Datetime'] = pd.NaT
            
# # # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # # #         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
# # # #              df_copy['Date'] = df_copy['Datetime'].dt.date
# # # #     else:
# # # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

# # # #     return df_copy

# # # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Effectue toutes les interpolations météorologiques en une seule passe.
# # # #     Cette fonction DOIT recevoir un DataFrame avec un DatetimeIndex.
# # # #     Il doit également contenir une colonne 'Station'.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame original avec les données interpolées,
# # # #                       la colonne 'Is_Daylight' calculée, la durée du jour, et un DatetimeIndex.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     # Assurez-vous que l'index est bien DatetimeIndex et nettoyez les NaT
# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame d'entrée pour l'interpolation DOIT avoir un DatetimeIndex.")
    
# # # #     initial_rows = len(df_processed)
# # # #     df_processed = df_processed[df_processed.index.notna()]
# # # #     if len(df_processed) == 0:
# # # #         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
# # # #     if initial_rows - len(df_processed) > 0:
# # # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")
    
# # # #     print(f"DEBUG (interpolation): Type de l'index du DataFrame initial: {type(df_processed.index)}")
# # # #     print(f"DEBUG (interpolation): Premières 5 valeurs de l'index après nettoyage des NaT: {df_processed.index[:5].tolist() if not df_processed.empty else 'DataFrame vide'}")

# # # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # # #         raise ValueError(
# # # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # # #         )

# # # #     if not df_gps['Station'].is_unique:
# # # #         print("Avertissement: La colonne 'Station' dans df_gps contient des noms de station dupliqués.")
# # # #         print("Ceci peut entraîner des comportements inattendus ou des stations non reconnues.")
# # # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # # #         print(f"Suppression de {len(df_gps) - len(df_gps_unique)} doublons dans df_gps (en gardant la première occurrence).")
# # # #     else:
# # # #         df_gps_unique = df_gps.copy()

# # # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # # #     for col in numerical_cols:
# # # #         if col in df_processed.columns:
# # # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # # #     df_processed_parts = []

# # # #     for station_name, group in df_processed.groupby('Station'):
# # # #         group_copy = group.copy()
# # # #         print(f"DEBUG (interpolation/groupby): Début du traitement du groupe '{station_name}'.")
        
# # # #         # Standardize group_copy.index to UTC first
# # # #         # This block ensures the index is UTC-aware before proceeding
# # # #         if group_copy.index.tz is None:
# # # #             group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # # #         elif group_copy.index.tz != pytz.utc: # If it's already tz-aware but not UTC, convert to UTC
# # # #             group_copy.index = group_copy.index.tz_convert('UTC')
# # # #         print(f"DEBUG (interpolation/groupby): Index Datetime pour '{station_name}' STANDARDISÉ à UTC. Dtype: {group_copy.index.dtype}")
        
# # # #         # S'assurer que l'index n'a pas de NaT après localisation
# # # #         group_copy = group_copy[group_copy.index.notna()]
# # # #         if group_copy.empty:
# # # #             warnings.warn(f"Le groupe '{station_name}' est vide après nettoyage de l'index Datetime. Il sera ignoré.")
# # # #             continue


# # # #         # --- Calculs Astral en utilisant le fuseau horaire local ---
# # # #         apply_fixed_daylight = True
# # # #         gps_data = gps_info_dict.get(station_name)
# # # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # # #             lat = gps_data['Lat']
# # # #             long = gps_data['Long']
# # # #             timezone_str = gps_data['Timezone']

# # # #             try:
# # # #                 local_tz = pytz.timezone(timezone_str)
# # # #                 # Create a local timezone-aware version of the index for Astral calculations
# # # #                 index_for_astral_local = group_copy.index.tz_convert(local_tz)

# # # #                 daily_sun_info = {}
# # # #                 # Get unique dates from the local-time index (these are timezone-aware Timestamps)
# # # #                 # Utilisation de .drop_duplicates() au lieu de .unique() pour garantir un objet Pandas
# # # #                 unique_dates_ts_local = index_for_astral_local.normalize().drop_duplicates()

# # # #                 # Ensure unique_dates is not empty before processing
# # # #                 if unique_dates_ts_local.empty: # Utilisez .empty pour les objets Pandas
# # # #                     raise ValueError("No unique dates found for Astral calculation.")
                
# # # #                 for ts_local_aware in unique_dates_ts_local: # Iterate over timezone-aware Timestamps
# # # #                     loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
                    
# # # #                     # Convert the timezone-aware Timestamp to a naive datetime.date object
# # # #                     # This is what Astral's sun.sun function expects for its 'date' parameter
# # # #                     # This conversion helps avoid potential re-localization warnings from Astral/pytz.
# # # #                     naive_date_for_astral = ts_local_aware.to_pydatetime().date()
                    
# # # #                     # Pass the NAIVE date object to Astral. Astral's observer handles the timezone internally.
# # # #                     s = sun.sun(loc.observer, date=naive_date_for_astral) 
# # # #                     daily_sun_info[naive_date_for_astral] = {
# # # #                         'sunrise': s['sunrise'],
# # # #                         'sunset': s['sunset']
# # # #                     }

# # # #                 # Explicitly create a list of naive dates for the DataFrame index
# # # #                 naive_unique_dates_for_index = [ts.date() for ts in unique_dates_ts_local] # Use .date() here as keys are naive
# # # #                 temp_df_sun_index = pd.Index(naive_unique_dates_for_index, name='Date_Local_Naive')
# # # #                 temp_df_sun = pd.DataFrame(index=temp_df_sun_index)
                
# # # #                 # NOUVEAUX DÉBOGAGES pour comprendre le type juste avant l'opération
# # # #                 print(f"DEBUG (astral_calc): unique_dates_ts_local type: {type(unique_dates_ts_local)}")
# # # #                 print(f"DEBUG (astral_calc): naive_unique_dates_for_index type: {type(naive_unique_dates_for_index)}")
# # # #                 print(f"DEBUG (astral_calc): temp_df_sun_index type: {type(temp_df_sun_index)}")
# # # #                 if not temp_df_sun.empty:
# # # #                     print(f"DEBUG (astral_calc): First element of temp_df_sun.index: {temp_df_sun.index[0]}")
# # # #                     print(f"DEBUG (astral_calc): Type of first element of temp_df_sun.index: {type(temp_df_sun.index[0])}")

# # # #                 # Correction: Utilisation de la compréhension de liste pour éviter le problème de .map()
# # # #                 temp_df_sun['sunrise_time_local'] = [daily_sun_info.get(date, {}).get('sunrise') for date in temp_df_sun.index]
# # # #                 temp_df_sun['sunset_time_local'] = [daily_sun_info.get(date, {}).get('sunset') for date in temp_df_sun.index]

# # # #                 # Merge with group_copy (which has UTC index)
# # # #                 # To merge, create a normalized local date column (naive) in group_copy
# # # #                 group_copy_reset = group_copy.reset_index()
# # # #                 group_copy_reset['Date_Local_Naive'] = group_copy_reset['Datetime'].dt.tz_convert(local_tz).dt.date

# # # #                 group_copy_reset = pd.merge(group_copy_reset, temp_df_sun, on='Date_Local_Naive', how='left')

# # # #                 # Convert merged local times back to UTC for comparison with original 'Datetime' (which is UTC)
# # # #                 group_copy_reset['sunrise_time_utc'] = group_copy_reset['sunrise_time_local'].dt.tz_convert('UTC')
# # # #                 group_copy_reset['sunset_time_utc'] = group_copy_reset['sunset_time_local'].dt.tz_convert('UTC')

# # # #                 group_copy_reset.loc[:, 'Is_Daylight'] = (group_copy_reset['Datetime'] >= group_copy_reset['sunrise_time_utc']) & \
# # # #                                                           (group_copy_reset['Datetime'] < group_copy_reset['sunset_time_utc'])

# # # #                 daylight_timedelta_local = group_copy_reset['sunset_time_local'] - group_copy_reset['sunrise_time_local']
                
# # # #                 # Correction ici pour formater la durée en HH:MM:SS
# # # #                 def format_timedelta_to_hms(td):
# # # #                     if pd.isna(td):
# # # #                         return np.nan
# # # #                     total_seconds = int(td.total_seconds())
# # # #                     hours = total_seconds // 3600
# # # #                     minutes = (total_seconds % 3600) // 60
# # # #                     seconds = total_seconds % 60
# # # #                     return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

# # # #                 group_copy_reset.loc[:, 'Daylight_Duration'] = daylight_timedelta_local.apply(format_timedelta_to_hms)


# # # #                 group_copy = group_copy_reset.set_index('Datetime')
# # # #                 group_copy = group_copy.drop(columns=['Date_Local_Naive', 'sunrise_time_local', 'sunset_time_local', 'sunrise_time_utc', 'sunset_time_utc'], errors='ignore')

# # # #                 print(f"Lever et coucher du soleil calculés pour {station_name}.")
# # # #                 apply_fixed_daylight = False

# # # #             except Exception as e:
# # # #                 print(f"Erreur lors du calcul du lever/coucher du soleil avec Astral pour {station_name}: {e}.")
# # # #                 traceback.print_exc() # Print full traceback for this specific error
# # # #                 warnings.warn(f"Calcul Astral impossible pour '{station_name}'. Utilisation de l'indicateur jour/nuit fixe.")
# # # #                 apply_fixed_daylight = True
# # # #         else:
# # # #             print(f"Avertissement: Coordonnées ou Fuseau horaire manquants/invalides pour le site '{station_name}' dans df_gps. Utilisation de l'indicateur jour/nuit fixe.")
# # # #             apply_fixed_daylight = True

# # # #         if apply_fixed_daylight:
# # # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00" # Règle fixe pour la durée
# # # #             print(f"Utilisation de l'indicateur jour/nuit fixe (7h-18h) pour {station_name}.")

# # # #         df_processed_parts.append(group_copy)

# # # #     if not df_processed_parts:
# # # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # # #     # Concaténation de toutes les parties. Puisque chaque partie a un DatetimeIndex,
# # # #     # nous pouvons concaténer directement sans ignore_index=True
# # # #     df_final = pd.concat(df_processed_parts)
# # # #     # Assurez-vous que l'index est trié et qu'il est bien nommé 'Datetime'
# # # #     df_final = pd.concat(df_processed_parts).sort_index()
# # # #     df_final.index.name = 'Datetime' 
# # # #     print(f"DEBUG (interpolation/concat): Index du DataFrame final après concaténation et tri: {type(df_final.index)}")
# # # #     print(f"DEBUG (interpolation/concat): Colonnes du DataFrame final après concaténation: {df_final.columns.tolist()}")


# # # #     # Nettoyage des colonnes temporaires (elles ne sont plus nécessaires ou n'existent plus)
# # # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date']
# # # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # # #     # Gestion intelligente de Rain_mm
# # # #     if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
# # # #         if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
# # # #             df_final = create_rain_mm(df_final)
# # # #             warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
# # # #         else:
# # # #             warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
# # # #             if 'Rain_mm' not in df_final.columns:
# # # #                 df_final['Rain_mm'] = np.nan


# # # #     # Interpolation standard et bornage pour les variables numériques
# # # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']

# # # #     for var in standard_vars:
# # # #         if var in df_final.columns:
# # # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # # #             if var in limits:
# # # #                 min_val = limits[var]['min']
# # # #                 max_val = limits[var]['max']
# # # #                 initial_nan_count = df_final[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_final.loc[df_final[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_final.loc[df_final[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_final[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # # #             print(f"DEBUG (interpolation/variable): Interpolation de '{var}'. Type de l'index de df_final: {type(df_final.index)}")
            
# # # #             # Interpolation seulement si l'index est bien DatetimeIndex
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/variable): L'index n'est pas un DatetimeIndex pour l'interpolation de '{var}'. Utilisation de la méthode 'linear'.")
# # # #                 df_final[var] = df_final[var].interpolate(method='linear', limit_direction='both')
# # # #             df_final[var] = df_final[var].bfill().ffill()

# # # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # # #     if 'Solar_R_W/m^2' in df_final.columns:
# # # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # # #         if 'Solar_R_W/m^2' in limits:
# # # #             min_val = limits['Solar_R_W/m^2']['min']
# # # #             max_val = limits['Solar_R_W/m^2']['max']
# # # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # # #         if 'Is_Daylight' in df_final.columns:
# # # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # # #             if 'Rain_mm' in df_final.columns:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # # #             else:
# # # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # # #             print(f"DEBUG (interpolation/solaire): Interpolation de 'Solar_R_W/m^2' (conditionnel). Type de l'index de df_final: {type(df_final.index)}")

# # # #             is_day = df_final['Is_Daylight']
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # # #             else:
# # # #                 print(f"Avertissement (interpolation/solaire): L'index n'est pas un DatetimeIndex pour l'interpolation de 'Solar_R_W/m^2'. Utilisation de la méthode 'linear'.")
# # # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both')

# # # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # # #         else:
# # # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()
# # # #             else:
# # # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both').bfill().ffill()

# # # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # # #     missing_after_interp = df_final.isna().sum()
# # # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # # #     if not columns_with_missing.empty:
# # # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # # #     else:
# # # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # # #     return df_final


# # # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # # #     de coordonnées UTM vers latitude/longitude WGS84.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # # #         raise ValueError(
# # # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # # #         )

# # # #     def convert_row(row):
# # # #         try:
# # # #             zone = int(row['zone'])
# # # #             hemisphere = str(row['hemisphere']).upper()
# # # #             is_northern = hemisphere == 'N'

# # # #             proj_utm = CRS.from_proj4(
# # # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # # #             )
# # # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # # #             return pd.Series({'Long': lon, 'Lat': lat})
# # # #         except Exception as e:
# # # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # # #     return df_copy

# # # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # # #     """
# # # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
# # # #     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

# # # #     Returns:
# # # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # # #     """
# # # #     print("Début de la préparation des données de coordonnées des stations...")
# # # #     data_dir = 'data'
# # # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # # #     files_info = [
# # # #         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
# # # #         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
# # # #         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
# # # #     ]

# # # #     loaded_dfs = []

# # # #     for file_info in files_info:
# # # #         output_file_path = os.path.join(data_dir, file_info['name'])
        
# # # #         if not os.path.exists(output_file_path):
# # # #             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
# # # #             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
# # # #             print(f"Téléchargement de {file_info['bassin']} terminé.")
# # # #         else:
# # # #             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
# # # #         loaded_dfs.append(pd.read_excel(output_file_path))

# # # #     vea_sissili_bassin = loaded_dfs[0]
# # # #     dano_bassin = loaded_dfs[1]
# # # #     dassari_bassin = loaded_dfs[2]

# # # #     # Prétraitement des DataFrames (votre code original)
# # # #     print("Début du prétraitement des données de stations...")
    
# # # #     # Vea Sissili
# # # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # # #     # Dassari
# # # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # # #     dano_bassin['zone'] = 30
# # # #     dano_bassin['hemisphere'] = 'N'
# # # #     dassari_bassin['zone'] = 31
# # # #     dassari_bassin['hemisphere'] = 'N'

# # # #     # Application de la fonction de conversion UTM vers GPS
# # # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # # #     # Ajout des fuseaux horaires
# # # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # # #     # Fusion de tous les bassins
# # # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # # #     # Renommer 'Name' en 'Station'
# # # #     bassins = bassins.rename(columns={'Name': 'Station'})

# # # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # # #     initial_rows = len(bassins)
# # # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # # #     if len(bassins) < initial_rows:
# # # #         print(f"Attention: {initial_rows - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # # #     # 5. Sauvegarde du DataFrame final en JSON
# # # #     output_json_path = os.path.join(data_dir, "station_coordinates.json")
# # # #     # Utiliser to_json avec orient='records' pour un format plus lisible et facile à charger
# # # #     bassins.to_json(output_json_path, orient='records', indent=4)
# # # #     print(f"\nPréparation des données terminée. Coordonnées des stations sauvegardées dans '{output_json_path}'.")
# # # #     print("Vous pouvez maintenant lancer votre application Flask.")

# # # #     return bassins # Retourner le DataFrame des données GPS


# # # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
# # # #     Conserve la première occurrence en cas de doublon.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame d'entrée.

# # # #     Returns:
# # # #         pd.DataFrame: Le DataFrame sans doublons.
# # # #     """
# # # #     if 'Station' in df.columns and 'Datetime' in df.columns:
# # # #         initial_rows = len(df)
# # # #         df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
# # # #         if len(df_cleaned) < initial_rows:
# # # #             warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
# # # #         return df_cleaned
# # # #     else:
# # # #         warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
# # # #         return df

# # # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # # #     """
# # # #     Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # # #         limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
# # # #     """
# # # #     df_processed = df.copy()

# # # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # # #         warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
# # # #         try:
# # # #             df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
# # # #             df_processed = df_processed[df_processed.index.notna()]
# # # #             if df_processed.empty:
# # # #                 raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
# # # #         except Exception as e:
# # # #             raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

# # # #     for var, vals in limits.items():
# # # #         if var in df_processed.columns:
# # # #             min_val = vals.get('min')
# # # #             max_val = vals.get('max')
# # # #             if min_val is not None or max_val is not None:
# # # #                 initial_nan_count = df_processed[var].isna().sum()
# # # #                 if min_val is not None:
# # # #                     df_processed.loc[df_processed[var] < min_val, var] = np.nan
# # # #                 if max_val is not None:
# # # #                     df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
# # # #                 new_nan_count = df_processed[var].isna().sum()
# # # #                 if new_nan_count > initial_nan_count:
# # # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
# # # #     return df_processed

# # # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
# # # #     Retourne l'objet Figure Plotly.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return go.Figure() # Retourne une figure vide si pas de données

# # # #     if periode == 'Journalière':
# # # #         resampled_df = filtered_df[variable].resample('D').mean()
# # # #     elif periode == 'Hebdomadaire':
# # # #         resampled_df = filtered_df[variable].resample('W').mean()
# # # #     elif periode == 'Mensuelle':
# # # #         resampled_df = filtered_df[variable].resample('M').mean()
# # # #     elif periode == 'Annuelle':
# # # #         resampled_df = filtered_df[variable].resample('Y').mean()
# # # #     else: # Données Brutes
# # # #         resampled_df = filtered_df[variable]

# # # #     resampled_df = resampled_df.dropna()

# # # #     if resampled_df.empty:
# # # #         return go.Figure() # Retourne une figure vide si pas de données après resample/dropna

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     color = colors.get(station, '#1f77b4')

# # # #     fig = go.Figure()
# # # #     fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                              mode='lines', name=f'{variable_meta["Nom"]} - {station}',
# # # #                              line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified",
# # # #         template="plotly_white" # Utiliser un template Plotly plus clair
# # # #     )
# # # #     return fig

# # # # def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
# # # #     """
# # # #     Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
# # # #     Retourne l'objet Figure Plotly.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

# # # #     fig = go.Figure()
    
# # # #     all_stations = df['Station'].unique()
# # # #     if len(all_stations) < 2:
# # # #         warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
# # # #         return go.Figure() # Retourne une figure vide si moins de 2 stations

# # # #     for station in all_stations:
# # # #         filtered_df = df[df['Station'] == station].copy()
# # # #         if filtered_df.empty:
# # # #             continue

# # # #         if periode == 'Journalière':
# # # #             resampled_df = filtered_df[variable].resample('D').mean()
# # # #         elif periode == 'Hebdomadaire':
# # # #             resampled_df = filtered_df[variable].resample('W').mean()
# # # #         elif periode == 'Mensuelle':
# # # #             resampled_df = filtered_df[variable].resample('M').mean()
# # # #         elif periode == 'Annuelle':
# # # #             resampled_df = filtered_df[variable].resample('Y').mean()
# # # #         else: # Données Brutes
# # # #             resampled_df = filtered_df[variable]

# # # #         resampled_df = resampled_df.dropna()
# # # #         if resampled_df.empty:
# # # #             continue
        
# # # #         color = colors.get(station, '#1f77b4')
# # # #         fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # # #                                  mode='lines', name=station,
# # # #                                  line=dict(color=color)))

# # # #     if not fig.data:
# # # #         return go.Figure() # Retourne une figure vide si aucune trace n'a été ajoutée

# # # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # # #     fig.update_layout(
# # # #         title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
# # # #         xaxis_title="Date",
# # # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # # #         hovermode="x unified",
# # # #         legend_title="Variables",
# # # #         template="plotly_white"
# # # #     )
# # # #     return fig


# # # # def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> go.Figure:
# # # #     """
# # # #     Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
# # # #     Retourne l'objet Figure Plotly.
# # # #     """
# # # #     if not isinstance(df.index, pd.DatetimeIndex):
# # # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

# # # #     filtered_df = df[df['Station'] == station].copy()
# # # #     if filtered_df.empty:
# # # #         return go.Figure() # Retourne une figure vide si pas de données

# # # #     numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight', 'Daylight_Duration']]

# # # #     if not numerical_vars:
# # # #         warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
# # # #         return go.Figure() # Retourne une figure vide si pas de variables numériques

# # # #     normalized_df = filtered_df[numerical_vars].copy()
# # # #     for col in normalized_df.columns:
# # # #         min_val = normalized_df[col].min()
# # # #         max_val = normalized_df[col].max()
# # # #         if max_val != min_val:
# # # #             normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
# # # #         else:
# # # #             normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

# # # #     normalized_df = normalized_df.dropna(how='all')

# # # #     if normalized_df.empty:
# # # #         return go.Figure() # Retourne une figure vide si pas de données après normalisation/dropna
    
# # # #     fig = go.Figure()
# # # #     for var in normalized_df.columns:
# # # #         var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
# # # #         color = colors.get(var, None)

# # # #         fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
# # # #                                  mode='lines', name=var_meta['Nom'],
# # # #                                  line=dict(color=color)))

# # # #     fig.update_layout(
# # # #         title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
# # # #         xaxis_title="Date",
# # # #         yaxis_title="Valeur Normalisée (0-1)",
# # # #         hovermode="x unified",
# # # #         legend_title="Variables",
# # # #         template="plotly_white"
# # # #     )
# # # #     return fig

# # # # def calculate_daily_summary_table(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # # #     groupées par station. Cette fonction renvoie un DataFrame de statistiques, non un graphique.
# # # #     C'est la fonction qui remplace l'ancienne 'daily_stats'.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # # #     """
# # # #     df_copy = df.copy()

# # # #     # Assurez-vous que 'Datetime' est une colonne et de type datetime
# # # #     if isinstance(df_copy.index, pd.DatetimeIndex):
# # # #         df_copy = df_copy.reset_index()

# # # #     df_copy['Datetime'] = pd.to_datetime(df_copy['Datetime'], errors='coerce')
# # # #     df_copy = df_copy.dropna(subset=['Datetime', 'Station'])

# # # #     if df_copy.empty:
# # # #         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans calculate_daily_summary_table.")
# # # #         return pd.DataFrame()

# # # #     if 'Is_Daylight' not in df_copy.columns:
# # # #         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
# # # #         df_copy['Is_Daylight'] = (df_copy['Datetime'].dt.hour >= 7) & (df_copy['Datetime'].dt.hour <= 18)

# # # #     numerical_cols = [col for col in df_copy.columns if pd.api.types.is_numeric_dtype(df_copy[col]) and col not in ['Station', 'Datetime', 'Is_Daylight', 'Daylight_Duration']]
    
# # # #     if not numerical_cols:
# # # #         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
# # # #         return pd.DataFrame()

# # # #     # Calcul des statistiques de base par jour
# # # #     daily_aggregated_df = df_copy.groupby(['Station', df_copy['Datetime'].dt.date]).agg({
# # # #         col: ['mean', 'min', 'max'] for col in numerical_cols if METADATA_VARIABLES.get(col, {}).get('is_rain') == False
# # # #     })

# # # #     # Renommage des colonnes agrégées pour les non-pluies
# # # #     daily_aggregated_df.columns = ['_'.join(col).strip() for col in daily_aggregated_df.columns.values]


# # # #     # Traitement spécifique pour la pluie (Rain_mm)
# # # #     if 'Rain_mm' in df_copy.columns and METADATA_VARIABLES.get('Rain_mm', {}).get('is_rain'):
# # # #         df_daily_rain = df_copy.groupby(['Station', df_copy['Datetime'].dt.date])['Rain_mm'].sum().reset_index()
# # # #         df_daily_rain = df_daily_rain.rename(columns={'Rain_mm': 'Rain_mm_sum'})

# # # #         # Fusionner les statistiques de pluie avec les autres
# # # #         if not daily_aggregated_df.empty:
# # # #             daily_aggregated_df = daily_aggregated_df.reset_index()
# # # #             daily_stats_df = pd.merge(daily_aggregated_df, df_daily_rain, on=['Station', 'Datetime'], how='left')
# # # #             daily_stats_df = daily_stats_df.rename(columns={'Datetime': 'Date'})
# # # #         else:
# # # #             daily_stats_df = df_daily_rain.rename(columns={'Datetime': 'Date'})
# # # #     else:
# # # #         daily_stats_df = daily_aggregated_df.reset_index().rename(columns={'Datetime': 'Date'})


# # # #     # Calcul des statistiques de saison et de sécheresse pour la pluie
# # # #     if 'Rain_mm' in df_copy.columns and METADATA_VARIABLES.get('Rain_mm', {}).get('is_rain'):
# # # #         df_daily_rain_raw = df_copy.groupby(['Station', pd.Grouper(key='Datetime', freq='D')])['Rain_mm'].sum().reset_index()
        
# # # #         RAIN_SEASON_GAP_THRESHOLD = pd.Timedelta(days=60)
# # # #         season_stats = []
# # # #         for station_name, station_df_rain in df_daily_rain_raw.groupby('Station'):
# # # #             station_df_rain = station_df_rain.set_index('Datetime').sort_index()
# # # #             rain_events = station_df_rain[station_df_rain['Rain_mm'] > 0].index

# # # #             if rain_events.empty:
# # # #                 season_stats.append({'Station': station_name, 'Moyenne_Saison_Pluvieuse': np.nan, 'Debut_Saison_Pluvieuse': pd.NaT, 'Fin_Saison_Pluvieuse': pd.NaT, 'Duree_Saison_Pluvieuse_Jours': np.nan})
# # # #                 continue
            
# # # #             block_ids = (rain_events.to_series().diff() > RAIN_SEASON_GAP_THRESHOLD).cumsum()
# # # #             valid_blocks = {}
# # # #             for block_id, rain_dates_in_block in rain_events.to_series().groupby(block_ids):
# # # #                 if not rain_dates_in_block.empty:
# # # #                     block_start = rain_dates_in_block.min()
# # # #                     block_end = rain_dates_in_block.max()
# # # #                     full_block_df = station_df_rain.loc[block_start:block_end]
# # # #                     valid_blocks[block_id] = full_block_df

# # # #             if not valid_blocks:
# # # #                 season_stats.append({'Station': station_name, 'Moyenne_Saison_Pluvieuse': np.nan, 'Debut_Saison_Pluvieuse': pd.NaT, 'Fin_Saison_Pluvieuse': pd.NaT, 'Duree_Saison_Pluvieuse_Jours': np.nan})
# # # #                 continue

# # # #             main_block_id = max(valid_blocks, key=lambda k: (valid_blocks[k].index.max() - valid_blocks[k].index.min()).days)
# # # #             main_season_df = valid_blocks[main_block_id]

# # # #             debut_saison = main_season_df.index.min()
# # # #             fin_saison = main_season_df.index.max()
# # # #             total_days = (fin_saison - debut_saison).days + 1
# # # #             moyenne_saison = main_season_df['Rain_mm'].sum() / total_days if total_days > 0 else 0

# # # #             season_stats.append({
# # # #                 'Station': station_name,
# # # #                 'Moyenne_Saison_Pluvieuse': moyenne_saison,
# # # #                 'Debut_Saison_Pluvieuse': debut_saison,
# # # #                 'Fin_Saison_Pluvieuse': fin_saison,
# # # #                 'Duree_Saison_Pluvieuse_Jours': total_days
# # # #             })
# # # #         df_season_stats = pd.DataFrame(season_stats)
        
# # # #         # Merge season stats into main daily_stats_df
# # # #         if not df_season_stats.empty:
# # # #             daily_stats_df = pd.merge(daily_stats_df, df_season_stats, on='Station', how='left')


# # # #     # Calcul des statistiques globales (Max, Min, Moyenne, Médiane, etc.) par station
# # # #     # pour les variables numériques, à inclure dans le tableau récapitulatif.
# # # #     final_stats_per_station = pd.DataFrame()
# # # #     for station_name in df_copy['Station'].unique():
# # # #         station_df = df_copy[df_copy['Station'] == station_name].copy()
# # # #         station_summary = {'Station': station_name}

# # # #         for var in numerical_cols:
# # # #             if var in station_df.columns and pd.api.types.is_numeric_dtype(station_df[var]):
# # # #                 # Filtrage pour la radiation solaire (seulement pendant le jour)
# # # #                 if var == 'Solar_R_W/m^2':
# # # #                     var_data = station_df.loc[station_df['Is_Daylight'], var].dropna()
# # # #                 else:
# # # #                     var_data = station_df[var].dropna()
                
# # # #                 if not var_data.empty:
# # # #                     station_summary[f'{var}_Maximum'] = var_data.max()
# # # #                     station_summary[f'{var}_Minimum'] = var_data.min()
# # # #                     station_summary[f'{var}_Moyenne'] = var_data.mean()
# # # #                     station_summary[f'{var}_Mediane'] = var_data.median()
                    
# # # #                     # Pour Rain_mm uniquement
# # # #                     if var == 'Rain_mm':
# # # #                         station_summary[f'{var}_Cumul_Annuel'] = station_df['Rain_mm'].sum()
# # # #                         # Moyenne des jours pluvieux (seulement les jours où il a plu)
# # # #                         rainy_days_data = station_df[station_df['Rain_mm'] > 0]['Rain_mm'].dropna()
# # # #                         station_summary[f'{var}_Moyenne_Jours_Pluvieux'] = rainy_days_data.mean() if not rainy_days_data.empty else np.nan

# # # #                         # Ajouter la durée de la saison pluvieuse et de sécheresse du df_season_stats
# # # #                         if 'Duree_Saison_Pluvieuse_Jours' in daily_stats_df.columns:
# # # #                             s_data = daily_stats_df[daily_stats_df['Station'] == station_name]
# # # #                             if not s_data.empty:
# # # #                                 station_summary[f'{var}_Duree_Saison_Pluvieuse_Jours'] = s_data['Duree_Saison_Pluvieuse_Jours'].iloc[0]
# # # #                                 # Placeholder pour sécheresse si non calculé ailleurs
# # # #                                 station_summary[f'{var}_Duree_Secheresse_Definie_Jours'] = np.nan # Vous devrez calculer ceci plus tard

# # # #         final_stats_per_station = pd.concat([final_stats_per_station, pd.DataFrame([station_summary])], ignore_index=True)
        
# # # #     return final_stats_per_station # Retourne le DataFrame de statistiques agrégées par station


# # # # def generate_variable_summary_plots_for_web(df: pd.DataFrame, station: str, variable: str, metadata: dict, palette: dict) -> plt.Figure:
# # # #     """
# # # #     Génère un graphique Matplotlib/Seaborn pour les statistiques agrégées d'une variable spécifique
# # # #     pour une station donnée, en utilisant la logique fournie par l'utilisateur pour 'daily_stats'.

# # # #     Args:
# # # #         df (pd.DataFrame): Le DataFrame global de données traitées avec DatetimeIndex et colonne 'Station'.
# # # #         station (str): Le nom de la station à visualiser.
# # # #         variable (str): La variable à visualiser (e.g., 'Air_Temp_Deg_C', 'Rain_mm').
# # # #         metadata (dict): Dictionnaire de métadonnées pour les variables (Nom, Unite, is_rain).
# # # #         palette (dict): Dictionnaire de couleurs pour les différentes métriques statistiques.

# # # #     Returns:
# # # #         plt.Figure: Un objet Figure Matplotlib contenant tous les sous-graphiques pour la variable sélectionnée.
# # # #     """
# # # #     df_station = df[df['Station'] == station].copy()

# # # #     if df_station.empty:
# # # #         fig, ax = plt.subplots(figsize=(10, 6))
# # # #         ax.text(0.5, 0.5, f"Aucune donnée pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# # # #         ax.axis('off')
# # # #         return fig

# # # #     # S'assurer que Datetime est un index datetime correct pour les données de la station
# # # #     if isinstance(df_station.index, pd.DatetimeIndex):
# # # #         df_station = df_station.reset_index() # Réinitialise l'index pour un accès facile aux colonnes
    
# # # #     df_station['Datetime'] = pd.to_datetime(df_station['Datetime'], errors='coerce')
# # # #     df_station = df_station.dropna(subset=['Datetime', 'Station'])
# # # #     df_station = df_station.set_index('Datetime').sort_index() # Remet l'index pour les opérations de séries temporelles

# # # #     if df_station.empty:
# # # #         fig, ax = plt.subplots(figsize=(10, 6))
# # # #         ax.text(0.5, 0.5, f"DataFrame vide après nettoyage des dates pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# # # #         ax.axis('off')
# # # #         return fig

# # # #     # Assurez-vous que 'Is_Daylight' existe (généralement géré par interpolation, mais au cas où)
# # # #     if 'Is_Daylight' not in df_station.columns:
# # # #         df_station['Is_Daylight'] = (df_station.index.hour >= 7) & (df_station.index.hour <= 18)


# # # #     var_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})

# # # #     # Dictionnaire pour stocker les statistiques pour la variable sélectionnée
# # # #     stats_for_plot = {}
# # # #     metrics_to_plot = []
    
# # # #     # --- LOGIQUE SPÉCIFIQUE À RAIN_MM (transférée de la fonction daily_stats de l'utilisateur) ---
# # # #     if var_meta.get('is_rain', False) and variable == 'Rain_mm':
# # # #         df_daily_rain = df_station.groupby(pd.Grouper(freq='D'))['Rain_mm'].sum().reset_index()
# # # #         df_daily_rain = df_daily_rain.rename(columns={'Rain_mm': 'Rain_mm_sum'})
# # # #         df_daily_rain['Datetime'] = pd.to_datetime(df_daily_rain['Datetime'])
# # # #         df_daily_rain = df_daily_rain.set_index('Datetime').sort_index()

# # # #         RAIN_SEASON_GAP_THRESHOLD = pd.Timedelta(days=60)
# # # #         rain_events = df_daily_rain[df_daily_rain['Rain_mm_sum'] > 0].index

# # # #         s_moyenne_saison = np.nan
# # # #         s_duree_saison = np.nan
# # # #         s_debut_saison = pd.NaT
# # # #         s_fin_saison = pd.NaT

# # # #         if not rain_events.empty:
# # # #             block_ids = (rain_events.to_series().diff() > RAIN_SEASON_GAP_THRESHOLD).cumsum()
# # # #             valid_blocks = {}
# # # #             for block_id, rain_dates_in_block in rain_events.to_series().groupby(block_ids):
# # # #                 if not rain_dates_in_block.empty:
# # # #                     block_start = rain_dates_in_block.min()
# # # #                     block_end = rain_dates_in_block.max()
# # # #                     full_block_df = df_daily_rain.loc[block_start:block_end]
# # # #                     valid_blocks[block_id] = full_block_df

# # # #             if valid_blocks:
# # # #                 main_block_id = max(valid_blocks, key=lambda k: (valid_blocks[k].index.max() - valid_blocks[k].index.min()).days)
# # # #                 main_season_df = valid_blocks[main_block_id]

# # # #                 s_debut_saison = main_season_df.index.min()
# # # #                 s_fin_saison = main_season_df.index.max()
# # # #                 total_days_season = (s_fin_saison - s_debut_saison).days + 1
# # # #                 s_moyenne_saison = main_season_df['Rain_mm_sum'].sum() / total_days_season if total_days_season > 0 else 0
# # # #                 s_duree_saison = total_days_season

# # # #         # Logique de détection de la sécheresse
# # # #         longest_dry_spell = np.nan
# # # #         debut_secheresse = pd.NaT
# # # #         fin_secheresse = pd.NaT

# # # #         full_daily_series_rain = df_daily_rain['Rain_mm_sum'].resample('D').sum().fillna(0)
# # # #         rainy_days_index = full_daily_series_rain[full_daily_series_rain > 0].index

# # # #         if not rainy_days_index.empty and pd.notna(s_moyenne_saison) and s_moyenne_saison > 0:
# # # #             temp_dry_spells = []
# # # #             for i in range(1, len(rainy_days_index)):
# # # #                 prev_rain_date = rainy_days_index[i-1]
# # # #                 current_rain_date = rainy_days_index[i]
# # # #                 dry_days_between_rains = (current_rain_date - prev_rain_date).days - 1

# # # #                 if dry_days_between_rains > 0:
# # # #                     rain_prev_day = full_daily_series_rain.loc[prev_rain_date]
# # # #                     temp_debut = pd.NaT
# # # #                     temp_duree = 0
# # # #                     for j in range(1, dry_days_between_rains + 1):
# # # #                         current_dry_date = prev_rain_date + timedelta(days=j)
# # # #                         current_ratio = rain_prev_day / j
# # # #                         if current_ratio < s_moyenne_saison: # Condition pour la sécheresse définie
# # # #                             temp_debut = current_dry_date
# # # #                             temp_duree = (current_rain_date - temp_debut).days
# # # #                             break
# # # #                     if pd.notna(temp_debut) and temp_duree > 0:
# # # #                         temp_dry_spells.append({
# # # #                             'Duree': temp_duree,
# # # #                             'Debut': temp_debut,
# # # #                             'Fin': current_rain_date - timedelta(days=1)
# # # #                         })
            
# # # #             if temp_dry_spells:
# # # #                 df_temp_dry = pd.DataFrame(temp_dry_spells)
# # # #                 idx_max_dry = df_temp_dry['Duree'].idxmax()
# # # #                 longest_dry_spell = df_temp_dry.loc[idx_max_dry, 'Duree']
# # # #                 debut_secheresse = df_temp_dry.loc[idx_max_dry, 'Debut']
# # # #                 fin_secheresse = df_temp_dry.loc[idx_max_dry, 'Fin']

# # # #         # Statistiques pour Rain_mm (tirées de la vue consolidée de daily_stats de l'utilisateur)
# # # #         rain_data_for_stats = df_station[variable].dropna()
        
# # # #         stats_for_plot['Maximum'] = rain_data_for_stats.max() if not rain_data_for_stats.empty else np.nan
# # # #         stats_for_plot['Minimum'] = rain_data_for_stats.min() if not rain_data_for_stats.empty else np.nan
# # # #         stats_for_plot['Mediane'] = rain_data_for_stats.median() if not rain_data_for_stats.empty else np.nan
# # # #         stats_for_plot['Cumul_Annuel'] = df_station[variable].sum()
        
# # # #         rainy_days_data = df_station[df_station[variable] > 0][variable].dropna()
# # # #         stats_for_plot['Moyenne_Jours_Pluvieux'] = rainy_days_data.mean() if not rainy_days_data.empty else np.nan
        
# # # #         stats_for_plot['Moyenne_Saison_Pluvieuse'] = s_moyenne_saison
# # # #         stats_for_plot['Duree_Saison_Pluvieuse_Jours'] = s_duree_saison
# # # #         stats_for_plot['Debut_Saison_Pluvieuse'] = s_debut_saison
# # # #         stats_for_plot['Fin_Saison_Pluvieuse'] = s_fin_saison
        
# # # #         stats_for_plot['Duree_Secheresse_Definie_Jours'] = longest_dry_spell
# # # #         stats_for_plot['Debut_Secheresse_Definie'] = debut_secheresse
# # # #         stats_for_plot['Fin_Secheresse_Definie'] = fin_secheresse

# # # #         # Récupérer les dates pour max/min pour Rain_mm
# # # #         max_date_idx = df_station[variable].idxmax() if not df_station[variable].empty else pd.NaT
# # # #         min_date_idx = df_station[variable].idxmin() if not df_station[variable].empty else pd.NaT
# # # #         stats_for_plot['Date_Maximum'] = max_date_idx if pd.notna(max_date_idx) else pd.NaT
# # # #         stats_for_plot['Date_Minimum'] = min_date_idx if pd.notna(min_date_idx) else pd.NaT
        
# # # #         metrics_to_plot = [
# # # #             'Maximum', 'Minimum', 'Cumul_Annuel', 'Mediane',
# # # #             'Moyenne_Jours_Pluvieux', 'Moyenne_Saison_Pluvieuse',
# # # #             'Duree_Saison_Pluvieuse_Jours', 'Duree_Secheresse_Definie_Jours'
# # # #         ]
# # # #         nrows, ncols = 4, 2
# # # #         figsize = (18, 16)
        
# # # #     # --- LOGIQUE POUR LES AUTRES VARIABLES (transférée de la fonction daily_stats de l'utilisateur) ---
# # # #     else:
# # # #         current_var_data = df_station[variable].dropna()
# # # #         if variable == 'Solar_R_W/m^2':
# # # #             current_var_data = df_station.loc[df_station['Is_Daylight'], variable].dropna()

# # # #         if current_var_data.empty:
# # # #             fig, ax = plt.subplots(figsize=(10, 6))
# # # #             ax.text(0.5, 0.5, f"Aucune donnée valide pour la variable {var_meta['Nom']} à {station}.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# # # #             ax.axis('off')
# # # #             return fig

# # # #         # Statistiques pour les autres variables (de daily_stats de l'utilisateur)
# # # #         stats_for_plot['Maximum'] = current_var_data.max()
# # # #         stats_for_plot['Minimum'] = current_var_data.min()
# # # #         stats_for_plot['Mediane'] = current_var_data.median()
# # # #         stats_for_plot['Moyenne'] = current_var_data.mean()

# # # #         # Récupérer les dates pour max/min
# # # #         max_idx = current_var_data.idxmax() if not current_var_data.empty else pd.NaT
# # # #         min_idx = current_var_data.idxmin() if not current_var_data.empty else pd.NaT

# # # #         stats_for_plot['Date_Maximum'] = max_idx if pd.notna(max_idx) else pd.NaT
# # # #         stats_for_plot['Date_Minimum'] = min_idx if pd.notna(min_idx) else pd.NaT

# # # #         metrics_to_plot = ['Maximum', 'Minimum', 'Moyenne', 'Mediane']
# # # #         nrows, ncols = 2, 2
# # # #         figsize = (18, 12)

# # # #     # Gérer le cas où stats_for_plot pourrait être vide (ex: toutes les données sont NaN)
# # # #     if not stats_for_plot:
# # # #         fig, ax = plt.subplots(figsize=(10, 6))
# # # #         ax.text(0.5, 0.5, f"Impossible de calculer des statistiques pour la variable '{variable}' à la station '{station}' (données manquantes ou non numériques).", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# # # #         ax.axis('off')
# # # #         return fig

# # # #     # --- PARTIE COMMUNE DE TRACÉ (adaptée de la fonction daily_stats de l'utilisateur) ---
# # # #     fig, axes = plt.subplots(nrows, ncols, figsize=figsize)
# # # #     plt.subplots_adjust(hspace=0.6, wspace=0.4) # Ajustement de l'espacement pour un meilleur ajustement
# # # #     axes = axes.flatten()

# # # #     fig.suptitle(f'Statistiques de {var_meta["Nom"]} pour la station {station}', fontsize=16, y=0.98)

# # # #     for i, metric in enumerate(metrics_to_plot):
# # # #         ax = axes[i]
# # # #         value = stats_for_plot.get(metric)
# # # #         if pd.isna(value):
# # # #             ax.text(0.5, 0.5, "Données non disponibles", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=12, color='gray')
# # # #             ax.axis('off')
# # # #             continue

# # # #         color = palette.get(metric.replace(' ', '_'), '#cccccc') # Utilisation de la `palette` passée en argument
        
# # # #         # Créer un DataFrame factice pour sns.barplot, car il attend un DataFrame
# # # #         plot_data_bar = pd.DataFrame({'Metric': [metric.replace('_', ' ')], 'Value': [value]})
# # # #         sns.barplot(x='Metric', y='Value', data=plot_data_bar, ax=ax, color=color, edgecolor='none')

# # # #         # Ajouter les annotations
# # # #         text = ""
# # # #         if metric in ['Duree_Saison_Pluvieuse_Jours', 'Duree_Secheresse_Definie_Jours']:
# # # #             start_date_key = f'Debut_{metric.replace("Jours", "")}'
# # # #             end_date_key = f'Fin_{metric.replace("Jours", "")}'
# # # #             start_date = stats_for_plot.get(start_date_key)
# # # #             end_date = stats_for_plot.get(end_date_key)
# # # #             date_info = ""
# # # #             if pd.notna(start_date) and pd.notna(end_date):
# # # #                 date_info = f"\ndu {start_date.strftime('%d/%m/%Y')} au {end_date.strftime('%d/%m/%Y')}"
# # # #             text = f"{int(value)} j{date_info}"
# # # #         elif metric in ['Maximum', 'Minimum', 'Cumul_Annuel', 'Moyenne_Jours_Pluvieux', 'Moyenne_Saison_Pluvieuse', 'Mediane', 'Moyenne']:
# # # #             unit = var_meta['Unite']
# # # #             date_str = ''
# # # #             # Vérifier les clés de date pour max/min si elles existent dans stats_for_plot
# # # #             if (metric == 'Maximum' and 'Date_Maximum' in stats_for_plot and pd.notna(stats_for_plot['Date_Maximum'])):
# # # #                 date_str = f"\n({stats_for_plot['Date_Maximum'].strftime('%d/%m/%Y')})"
# # # #             elif (metric == 'Minimum' and 'Date_Minimum' in stats_for_plot and pd.notna(stats_for_plot['Date_Minimum'])):
# # # #                 date_str = f"\n({stats_for_plot['Date_Minimum'].strftime('%d/%m/%Y')})"
            
# # # #             text = f"{value:.1f} {unit}{date_str}"
# # # #         else:
# # # #             text = f"{value:.1f} {var_meta['Unite']}"

# # # #         # Positionne le texte sur le graphique.
# # # #         ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
# # # #                 text, ha='center', va='bottom', fontsize=9, color='black',
# # # #                 bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))
        
# # # #         ax.set_title(f"{var_meta['Nom']} {metric.replace('_', ' ')}", fontsize=11)
# # # #         ax.set_xlabel("")
# # # #         ax.set_ylabel(f"Valeur ({var_meta['Unite']})", fontsize=10)
# # # #         ax.tick_params(axis='x', rotation=0)
# # # #         ax.set_xticklabels([])

# # # #     # Si moins de graphiques que de sous-graphiques, désactiver les axes inutilisés
# # # #     for j in range(i + 1, len(axes)):
# # # #         fig.delaxes(axes[j])

# # # #     plt.tight_layout(rect=[0, 0, 1, 0.96])

# # # #     return fig


# # # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # # #     """
# # # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # # #     groupées par station.

# # # #     Args:
# # # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # # #     Returns:
# # # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # # #     """
# # # #     df = df.copy()

# # # #     # Assurez-vous que 'Datetime' est une colonne et de type datetime
# # # #     if isinstance(df.index, pd.DatetimeIndex):
# # # #         df = df.reset_index()

# # # #     df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')
# # # #     df = df.dropna(subset=['Datetime', 'Station'])

# # # #     if df.empty:
# # # #         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans daily_stats.")
# # # #         return pd.DataFrame()

# # # #     if 'Is_Daylight' not in df.columns:
# # # #         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
# # # #         df['Is_Daylight'] = (df['Datetime'].dt.hour >= 7) & (df['Datetime'].dt.hour <= 18)

# # # #     numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Datetime', 'Is_Daylight']]
    
# # # #     if not numerical_cols:
# # # #         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
# # # #         return pd.DataFrame()

# # # #     agg_funcs = {col: ['mean', 'min', 'max'] for col in numerical_cols}
    
# # # #     if 'Rain_mm' in numerical_cols:
# # # #         agg_funcs['Rain_mm'].append('sum')

# # # #     daily_stats_df = df.groupby(['Station', df['Datetime'].dt.date]).agg(agg_funcs)

# # # #     daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

# # # #     daily_stats_df = daily_stats_df.rename_axis(index={'Datetime': 'Date'})
# # # #     daily_stats_df = daily_stats_df.reset_index()

# # # #     return daily_stats_df

# # # import pandas as pd
# # # from pyproj import CRS, Transformer
# # # import pytz
# # # from astral.location import LocationInfo
# # # from astral import sun
# # # import numpy as np
# # # import warnings
# # # import os
# # # import gdown # Importation du module gdown pour le téléchargement
# # # import plotly.graph_objects as go # Importation pour gérer les objets Figure Plotly
# # # import matplotlib.pyplot as plt # NOUVEAUX IMPORTS pour Matplotlib
# # # import seaborn as sns
# # # import traceback # Importation de traceback pour les messages d'erreur détaillés
# # # import math # Ajouté pour math.ceil
# # # from datetime import timedelta # Ajouté pour timedelta

# # # # Importation des configurations et métadonnées depuis config.py
# # # from config import METADATA_VARIABLES, PALETTE_DEFAUT, DATA_LIMITS

# # # # Fonction utilitaire pour créer Rain_mm si nécessaire
# # # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# # #     """
# # #     Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
# # #     Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
# # #     Si aucune des colonnes sources n'est présente, 'Rain_mm' n'est pas créée.
# # #     """
# # #     df_copy = df.copy()
# # #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
# # #         warnings.warn("Colonne Rain_mm créée en fusionnant Rain_01_mm et Rain_02_mm.")
# # #     elif 'Rain_01_mm' in df_copy.columns:
# # #         df_copy['Rain_mm'] = df_copy['Rain_01_mm']
# # #         warnings.warn("Colonne Rain_mm créée à partir de Rain_01_mm.")
# # #     elif 'Rain_02_mm' in df_copy.columns:
# # #         df_copy['Rain_mm'] = df_copy['Rain_02_mm']
# # #         warnings.warn("Colonne Rain_mm créée à partir de Rain_02_mm.")
# # #     else:
# # #         # Si ni 'Rain_01_mm' ni 'Rain_02_mm' n'existe, ne pas créer 'Rain_mm'
# # #         warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents. La colonne 'Rain_mm' ne sera pas créée.")
# # #     return df_copy


# # # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# # #     """
# # #     Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
# # #     ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

# # #     Args:
# # #         df (pd.DataFrame): DataFrame d'entrée.
# # #         bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
# # #                                 Utilisé pour la logique de conversion de la colonne 'Date'.

# # #     Returns:
# # #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# # #     """
# # #     df_copy = df.copy()

# # #     # Prioriser la colonne 'Date' si elle existe pour VEA_SISSILI ou si pas de colonnes Y/M/D/H/M
# # #     if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not any(col in df_copy.columns for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'])):
# # #         try:
# # #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# # #         except Exception as e:
# # #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime pour le bassin {bassin}: {e}")
# # #             df_copy['Datetime'] = pd.NaT
# # #     else:
# # #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
# # #         for col in date_cols:
# # #             if col in df_copy.columns:
# # #                 df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

# # #         try:
# # #             existing_date_components = [col for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'] if col in df_copy.columns]
            
# # #             if not existing_date_components:
# # #                 raise ValueError("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute) trouvée.")

# # #             date_strings = df_copy.apply(
# # #                 lambda row: f"{int(row.get('Year', 2000))}-"
# # #                             f"{int(row.get('Month', 1)):02d}-"
# # #                             f"{int(row.get('Day', 1)):02d} "
# # #                             f"{int(row.get('Hour', 0)):02d}:"
# # #                             f"{int(row.get('Minute', 0)):02d}",
# # #                 axis=1
# # #             )
# # #             df_copy['Datetime'] = pd.to_datetime(date_strings, errors='coerce')
            
# # #         except Exception as e:
# # #             warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
# # #             df_copy['Datetime'] = pd.NaT
            
# # #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# # #         df_copy['Year'] = df_copy['Datetime'].dt.year
# # #         df_copy['Month'] = df_copy['Datetime'].dt.month
# # #         df_copy['Day'] = df_copy['Datetime'].dt.day
# # #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# # #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# # #         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
# # #              df_copy['Date'] = df_copy['Datetime'].dt.date
# # #     else:
# # #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

# # #     return df_copy

# # # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# # #     """
# # #     Effectue toutes les interpolations météorologiques en une seule passe.
# # #     Cette fonction DOIT recevoir un DataFrame avec un DatetimeIndex.
# # #     Il doit également contenir une colonne 'Station'.

# # #     Args:
# # #         df (pd.DataFrame): Le DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# # #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# # #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# # #     Returns:
# # #         pd.DataFrame: Le DataFrame original avec les données interpolées,
# # #                       la colonne 'Is_Daylight' calculée, la durée du jour, et un DatetimeIndex.
# # #     """
# # #     df_processed = df.copy()

# # #     # Assurez-vous que l'index est bien DatetimeIndex et nettoyez les NaT
# # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # #         raise TypeError("Le DataFrame d'entrée pour l'interpolation DOIT avoir un DatetimeIndex.")
    
# # #     initial_rows = len(df_processed)
# # #     df_processed = df_processed[df_processed.index.notna()]
# # #     if len(df_processed) == 0:
# # #         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
# # #     if initial_rows - len(df_processed) > 0:
# # #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")
    
# # #     print(f"DEBUG (interpolation): Type de l'index du DataFrame initial: {type(df_processed.index)}")
# # #     print(f"DEBUG (interpolation): Premières 5 valeurs de l'index après nettoyage des NaT: {df_processed.index[:5].tolist() if not df_processed.empty else 'DataFrame vide'}")

# # #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# # #     if not all(col in df_gps.columns for col in required_gps_cols):
# # #         raise ValueError(
# # #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# # #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# # #         )

# # #     if not df_gps['Station'].is_unique:
# # #         print("Avertissement: La colonne 'Station' dans df_gps contient des noms de station dupliqués.")
# # #         print("Ceci peut entraîner des comportements inattendus ou des stations non reconnues.")
# # #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# # #         print(f"Suppression de {len(df_gps) - len(df_gps_unique)} doublons dans df_gps (en gardant la première occurrence).")
# # #     else:
# # #         df_gps_unique = df_gps.copy()

# # #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# # #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# # #     for col in numerical_cols:
# # #         if col in df_processed.columns:
# # #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# # #     df_processed_parts = []

# # #     for station_name, group in df_processed.groupby('Station'):
# # #         group_copy = group.copy()
# # #         print(f"DEBUG (interpolation/groupby): Début du traitement du groupe '{station_name}'.")
        
# # #         # Standardize group_copy.index to UTC first
# # #         # This block ensures the index is UTC-aware before proceeding
# # #         if group_copy.index.tz is None:
# # #             group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# # #         elif group_copy.index.tz != pytz.utc: # If it's already tz-aware but not UTC, convert to UTC
# # #             group_copy.index = group_copy.index.tz_convert('UTC')
# # #         print(f"DEBUG (interpolation/groupby): Index Datetime pour '{station_name}' STANDARDISÉ à UTC. Dtype: {group_copy.index.dtype}")
        
# # #         # S'assurer que l'index n'a pas de NaT après localisation
# # #         group_copy = group_copy[group_copy.index.notna()]
# # #         if group_copy.empty:
# # #             warnings.warn(f"Le groupe '{station_name}' est vide après nettoyage de l'index Datetime. Il sera ignoré.")
# # #             continue


# # #         # --- Calculs Astral en utilisant le fuseau horaire local ---
# # #         apply_fixed_daylight = True
# # #         gps_data = gps_info_dict.get(station_name)
# # #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# # #             lat = gps_data['Lat']
# # #             long = gps_data['Long']
# # #             timezone_str = gps_data['Timezone']

# # #             try:
# # #                 local_tz = pytz.timezone(timezone_str)
# # #                 # Create a local timezone-aware version of the index for Astral calculations
# # #                 index_for_astral_local = group_copy.index.tz_convert(local_tz)

# # #                 daily_sun_info = {}
# # #                 # Get unique dates from the local-time index (these are timezone-aware Timestamps)
# # #                 # Utilisation de .drop_duplicates() au lieu de .unique() pour garantir un objet Pandas
# # #                 unique_dates_ts_local = index_for_astral_local.normalize().drop_duplicates()

# # #                 # Ensure unique_dates is not empty before processing
# # #                 if unique_dates_ts_local.empty: # Utilisez .empty pour les objets Pandas
# # #                     raise ValueError("No unique dates found for Astral calculation.")
                
# # #                 for ts_local_aware in unique_dates_ts_local: # Iterate over timezone-aware Timestamps
# # #                     loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
                    
# # #                     # Convert the timezone-aware Timestamp to a naive datetime.date object
# # #                     # This is what Astral's sun.sun function expects for its 'date' parameter
# # #                     # This conversion helps avoid potential re-localization warnings from Astral/pytz.
# # #                     naive_date_for_astral = ts_local_aware.to_pydatetime().date()
                    
# # #                     # Pass the NAIVE date object to Astral. Astral's observer handles the timezone internally.
# # #                     s = sun.sun(loc.observer, date=naive_date_for_astral) 
# # #                     daily_sun_info[naive_date_for_astral] = {
# # #                         'sunrise': s['sunrise'],
# # #                         'sunset': s['sunset']
# # #                     }

# # #                 # Explicitly create a list of naive dates for the DataFrame index
# # #                 naive_unique_dates_for_index = [ts.date() for ts in unique_dates_ts_local] # Use .date() here as keys are naive
# # #                 temp_df_sun_index = pd.Index(naive_unique_dates_for_index, name='Date_Local_Naive')
# # #                 temp_df_sun = pd.DataFrame(index=temp_df_sun_index)
                
# # #                 # NOUVEAUX DÉBOGAGES pour comprendre le type juste avant l'opération
# # #                 print(f"DEBUG (astral_calc): unique_dates_ts_local type: {type(unique_dates_ts_local)}")
# # #                 print(f"DEBUG (astral_calc): naive_unique_dates_for_index type: {type(naive_unique_dates_for_index)}")
# # #                 print(f"DEBUG (astral_calc): temp_df_sun_index type: {type(temp_df_sun_index)}")
# # #                 if not temp_df_sun.empty:
# # #                     print(f"DEBUG (astral_calc): First element of temp_df_sun.index: {temp_df_sun.index[0]}")
# # #                     print(f"DEBUG (astral_calc): Type of first element of temp_df_sun.index: {type(temp_df_sun.index[0])}")

# # #                 # Correction: Utilisation de la compréhension de liste pour éviter le problème de .map()
# # #                 temp_df_sun['sunrise_time_local'] = [daily_sun_info.get(date, {}).get('sunrise') for date in temp_df_sun.index]
# # #                 temp_df_sun['sunset_time_local'] = [daily_sun_info.get(date, {}).get('sunset') for date in temp_df_sun.index]

# # #                 # Merge with group_copy (which has UTC index)
# # #                 # To merge, create a normalized local date column (naive) in group_copy
# # #                 group_copy_reset = group_copy.reset_index()
# # #                 group_copy_reset['Date_Local_Naive'] = group_copy_reset['Datetime'].dt.tz_convert(local_tz).dt.date

# # #                 group_copy_reset = pd.merge(group_copy_reset, temp_df_sun, on='Date_Local_Naive', how='left')

# # #                 # Convert merged local times back to UTC for comparison with original 'Datetime' (which is UTC)
# # #                 group_copy_reset['sunrise_time_utc'] = group_copy_reset['sunrise_time_local'].dt.tz_convert('UTC')
# # #                 group_copy_reset['sunset_time_utc'] = group_copy_reset['sunset_time_local'].dt.tz_convert('UTC')

# # #                 group_copy_reset.loc[:, 'Is_Daylight'] = (group_copy_reset['Datetime'] >= group_copy_reset['sunrise_time_utc']) & \
# # #                                                           (group_copy_reset['Datetime'] < group_copy_reset['sunset_time_utc'])

# # #                 daylight_timedelta_local = group_copy_reset['sunset_time_local'] - group_copy_reset['sunrise_time_local']
                
# # #                 # Correction ici pour formater la durée en HH:MM:SS
# # #                 def format_timedelta_to_hms(td):
# # #                     if pd.isna(td):
# # #                         return np.nan
# # #                     total_seconds = int(td.total_seconds())
# # #                     hours = total_seconds // 3600
# # #                     minutes = (total_seconds % 3600) // 60
# # #                     seconds = total_seconds % 60
# # #                     return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

# # #                 group_copy_reset.loc[:, 'Daylight_Duration'] = daylight_timedelta_local.apply(format_timedelta_to_hms)


# # #                 group_copy = group_copy_reset.set_index('Datetime')
# # #                 group_copy = group_copy.drop(columns=['Date_Local_Naive', 'sunrise_time_local', 'sunset_time_local', 'sunrise_time_utc', 'sunset_time_utc'], errors='ignore')

# # #                 print(f"Lever et coucher du soleil calculés pour {station_name}.")
# # #                 apply_fixed_daylight = False

# # #             except Exception as e:
# # #                 print(f"Erreur lors du calcul du lever/coucher du soleil avec Astral pour {station_name}: {e}.")
# # #                 traceback.print_exc() # Print full traceback for this specific error
# # #                 warnings.warn(f"Calcul Astral impossible pour '{station_name}'. Utilisation de l'indicateur jour/nuit fixe.")
# # #                 apply_fixed_daylight = True
# # #         else:
# # #             print(f"Avertissement: Coordonnées ou Fuseau horaire manquants/invalides pour le site '{station_name}' dans df_gps. Utilisation de l'indicateur jour/nuit fixe.")
# # #             apply_fixed_daylight = True

# # #         if apply_fixed_daylight:
# # #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# # #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00" # Règle fixe pour la durée
# # #             print(f"Utilisation de l'indicateur jour/nuit fixe (7h-18h) pour {station_name}.")

# # #         df_processed_parts.append(group_copy)

# # #     if not df_processed_parts:
# # #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# # #     # Concaténation de toutes les parties. Puisque chaque partie a un DatetimeIndex,
# # #     # nous pouvons concaténer directement sans ignore_index=True
# # #     df_final = pd.concat(df_processed_parts)
# # #     # Assurez-vous que l'index est trié et qu'il est bien nommé 'Datetime'
# # #     df_final = pd.concat(df_processed_parts).sort_index()
# # #     df_final.index.name = 'Datetime' 
# # #     print(f"DEBUG (interpolation/concat): Index du DataFrame final après concaténation et tri: {type(df_final.index)}")
# # #     print(f"DEBUG (interpolation/concat): Colonnes du DataFrame final après concaténation: {df_final.columns.tolist()}")


# # #     # Nettoyage des colonnes temporaires (elles ne sont plus nécessaires ou n'existent plus)
# # #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date']
# # #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# # #     # Gestion intelligente de Rain_mm : Appel de la fonction modifiée
# # #     df_final = create_rain_mm(df_final)

# # #     # Note : Si create_rain_mm ne crée pas la colonne, elle ne sera pas présente ici.
# # #     # Les autres parties du code devront gérer l'absence de 'Rain_mm'.

# # #     # Interpolation standard et bornage pour les variables numériques
# # #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# # #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# # #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']

# # #     for var in standard_vars:
# # #         if var in df_final.columns: # Vérifier si la colonne existe avant de la traiter
# # #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# # #             if var in limits:
# # #                 min_val = limits[var]['min']
# # #                 max_val = limits[var]['max']
# # #                 initial_nan_count = df_final[var].isna().sum()
# # #                 if min_val is not None:
# # #                     df_final.loc[df_final[var] < min_val, var] = np.nan
# # #                 if max_val is not None:
# # #                     df_final.loc[df_final[var] > max_val, var] = np.nan
                
# # #                 new_nan_count = df_final[var].isna().sum()
# # #                 if new_nan_count > initial_nan_count:
# # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# # #             print(f"DEBUG (interpolation/variable): Interpolation de '{var}'. Type de l'index de df_final: {type(df_final.index)}")
            
# # #             # Interpolation seulement si l'index est bien DatetimeIndex
# # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # #                 df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# # #             else:
# # #                 print(f"Avertissement (interpolation/variable): L'index n'est pas un DatetimeIndex pour l'interpolation de '{var}'. Utilisation de la méthode 'linear'.")
# # #                 df_final[var] = df_final[var].interpolate(method='linear', limit_direction='both')
# # #             df_final[var] = df_final[var].bfill().ffill()

# # #     # Interpolation CONDITIONNELLE de la radiation solaire
# # #     if 'Solar_R_W/m^2' in df_final.columns: # Vérifier si la colonne existe
# # #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# # #         if 'Solar_R_W/m^2' in limits:
# # #             min_val = limits['Solar_R_W/m^2']['min']
# # #             max_val = limits['Solar_R_W/m^2']['max']
# # #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# # #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# # #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# # #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# # #         if 'Is_Daylight' in df_final.columns:
# # #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# # #             if 'Rain_mm' in df_final.columns: # Vérifier l'existence de 'Rain_mm'
# # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# # #             else:
# # #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# # #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# # #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# # #             print(f"DEBUG (interpolation/solaire): Interpolation de 'Solar_R_W/m^2' (conditionnel). Type de l'index de df_final: {type(df_final.index)}")

# # #             is_day = df_final['Is_Daylight']
# # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# # #             else:
# # #                 print(f"Avertissement (interpolation/solaire): L'index n'est pas un DatetimeIndex pour l'interpolation de 'Solar_R_W/m^2'. Utilisation de la méthode 'linear'.")
# # #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both')

# # #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# # #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# # #             warnings.warn("Radiation solaire interpolée avec succès.")
# # #         else:
# # #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# # #             if isinstance(df_final.index, pd.DatetimeIndex):
# # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()
# # #             else:
# # #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both').bfill().ffill()

# # #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# # #     missing_after_interp = df_final.isna().sum()
# # #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# # #     if not columns_with_missing.empty:
# # #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# # #     else:
# # #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# # #     return df_final


# # # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# # #     """
# # #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# # #     de coordonnées UTM vers latitude/longitude WGS84.

# # #     Args:
# # #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# # #     Returns:
# # #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# # #     """
# # #     df_copy = df.copy()

# # #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# # #     if not all(col in df_copy.columns for col in required_utm_cols):
# # #         raise ValueError(
# # #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# # #         )

# # #     def convert_row(row):
# # #         try:
# # #             zone = int(row['zone'])
# # #             hemisphere = str(row['hemisphere']).upper()
# # #             is_northern = hemisphere == 'N'

# # #             proj_utm = CRS.from_proj4(
# # #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# # #             )
# # #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# # #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# # #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# # #             return pd.Series({'Long': lon, 'Lat': lat})
# # #         except Exception as e:
# # #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# # #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# # #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# # #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# # #     return df_copy

# # # def _load_and_prepare_gps_data() -> pd.DataFrame:
# # #     """
# # #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# # #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# # #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# # #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# # #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
# # #     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

# # #     Returns:
# # #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# # #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# # #     """
# # #     print("Début de la préparation des données de coordonnées des stations...")
# # #     data_dir = 'data'
# # #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# # #     files_info = [
# # #         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
# # #         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
# # #         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
# # #     ]

# # #     loaded_dfs = []

# # #     for file_info in files_info:
# # #         output_file_path = os.path.join(data_dir, file_info['name'])
        
# # #         if not os.path.exists(output_file_path):
# # #             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
# # #             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
# # #             print(f"Téléchargement de {file_info['bassin']} terminé.")
# # #         else:
# # #             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
# # #         loaded_dfs.append(pd.read_excel(output_file_path))

# # #     vea_sissili_bassin = loaded_dfs[0]
# # #     dano_bassin = loaded_dfs[1]
# # #     dassari_bassin = loaded_dfs[2]

# # #     # Prétraitement des DataFrames (votre code original)
# # #     print("Début du prétraitement des données de stations...")
    
# # #     # Vea Sissili
# # #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# # #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# # #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# # #     # Dassari
# # #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# # #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# # #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# # #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# # #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# # #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# # #     dano_bassin['zone'] = 30
# # #     dano_bassin['hemisphere'] = 'N'
# # #     dassari_bassin['zone'] = 31
# # #     dassari_bassin['hemisphere'] = 'N'

# # #     # Application de la fonction de conversion UTM vers GPS
# # #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# # #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# # #     # Ajout des fuseaux horaires
# # #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# # #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# # #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# # #     # Fusion de tous les bassins
# # #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# # #     # Renommer 'Name' en 'Station'
# # #     bassins = bassins.rename(columns={'Name': 'Station'})

# # #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# # #     initial_rows = len(bassins)
# # #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# # #     if len(bassins) < initial_rows:
# # #         print(f"Attention: {initial_rows - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# # #     # 5. Sauvegarde du DataFrame final en JSON
# # #     output_json_path = os.path.join(data_dir, "station_coordinates.json")
# # #     # Utiliser to_json avec orient='records' pour un format plus lisible et facile à charger
# # #     bassins.to_json(output_json_path, orient='records', indent=4)
# # #     print(f"\nPréparation des données terminée. Coordonnées des stations sauvegardées dans '{output_json_path}'.")
# # #     print("Vous pouvez maintenant lancer votre application Flask.")

# # #     return bassins # Retourner le DataFrame des données GPS


# # # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# # #     """
# # #     Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
# # #     Conserve la première occurrence en cas de doublon.

# # #     Args:
# # #         df (pd.DataFrame): Le DataFrame d'entrée.

# # #     Returns:
# # #         pd.DataFrame: Le DataFrame sans doublons.
# # #     """
# # #     if 'Station' in df.columns and 'Datetime' in df.columns:
# # #         initial_rows = len(df)
# # #         df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
# # #         if len(df_cleaned) < initial_rows:
# # #             warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
# # #         return df_cleaned
# # #     else:
# # #         warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
# # #         return df

# # # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# # #     """
# # #     Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

# # #     Args:
# # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# # #         limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

# # #     Returns:
# # #         pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
# # #     """
# # #     df_processed = df.copy()

# # #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# # #         warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
# # #         try:
# # #             df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
# # #             df_processed = df_processed[df_processed.index.notna()]
# # #             if df_processed.empty:
# # #                 raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
# # #         except Exception as e:
# # #             raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

# # #     for var, vals in limits.items():
# # #         if var in df_processed.columns:
# # #             min_val = vals.get('min')
# # #             max_val = vals.get('max')
# # #             if min_val is not None or max_val is not None:
# # #                 initial_nan_count = df_processed[var].isna().sum()
# # #                 if min_val is not None:
# # #                     df_processed.loc[df_processed[var] < min_val, var] = np.nan
# # #                 if max_val is not None:
# # #                     df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
# # #                 new_nan_count = df_processed[var].isna().sum()
# # #                 if new_nan_count > initial_nan_count:
# # #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
# # #     return df_processed

# # # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
# # #     """
# # #     Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
# # #     Retourne l'objet Figure Plotly.
# # #     """
# # #     if not isinstance(df.index, pd.DatetimeIndex):
# # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

# # #     filtered_df = df[df['Station'] == station].copy()
# # #     if filtered_df.empty:
# # #         return go.Figure() # Retourne une figure vide si pas de données

# # #     if periode == 'Journalière':
# # #         resampled_df = filtered_df[variable].resample('D').mean()
# # #     elif periode == 'Hebdomadaire':
# # #         resampled_df = filtered_df[variable].resample('W').mean()
# # #     elif periode == 'Mensuelle':
# # #         resampled_df = filtered_df[variable].resample('M').mean()
# # #     elif periode == 'Annuelle':
# # #         resampled_df = filtered_df[variable].resample('Y').mean()
# # #     else: # Données Brutes
# # #         resampled_df = filtered_df[variable]

# # #     resampled_df = resampled_df.dropna()

# # #     if resampled_df.empty:
# # #         return go.Figure() # Retourne une figure vide si pas de données après resample/dropna

# # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # #     color = colors.get(station, '#1f77b4')

# # #     fig = go.Figure()
# # #     fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # #                              mode='lines', name=f'{variable_meta["Nom"]} - {station}',
# # #                              line=dict(color=color)))

# # #     fig.update_layout(
# # #         title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
# # #         xaxis_title="Date",
# # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # #         hovermode="x unified",
# # #         template="plotly_white" # Utiliser un template Plotly plus clair
# # #     )
# # #     return fig

# # # def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
# # #     """
# # #     Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
# # #     Retourne l'objet Figure Plotly.
# # #     """
# # #     if not isinstance(df.index, pd.DatetimeIndex):
# # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

# # #     fig = go.Figure()
    
# # #     all_stations = df['Station'].unique()
# # #     if len(all_stations) < 2:
# # #         warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
# # #         return go.Figure() # Retourne une figure vide si moins de 2 stations

# # #     for station in all_stations:
# # #         filtered_df = df[df['Station'] == station].copy()
# # #         if filtered_df.empty:
# # #             continue

# # #         if periode == 'Journalière':
# # #             resampled_df = filtered_df[variable].resample('D').mean()
# # #         elif periode == 'Hebdomadaire':
# # #             resampled_df = filtered_df[variable].resample('W').mean()
# # #         elif periode == 'Mensuelle':
# # #             resampled_df = filtered_df[variable].resample('M').mean()
# # #         elif periode == 'Annuelle':
# # #             resampled_df = filtered_df[variable].resample('Y').mean()
# # #         else: # Données Brutes
# # #             resampled_df = filtered_df[variable]

# # #         resampled_df = resampled_df.dropna()
# # #         if resampled_df.empty:
# # #             continue
        
# # #         color = colors.get(station, '#1f77b4')
# # #         fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# # #                                  mode='lines', name=station,
# # #                                  line=dict(color=color)))

# # #     if not fig.data:
# # #         return go.Figure() # Retourne une figure vide si aucune trace n'a été ajoutée

# # #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# # #     fig.update_layout(
# # #         title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
# # #         xaxis_title="Date",
# # #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# # #         hovermode="x unified",
# # #         legend_title="Variables",
# # #         template="plotly_white"
# # #     )
# # #     return fig


# # # def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> go.Figure:
# # #     """
# # #     Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
# # #     Retourne l'objet Figure Plotly.
# # #     """
# # #     if not isinstance(df.index, pd.DatetimeIndex):
# # #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

# # #     filtered_df = df[df['Station'] == station].copy()
# # #     if filtered_df.empty:
# # #         return go.Figure() # Retourne une figure vide si pas de données

# # #     numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight', 'Daylight_Duration']]

# # #     if not numerical_vars:
# # #         warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
# # #         return go.Figure() # Retourne une figure vide si pas de variables numériques

# # #     normalized_df = filtered_df[numerical_vars].copy()
# # #     for col in normalized_df.columns:
# # #         min_val = normalized_df[col].min()
# # #         max_val = normalized_df[col].max()
# # #         if max_val != min_val:
# # #             normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
# # #         else:
# # #             normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

# # #     normalized_df = normalized_df.dropna(how='all')

# # #     if normalized_df.empty:
# # #         return go.Figure() # Retourne une figure vide si pas de données après normalisation/dropna
    
# # #     fig = go.Figure()
# # #     for var in normalized_df.columns:
# # #         var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
# # #         color = colors.get(var, None)

# # #         fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
# # #                                  mode='lines', name=var_meta['Nom'],
# # #                                  line=dict(color=color)))

# # #     fig.update_layout(
# # #         title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
# # #         xaxis_title="Date",
# # #         yaxis_title="Valeur Normalisée (0-1)",
# # #         hovermode="x unified",
# # #         legend_title="Variables",
# # #         template="plotly_white"
# # #     )
# # #     return fig

# # # def calculate_daily_summary_table(df: pd.DataFrame) -> pd.DataFrame:
# # #     """
# # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # #     groupées par station. Cette fonction renvoie un DataFrame de statistiques, non un graphique.
# # #     C'est la fonction qui remplace l'ancienne 'daily_stats'.

# # #     Args:
# # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # #     Returns:
# # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # #     """
# # #     df_copy = df.copy()

# # #     # Assurez-vous que 'Datetime' est une colonne et de type datetime
# # #     if isinstance(df_copy.index, pd.DatetimeIndex):
# # #         df_copy = df_copy.reset_index()

# # #     df_copy['Datetime'] = pd.to_datetime(df_copy['Datetime'], errors='coerce')
# # #     df_copy = df_copy.dropna(subset=['Datetime', 'Station'])

# # #     if df_copy.empty:
# # #         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans calculate_daily_summary_table.")
# # #         return pd.DataFrame()

# # #     if 'Is_Daylight' not in df_copy.columns:
# # #         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
# # #         df_copy['Is_Daylight'] = (df_copy['Datetime'].dt.hour >= 7) & (df_copy['Datetime'].dt.hour <= 18)

# # #     numerical_cols = [col for col in df_copy.columns if pd.api.types.is_numeric_dtype(df_copy[col]) and col not in ['Station', 'Datetime', 'Is_Daylight', 'Daylight_Duration']]
    
# # #     if not numerical_cols:
# # #         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
# # #         return pd.DataFrame()

# # #     # Calcul des statistiques de base par jour
# # #     daily_aggregated_df = df_copy.groupby(['Station', df_copy['Datetime'].dt.date]).agg({
# # #         col: ['mean', 'min', 'max'] for col in numerical_cols if METADATA_VARIABLES.get(col, {}).get('is_rain') == False
# # #     })

# # #     # Renommage des colonnes agrégées pour les non-pluies
# # #     daily_aggregated_df.columns = ['_'.join(col).strip() for col in daily_aggregated_df.columns.values]


# # #     # Traitement spécifique pour la pluie (Rain_mm)
# # #     if 'Rain_mm' in df_copy.columns and METADATA_VARIABLES.get('Rain_mm', {}).get('is_rain'):
# # #         df_daily_rain = df_copy.groupby(['Station', df_copy['Datetime'].dt.date])['Rain_mm'].sum().reset_index()
# # #         df_daily_rain = df_daily_rain.rename(columns={'Rain_mm': 'Rain_mm_sum'})

# # #         # Fusionner les statistiques de pluie avec les autres
# # #         if not daily_aggregated_df.empty:
# # #             daily_aggregated_df = daily_aggregated_df.reset_index()
# # #             daily_stats_df = pd.merge(daily_aggregated_df, df_daily_rain, on=['Station', 'Datetime'], how='left')
# # #             daily_stats_df = daily_stats_df.rename(columns={'Datetime': 'Date'})
# # #         else:
# # #             daily_stats_df = df_daily_rain.rename(columns={'Datetime': 'Date'})
# # #     else:
# # #         daily_stats_df = daily_aggregated_df.reset_index().rename(columns={'Datetime': 'Date'})


# # #     # Calcul des statistiques de saison et de sécheresse pour la pluie
# # #     if 'Rain_mm' in df_copy.columns and METADATA_VARIABLES.get('Rain_mm', {}).get('is_rain'):
# # #         df_daily_rain_raw = df_copy.groupby(['Station', pd.Grouper(key='Datetime', freq='D')])['Rain_mm'].sum().reset_index()
        
# # #         RAIN_SEASON_GAP_THRESHOLD = pd.Timedelta(days=60)
# # #         season_stats = []
# # #         for station_name, station_df_rain in df_daily_rain_raw.groupby('Station'):
# # #             station_df_rain = station_df_rain.set_index('Datetime').sort_index()
# # #             rain_events = station_df_rain[station_df_rain['Rain_mm'] > 0].index

# # #             if rain_events.empty:
# # #                 season_stats.append({'Station': station_name, 'Moyenne_Saison_Pluvieuse': np.nan, 'Debut_Saison_Pluvieuse': pd.NaT, 'Fin_Saison_Pluvieuse': pd.NaT, 'Duree_Saison_Pluvieuse_Jours': np.nan})
# # #                 continue
            
# # #             block_ids = (rain_events.to_series().diff() > RAIN_SEASON_GAP_THRESHOLD).cumsum()
# # #             valid_blocks = {}
# # #             for block_id, rain_dates_in_block in rain_events.to_series().groupby(block_ids):
# # #                 if not rain_dates_in_block.empty:
# # #                     block_start = rain_dates_in_block.min()
# # #                     block_end = rain_dates_in_block.max()
# # #                     full_block_df = station_df_rain.loc[block_start:block_end]
# # #                     valid_blocks[block_id] = full_block_df

# # #             if not valid_blocks:
# # #                 season_stats.append({'Station': station_name, 'Moyenne_Saison_Pluvieuse': np.nan, 'Debut_Saison_Pluvieuse': pd.NaT, 'Fin_Saison_Pluvieuse': pd.NaT, 'Duree_Saison_Pluvieuse_Jours': np.nan})
# # #                 continue

# # #             main_block_id = max(valid_blocks, key=lambda k: (valid_blocks[k].index.max() - valid_blocks[k].index.min()).days)
# # #             main_season_df = valid_blocks[main_block_id]

# # #             debut_saison = main_season_df.index.min()
# # #             fin_saison = main_season_df.index.max()
# # #             total_days = (fin_saison - debut_saison).days + 1
# # #             moyenne_saison = main_season_df['Rain_mm'].sum() / total_days if total_days > 0 else 0

# # #             season_stats.append({
# # #                 'Station': station_name,
# # #                 'Moyenne_Saison_Pluvieuse': moyenne_saison,
# # #                 'Debut_Saison_Pluvieuse': debut_saison,
# # #                 'Fin_Saison_Pluvieuse': fin_saison,
# # #                 'Duree_Saison_Pluvieuse_Jours': total_days
# # #             })
# # #         df_season_stats = pd.DataFrame(season_stats)
        
# # #         # Merge season stats into main daily_stats_df
# # #         if not df_season_stats.empty:
# # #             daily_stats_df = pd.merge(daily_stats_df, df_season_stats, on='Station', how='left')


# # #     # Calcul des statistiques globales (Max, Min, Moyenne, Médiane, etc.) par station
# # #     # pour les variables numériques, à inclure dans le tableau récapitulatif.
# # #     final_stats_per_station = pd.DataFrame()
# # #     for station_name in df_copy['Station'].unique():
# # #         station_df = df_copy[df_copy['Station'] == station_name].copy()
# # #         station_summary = {'Station': station_name}

# # #         for var in numerical_cols:
# # #             if var in station_df.columns and pd.api.types.is_numeric_dtype(station_df[var]):
# # #                 # Filtrage pour la radiation solaire (seulement pendant le jour)
# # #                 if var == 'Solar_R_W/m^2':
# # #                     var_data = station_df.loc[station_df['Is_Daylight'], var].dropna()
# # #                 else:
# # #                     var_data = station_df[var].dropna()
                
# # #                 if not var_data.empty:
# # #                     station_summary[f'{var}_Maximum'] = var_data.max()
# # #                     station_summary[f'{var}_Minimum'] = var_data.min()
# # #                     station_summary[f'{var}_Moyenne'] = var_data.mean()
# # #                     station_summary[f'{var}_Mediane'] = var_data.median()
                    
# # #                     # Pour Rain_mm uniquement
# # #                     if var == 'Rain_mm':
# # #                         station_summary[f'{var}_Cumul_Annuel'] = station_df['Rain_mm'].sum()
# # #                         # Moyenne des jours pluvieux (seulement les jours où il a plu)
# # #                         rainy_days_data = station_df[station_df['Rain_mm'] > 0]['Rain_mm'].dropna()
# # #                         station_summary[f'{var}_Moyenne_Jours_Pluvieux'] = rainy_days_data.mean() if not rainy_days_data.empty else np.nan

# # #                         # Ajouter la durée de la saison pluvieuse et de sécheresse du df_season_stats
# # #                         if 'Duree_Saison_Pluvieuse_Jours' in daily_stats_df.columns:
# # #                             s_data = daily_stats_df[daily_stats_df['Station'] == station_name]
# # #                             if not s_data.empty:
# # #                                 station_summary[f'{var}_Duree_Saison_Pluvieuse_Jours'] = s_data['Duree_Saison_Pluvieuse_Jours'].iloc[0]
# # #                                 # Placeholder pour sécheresse si non calculé ailleurs
# # #                                 station_summary[f'{var}_Duree_Secheresse_Definie_Jours'] = np.nan # Vous devrez calculer ceci plus tard

# # #         final_stats_per_station = pd.concat([final_stats_per_station, pd.DataFrame([station_summary])], ignore_index=True)
        
# # #     return final_stats_per_station # Retourne le DataFrame de statistiques agrégées par station


# # # def generate_variable_summary_plots_for_web(df: pd.DataFrame, station: str, variable: str, metadata: dict, palette: dict) -> plt.Figure:
# # #     """
# # #     Génère un graphique Matplotlib/Seaborn pour les statistiques agrégées d'une variable spécifique
# # #     pour une station donnée, en utilisant la logique fournie par l'utilisateur pour 'daily_stats'.

# # #     Args:
# # #         df (pd.DataFrame): Le DataFrame global de données traitées avec DatetimeIndex et colonne 'Station'.
# # #         station (str): Le nom de la station à visualiser.
# # #         variable (str): La variable à visualiser (e.g., 'Air_Temp_Deg_C', 'Rain_mm').
# # #         metadata (dict): Dictionnaire de métadonnées pour les variables (Nom, Unite, is_rain).
# # #         palette (dict): Dictionnaire de couleurs pour les différentes métriques statistiques.

# # #     Returns:
# # #         plt.Figure: Un objet Figure Matplotlib contenant tous les sous-graphiques pour la variable sélectionnée.
# # #     """
# # #     df_station = df[df['Station'] == station].copy()

# # #     if df_station.empty:
# # #         fig, ax = plt.subplots(figsize=(10, 6))
# # #         ax.text(0.5, 0.5, f"Aucune donnée pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# # #         ax.axis('off')
# # #         return fig

# # #     # S'assurer que Datetime est un index datetime correct pour les données de la station
# # #     if isinstance(df_station.index, pd.DatetimeIndex):
# # #         df_station = df_station.reset_index() # Réinitialise l'index pour un accès facile aux colonnes
    
# # #     df_station['Datetime'] = pd.to_datetime(df_station['Datetime'], errors='coerce')
# # #     df_station = df_station.dropna(subset=['Datetime', 'Station'])
# # #     df_station = df_station.set_index('Datetime').sort_index() # Remet l'index pour les opérations de séries temporelles

# # #     if df_station.empty:
# # #         fig, ax = plt.subplots(figsize=(10, 6))
# # #         ax.text(0.5, 0.5, f"DataFrame vide après nettoyage des dates pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# # #         ax.axis('off')
# # #         return fig

# # #     # Assurez-vous que 'Is_Daylight' existe (généralement géré par interpolation, mais au cas où)
# # #     if 'Is_Daylight' not in df_station.columns:
# # #         df_station['Is_Daylight'] = (df_station.index.hour >= 7) & (df_station.index.hour <= 18)


# # #     var_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})

# # #     # Dictionnaire pour stocker les statistiques pour la variable sélectionnée
# # #     stats_for_plot = {}
# # #     metrics_to_plot = []
    
# # #     # --- LOGIQUE SPÉCIFIQUE À RAIN_MM (transférée de la fonction daily_stats de l'utilisateur) ---
# # #     if var_meta.get('is_rain', False) and variable == 'Rain_mm':
# # #         # Vérifier si 'Rain_mm' existe avant de procéder aux calculs spécifiques à la pluie
# # #         if 'Rain_mm' not in df_station.columns:
# # #             fig, ax = plt.subplots(figsize=(10, 6))
# # #             ax.text(0.5, 0.5, f"La variable 'Rain_mm' n'est pas disponible pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# # #             ax.axis('off')
# # #             return fig

# # #         df_daily_rain = df_station.groupby(pd.Grouper(freq='D'))['Rain_mm'].sum().reset_index()
# # #         df_daily_rain = df_daily_rain.rename(columns={'Rain_mm': 'Rain_mm_sum'})
# # #         df_daily_rain['Datetime'] = pd.to_datetime(df_daily_rain['Datetime'])
# # #         df_daily_rain = df_daily_rain.set_index('Datetime').sort_index()

# # #         RAIN_SEASON_GAP_THRESHOLD = pd.Timedelta(days=60)
# # #         rain_events = df_daily_rain[df_daily_rain['Rain_mm_sum'] > 0].index

# # #         s_moyenne_saison = np.nan
# # #         s_duree_saison = np.nan
# # #         s_debut_saison = pd.NaT
# # #         s_fin_saison = pd.NaT

# # #         if not rain_events.empty:
# # #             block_ids = (rain_events.to_series().diff() > RAIN_SEASON_GAP_THRESHOLD).cumsum()
# # #             valid_blocks = {}
# # #             for block_id, rain_dates_in_block in rain_events.to_series().groupby(block_ids):
# # #                 if not rain_dates_in_block.empty:
# # #                     block_start = rain_dates_in_block.min()
# # #                     block_end = rain_dates_in_block.max()
# # #                     full_block_df = df_daily_rain.loc[block_start:block_end]
# # #                     valid_blocks[block_id] = full_block_df

# # #             if valid_blocks:
# # #                 main_block_id = max(valid_blocks, key=lambda k: (valid_blocks[k].index.max() - valid_blocks[k].index.min()).days)
# # #                 main_season_df = valid_blocks[main_block_id]

# # #                 s_debut_saison = main_season_df.index.min()
# # #                 s_fin_saison = main_season_df.index.max()
# # #                 total_days_season = (s_fin_saison - s_debut_saison).days + 1
# # #                 s_moyenne_saison = main_season_df['Rain_mm_sum'].sum() / total_days_season if total_days_season > 0 else 0
# # #                 s_duree_saison = total_days_season

# # #         # Logique de détection de la sécheresse
# # #         longest_dry_spell = np.nan
# # #         debut_secheresse = pd.NaT
# # #         fin_secheresse = pd.NaT

# # #         full_daily_series_rain = df_daily_rain['Rain_mm_sum'].resample('D').sum().fillna(0)
# # #         rainy_days_index = full_daily_series_rain[full_daily_series_rain > 0].index

# # #         if not rainy_days_index.empty and pd.notna(s_moyenne_saison) and s_moyenne_saison > 0:
# # #             temp_dry_spells = []
# # #             for i in range(1, len(rainy_days_index)):
# # #                 prev_rain_date = rainy_days_index[i-1]
# # #                 current_rain_date = rainy_days_index[i]
# # #                 dry_days_between_rains = (current_rain_date - prev_rain_date).days - 1

# # #                 if dry_days_between_rains > 0:
# # #                     rain_prev_day = full_daily_series_rain.loc[prev_rain_date]
# # #                     temp_debut = pd.NaT
# # #                     temp_duree = 0
# # #                     for j in range(1, dry_days_between_rains + 1):
# # #                         current_dry_date = prev_rain_date + timedelta(days=j)
# # #                         current_ratio = rain_prev_day / j
# # #                         if current_ratio < s_moyenne_saison: # Condition pour la sécheresse définie
# # #                             temp_debut = current_dry_date
# # #                             temp_duree = (current_rain_date - temp_debut).days
# # #                             break
# # #                     if pd.notna(temp_debut) and temp_duree > 0:
# # #                         temp_dry_spells.append({
# # #                             'Duree': temp_duree,
# # #                             'Debut': temp_debut,
# # #                             'Fin': current_rain_date - timedelta(days=1)
# # #                         })
            
# # #             if temp_dry_spells:
# # #                 df_temp_dry = pd.DataFrame(temp_dry_spells)
# # #                 idx_max_dry = df_temp_dry['Duree'].idxmax()
# # #                 longest_dry_spell = df_temp_dry.loc[idx_max_dry, 'Duree']
# # #                 debut_secheresse = df_temp_dry.loc[idx_max_dry, 'Debut']
# # #                 fin_secheresse = df_temp_dry.loc[idx_max_dry, 'Fin']

# # #         # Statistiques pour Rain_mm (tirées de la vue consolidée de daily_stats de l'utilisateur)
# # #         rain_data_for_stats = df_station[variable].dropna()
        
# # #         stats_for_plot['Maximum'] = rain_data_for_stats.max() if not rain_data_for_stats.empty else np.nan
# # #         stats_for_plot['Minimum'] = rain_data_for_stats.min() if not rain_data_for_stats.empty else np.nan
# # #         stats_for_plot['Mediane'] = rain_data_for_stats.median() if not rain_data_for_stats.empty else np.nan
# # #         stats_for_plot['Cumul_Annuel'] = df_station[variable].sum()
        
# # #         rainy_days_data = df_station[df_station[variable] > 0][variable].dropna()
# # #         stats_for_plot['Moyenne_Jours_Pluvieux'] = rainy_days_data.mean() if not rainy_days_data.empty else np.nan
        
# # #         stats_for_plot['Moyenne_Saison_Pluvieuse'] = s_moyenne_saison
# # #         stats_for_plot['Duree_Saison_Pluvieuse_Jours'] = s_duree_saison
# # #         stats_for_plot['Debut_Saison_Pluvieuse'] = s_debut_saison
# # #         stats_for_plot['Fin_Saison_Pluvieuse'] = s_fin_saison
        
# # #         stats_for_plot['Duree_Secheresse_Definie_Jours'] = longest_dry_spell
# # #         stats_for_plot['Debut_Secheresse_Definie'] = debut_secheresse
# # #         stats_for_plot['Fin_Secheresse_Definie'] = fin_secheresse

# # #         # Récupérer les dates pour max/min pour Rain_mm
# # #         max_date_idx = df_station[variable].idxmax() if not df_station[variable].empty else pd.NaT
# # #         min_date_idx = df_station[variable].idxmin() if not df_station[variable].empty else pd.NaT
# # #         stats_for_plot['Date_Maximum'] = max_date_idx if pd.notna(max_date_idx) else pd.NaT
# # #         stats_for_plot['Date_Minimum'] = min_date_idx if pd.notna(min_date_idx) else pd.NaT
        
# # #         metrics_to_plot = [
# # #             'Maximum', 'Minimum', 'Cumul_Annuel', 'Mediane',
# # #             'Moyenne_Jours_Pluvieux', 'Moyenne_Saison_Pluvieuse',
# # #             'Duree_Saison_Pluvieuse_Jours', 'Duree_Secheresse_Definie_Jours'
# # #         ]
# # #         nrows, ncols = 4, 2
# # #         figsize = (18, 16)
        
# # #     # --- LOGIQUE POUR LES AUTRES VARIABLES (transférée de la fonction daily_stats de l'utilisateur) ---
# # #     else:
# # #         current_var_data = df_station[variable].dropna()
# # #         if variable == 'Solar_R_W/m^2':
# # #             current_var_data = df_station.loc[df_station['Is_Daylight'], variable].dropna()

# # #         if current_var_data.empty:
# # #             fig, ax = plt.subplots(figsize=(10, 6))
# # #             ax.text(0.5, 0.5, f"Aucune donnée valide pour la variable {var_meta['Nom']} à {station}.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# # #             ax.axis('off')
# # #             return fig

# # #         # Statistiques pour les autres variables (de daily_stats de l'utilisateur)
# # #         stats_for_plot['Maximum'] = current_var_data.max()
# # #         stats_for_plot['Minimum'] = current_var_data.min()
# # #         stats_for_plot['Mediane'] = current_var_data.median()
# # #         stats_for_plot['Moyenne'] = current_var_data.mean()

# # #         # Récupérer les dates pour max/min
# # #         max_idx = current_var_data.idxmax() if not current_var_data.empty else pd.NaT
# # #         min_idx = current_var_data.idxmin() if not current_var_data.empty else pd.NaT

# # #         stats_for_plot['Date_Maximum'] = max_idx if pd.notna(max_idx) else pd.NaT
# # #         stats_for_plot['Date_Minimum'] = min_idx if pd.notna(min_idx) else pd.NaT

# # #         metrics_to_plot = ['Maximum', 'Minimum', 'Moyenne', 'Mediane']
# # #         nrows, ncols = 2, 2
# # #         figsize = (18, 12)

# # #     # Gérer le cas où stats_for_plot pourrait être vide (ex: toutes les données sont NaN)
# # #     if not stats_for_plot:
# # #         fig, ax = plt.subplots(figsize=(10, 6))
# # #         ax.text(0.5, 0.5, f"Impossible de calculer des statistiques pour la variable '{variable}' à la station '{station}' (données manquantes ou non numériques).", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# # #         ax.axis('off')
# # #         return fig

# # #     # --- PARTIE COMMUNE DE TRACÉ (adaptée de la fonction daily_stats de l'utilisateur) ---
# # #     fig, axes = plt.subplots(nrows, ncols, figsize=figsize)
# # #     plt.subplots_adjust(hspace=0.6, wspace=0.4) # Ajustement de l'espacement pour un meilleur ajustement
# # #     axes = axes.flatten()

# # #     fig.suptitle(f'Statistiques de {var_meta["Nom"]} pour la station {station}', fontsize=16, y=0.98)

# # #     for i, metric in enumerate(metrics_to_plot):
# # #         ax = axes[i]
# # #         value = stats_for_plot.get(metric)
# # #         if pd.isna(value):
# # #             ax.text(0.5, 0.5, "Données non disponibles", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=12, color='gray')
# # #             ax.axis('off')
# # #             continue

# # #         color = palette.get(metric.replace(' ', '_'), '#cccccc') # Utilisation de la `palette` passée en argument
        
# # #         # Créer un DataFrame factice pour sns.barplot, car il attend un DataFrame
# # #         plot_data_bar = pd.DataFrame({'Metric': [metric.replace('_', ' ')], 'Value': [value]})
# # #         sns.barplot(x='Metric', y='Value', data=plot_data_bar, ax=ax, color=color, edgecolor='none')

# # #         # Ajouter les annotations
# # #         text = ""
# # #         if metric in ['Duree_Saison_Pluvieuse_Jours', 'Duree_Secheresse_Definie_Jours']:
# # #             start_date_key = f'Debut_{metric.replace("Jours", "")}'
# # #             end_date_key = f'Fin_{metric.replace("Jours", "")}'
# # #             start_date = stats_for_plot.get(start_date_key)
# # #             end_date = stats_for_plot.get(end_date_key)
# # #             date_info = ""
# # #             if pd.notna(start_date) and pd.notna(end_date):
# # #                 date_info = f"\ndu {start_date.strftime('%d/%m/%Y')} au {end_date.strftime('%d/%m/%Y')}"
# # #             text = f"{int(value)} j{date_info}"
# # #         elif metric in ['Maximum', 'Minimum', 'Cumul_Annuel', 'Moyenne_Jours_Pluvieux', 'Moyenne_Saison_Pluvieuse', 'Mediane', 'Moyenne']:
# # #             unit = var_meta['Unite']
# # #             date_str = ''
# # #             # Vérifier les clés de date pour max/min si elles existent dans stats_for_plot
# # #             if (metric == 'Maximum' and 'Date_Maximum' in stats_for_plot and pd.notna(stats_for_plot['Date_Maximum'])):
# # #                 date_str = f"\n({stats_for_plot['Date_Maximum'].strftime('%d/%m/%Y')})"
# # #             elif (metric == 'Minimum' and 'Date_Minimum' in stats_for_plot and pd.notna(stats_for_plot['Date_Minimum'])):
# # #                 date_str = f"\n({stats_for_plot['Date_Minimum'].strftime('%d/%m/%Y')})"
            
# # #             text = f"{value:.1f} {unit}{date_str}"
# # #         else:
# # #             text = f"{value:.1f} {var_meta['Unite']}"

# # #         # Positionne le texte sur le graphique.
# # #         ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
# # #                 text, ha='center', va='bottom', fontsize=9, color='black',
# # #                 bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))
        
# # #         ax.set_title(f"{var_meta['Nom']} {metric.replace('_', ' ')}", fontsize=11)
# # #         ax.set_xlabel("")
# # #         ax.set_ylabel(f"Valeur ({var_meta['Unite']})", fontsize=10)
# # #         ax.tick_params(axis='x', rotation=0)
# # #         ax.set_xticklabels([])

# # #     # Si moins de graphiques que de sous-graphiques, désactiver les axes inutilisés
# # #     for j in range(i + 1, len(axes)):
# # #         fig.delaxes(axes[j])

# # #     plt.tight_layout(rect=[0, 0, 1, 0.96])

# # #     return fig


# # # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# # #     """
# # #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# # #     groupées par station.

# # #     Args:
# # #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# # #     Returns:
# # #         pd.DataFrame: DataFrame avec les statistiques journalières.
# # #     """
# # #     df = df.copy()

# # #     # Assurez-vous que 'Datetime' est une colonne et de type datetime
# # #     if isinstance(df.index, pd.DatetimeIndex):
# # #         df = df.reset_index()

# # #     df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')
# # #     df = df.dropna(subset=['Datetime', 'Station'])

# # #     if df.empty:
# # #         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans daily_stats.")
# # #         return pd.DataFrame()

# # #     if 'Is_Daylight' not in df.columns:
# # #         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
# # #         df['Is_Daylight'] = (df['Datetime'].dt.hour >= 7) & (df['Datetime'].dt.hour <= 18)

# # #     numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Datetime', 'Is_Daylight']]
    
# # #     if not numerical_cols:
# # #         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
# # #         return pd.DataFrame()

# # #     agg_funcs = {col: ['mean', 'min', 'max'] for col in numerical_cols}
    
# # #     if 'Rain_mm' in numerical_cols:
# # #         agg_funcs['Rain_mm'].append('sum')

# # #     daily_stats_df = df.groupby(['Station', df['Datetime'].dt.date]).agg(agg_funcs)

# # #     daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

# # #     daily_stats_df = daily_stats_df.rename_axis(index={'Datetime': 'Date'})
# # #     daily_stats_df = daily_stats_df.reset_index()

# # #     return daily_stats_df


# # import pandas as pd
# # from pyproj import CRS, Transformer
# # import pytz
# # from astral.location import LocationInfo
# # from astral import sun
# # import numpy as np
# # import warnings
# # import os
# # import gdown # Importation du module gdown pour le téléchargement
# # import plotly.graph_objects as go # Importation pour gérer les objets Figure Plotly
# # import matplotlib.pyplot as plt # NOUVEAUX IMPORTS pour Matplotlib
# # import seaborn as sns
# # import traceback # Importation de traceback pour les messages d'erreur détaillés
# # import math # Ajouté pour math.ceil
# # from datetime import timedelta # Ajouté pour timedelta

# # # Importation des configurations et métadonnées depuis config.py
# # from config import METADATA_VARIABLES, PALETTE_DEFAUT, DATA_LIMITS

# # # --- Configurations pour les stations Vea Sissili ---
# # stations_vea_a_9_variables = ['Oualem', 'Nebou', 'Nabugubelle', 'Manyoro', 'Gwosi', 'Doninga','Bongo Soe']

# # colonnes_a_renommer_stations_vea_a_9_variables = {
# #     'Rain_mm_Tot': 'Rain_mm',
# #     'AirTC_Avg': 'Air_Temp_Deg_C',
# #     'RH': 'Rel_H_%',
# #     'SlrW_Avg': 'Solar_R_W/m^2',
# #     'WS_ms_S_WVT': 'Wind_Sp_m/sec',
# #     'WindDir_D1_WVT': 'Wind_Dir_Deg',
# #     'Date': 'Datetime' # Renommer 'Date' en 'Datetime' pour compatibilité
# # }

# # colonnes_a_sup_stations_vea_a_9_variables = ['SlrkJ_Tot', 'WS_ms_Avg', 'WindDir', 'Rain_01_mm_Tot', 'Rain_02_mm_Tot']

# # colonnes_a_renommer_aniabisi = {
# #     'Rain_mm_Tot': 'Rain_mm',
# #     'AirTC_Avg': 'Air_Temp_Deg_C',
# #     'RH': 'Rel_H_%',
# #     'SlrW_Avg': 'Solar_R_W/m^2',
# #     'WS_ms_S_WVT': 'Wind_Sp_m/sec',
# #     'WindDir_D1_WVT': 'Wind_Dir_Deg',
# #     'Date': 'Datetime' # Renommer 'Date' en 'Datetime' pour compatibilité
# # }

# # colonnes_a_sup_aniabisi = ['Intensity_RT_Avg', 'Acc_NRT_Tot', 'Acc_RT_NRT_Tot', 'SR01Up_Avg', 'SR01Dn_Avg', 'IR01Up_Avg', 'IR01Dn_Avg','IR01UpCo_Avg', 'IR01DnCo_Avg']

# # colonnes_a_renommer_atampisi = {
# #     'Rain_01_mm_Tot': 'Rain_01_mm',
# #     'Rain_02_mm_Tot': 'Rain_02_mm',
# #     'AirTC_Avg': 'Air_Temp_Deg_C',
# #     'RH': 'Rel_H_%',
# #     'SlrW_Avg': 'Solar_R_W/m^2',
# #     'WS_ms_Avg': 'Wind_Sp_m/sec',
# #     'WindDir': 'Wind_Dir_Deg',
# #     'Date': 'Datetime' # Renommer 'Date' en 'Datetime' pour compatibilité
# # }
# # # Note: Pour 'Atampisi', aucune colonne à supprimer n'est spécifiée dans votre requête,
# # # donc la fonction `filter_colonnes` gérera `delete_columns=None` par défaut.

# # # --- Fin des configurations pour les stations Vea Sissili ---


# # # Fonction utilitaire pour créer Rain_mm si nécessaire
# # def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
# #     """
# #     Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
# #     Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
# #     Si aucune des colonnes sources n'est présente, 'Rain_mm' n'est pas créée.
# #     """
# #     df_copy = df.copy()
# #     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
# #         df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
# #         warnings.warn("Colonne Rain_mm créée en fusionnant Rain_01_mm et Rain_02_mm.")
# #     elif 'Rain_01_mm' in df_copy.columns:
# #         df_copy['Rain_mm'] = df_copy['Rain_01_mm']
# #         warnings.warn("Colonne Rain_mm créée à partir de Rain_01_mm.")
# #     elif 'Rain_02_mm' in df_copy.columns:
# #         df_copy['Rain_mm'] = df_copy['Rain_02_mm']
# #         warnings.warn("Colonne Rain_mm créée à partir de Rain_02_mm.")
# #     else:
# #         # Si ni 'Rain_01_mm' ni 'Rain_02_mm' n'existe, ne pas créer 'Rain_mm'
# #         warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents. La colonne 'Rain_mm' ne sera pas créée.")
# #     return df_copy

# # def filter_colonnes(df: pd.DataFrame, renamed_columns: dict, delete_columns: list = None) -> pd.DataFrame:
# #     """
# #     Traite un DataFrame en renommant les colonnes, en supprimant des colonnes
# #     et en extrayant les composantes de date/heure si une colonne 'Date' est présente.

# #     Args:
# #         df (pd.DataFrame): DataFrame à traiter.
# #         renamed_columns (dict): Dictionnaire pour renommer les colonnes.
# #         delete_columns (list, optional): Liste des colonnes à supprimer. Defaults to None.

# #     Returns:
# #         pd.DataFrame: DataFrame nettoyé et enrichi.
# #     """
# #     df_copy = df.copy()

# #     # Conversion de Date en datetime et extraction des composantes
# #     # La colonne 'Date' peut être absente ou déjà renommée 'Datetime'
# #     # Utilisation de .get() pour éviter KeyError
# #     date_col_name = None
# #     if 'Date' in df_copy.columns:
# #         date_col_name = 'Date'
# #     elif 'Datetime' in df_copy.columns and 'Date' not in renamed_columns.values():
# #         # Si 'Datetime' existe déjà et n'est pas une cible de renommage pour 'Date'
# #         # On suppose qu'elle est déjà bien formée.
# #         pass
    
# #     if date_col_name:
# #         df_copy[date_col_name] = pd.to_datetime(df_copy[date_col_name], errors="coerce")
# #         df_copy.dropna(subset=[date_col_name], inplace=True)

# #         if not df_copy.empty:
# #             # Extraire année, mois, jour, heure, minute à partir de la colonne de date/heure source
# #             # Assurez-vous que les colonnes n'existent pas avant de les créer pour éviter les avertissements
# #             if 'Year' not in df_copy.columns:
# #                 df_copy["Year"] = df_copy[date_col_name].dt.year
# #             if 'Month' not in df_copy.columns:
# #                 df_copy["Month"] = df_copy[date_col_name].dt.month
# #             if 'Day' not in df_copy.columns:
# #                 df_copy["Day"] = df_copy[date_col_name].dt.day
# #             if 'Hour' not in df_copy.columns:
# #                 df_copy["Hour"] = df_copy[date_col_name].dt.hour
# #             if 'Minute' not in df_copy.columns:
# #                 df_copy["Minute"] = df_copy[date_col_name].dt.minute
# #         else:
# #             warnings.warn(f"Après la conversion de la colonne '{date_col_name}', le DataFrame est vide. Impossible d'extraire les composantes temporelles.")
# #     else:
# #         warnings.warn("Avertissement: Colonne 'Date' ou 'Datetime' absente ou déjà traitée. Composantes temporelles non extraites ici.")

# #     # Supprimer les colonnes spécifiées
# #     if delete_columns is not None:
# #         df_copy.drop(columns=delete_columns, inplace=True, errors='ignore')

# #     # Renommer les colonnes
# #     df_copy.rename(columns=renamed_columns, inplace=True, errors='ignore')

# #     return df_copy


# # def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
# #     """
# #     Crée ou valide la colonne 'Datetime' et ses composantes (Year, Month, Day, Hour, Minute, Date)
# #     à partir de colonnes existantes. Cette fonction est robuste pour différents formats.

# #     Args:
# #         df (pd.DataFrame): DataFrame d'entrée.
# #         bassin (str, optional): Nom du bassin (actuellement non utilisé, mais utile pour des logiques futures).

# #     Returns:
# #         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
# #     """
# #     df_copy = df.copy()

# #     # Si 'Datetime' est déjà un type datetime et n'a pas de NaT, on ne fait rien
# #     if pd.api.types.is_datetime64_any_dtype(df_copy.get('Datetime')) and not df_copy['Datetime'].isnull().any():
# #         # S'assurer que les composantes sont extraites si elles manquent
# #         if 'Year' not in df_copy.columns:
# #             df_copy['Year'] = df_copy['Datetime'].dt.year
# #         if 'Month' not in df_copy.columns:
# #             df_copy['Month'] = df_copy['Datetime'].dt.month
# #         if 'Day' not in df_copy.columns:
# #             df_copy['Day'] = df_copy['Datetime'].dt.day
# #         if 'Hour' not in df_copy.columns:
# #             df_copy['Hour'] = df_copy['Datetime'].dt.hour
# #         if 'Minute' not in df_copy.columns:
# #             df_copy['Minute'] = df_copy['Datetime'].dt.minute
# #         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
# #              df_copy['Date'] = df_copy['Datetime'].dt.date
# #         return df_copy

# #     # Si 'Date' existe, tenter de la convertir en 'Datetime'
# #     if 'Date' in df_copy.columns:
# #         try:
# #             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
# #             df_copy.dropna(subset=['Datetime'], inplace=True) # Supprimer les lignes avec Datetime invalide
# #         except Exception as e:
# #             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime pour le bassin {bassin}: {e}")
# #             df_copy['Datetime'] = pd.NaT # Assigner NaT si conversion échoue
    
# #     # Sinon, tenter de créer 'Datetime' à partir de Year, Month, Day, Hour, Minute
# #     else:
# #         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
# #         existing_date_components = [col for col in date_cols if col in df_copy.columns]
        
# #         if not existing_date_components:
# #             warnings.warn("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute, Date) trouvée. 'Datetime' ne sera pas créé.")
# #             return df_copy # Retourner le DataFrame original si aucune composante de date

# #         try:
# #             # Convertir les colonnes de date/heure en numérique, remplir les NaN avec 0 pour la conversion string
# #             for col in date_cols:
# #                 if col in df_copy.columns:
# #                     df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce').fillna(0).astype(int)

# #             # Créer une chaîne de caractères pour la conversion en Datetime
# #             # Utilisation de .get() pour gérer les colonnes potentiellement manquantes avec des valeurs par défaut
# #             df_copy['temp_date_str'] = df_copy.apply(
# #                 lambda row: f"{int(row.get('Year', 2000)):04d}-"
# #                             f"{int(row.get('Month', 1)):02d}-"
# #                             f"{int(row.get('Day', 1)):02d} "
# #                             f"{int(row.get('Hour', 0)):02d}:"
# #                             f"{int(row.get('Minute', 0)):02d}",
# #                 axis=1
# #             )
# #             df_copy['Datetime'] = pd.to_datetime(df_copy['temp_date_str'], errors='coerce')
# #             df_copy.drop(columns=['temp_date_str'], inplace=True, errors='ignore')
# #             df_copy.dropna(subset=['Datetime'], inplace=True) # Supprimer les lignes avec Datetime invalide
            
# #         except Exception as e:
# #             warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
# #             df_copy['Datetime'] = pd.NaT # Assigner NaT si conversion échoue
            
# #     # Extraction finale des composantes si 'Datetime' a été créé/mis à jour
# #     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
# #         df_copy['Year'] = df_copy['Datetime'].dt.year
# #         df_copy['Month'] = df_copy['Datetime'].dt.month
# #         df_copy['Day'] = df_copy['Datetime'].dt.day
# #         df_copy['Hour'] = df_copy['Datetime'].dt.hour
# #         df_copy['Minute'] = df_copy['Datetime'].dt.minute
# #         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
# #              df_copy['Date'] = df_copy['Datetime'].dt.date
# #     else:
# #         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

# #     return df_copy

# # def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
# #     """
# #     Effectue toutes les interpolations météorologiques en une seule passe.
# #     Cette fonction DOIT recevoir un DataFrame avec un DatetimeIndex.
# #     Il doit également contenir une colonne 'Station'.

# #     Args:
# #         df (pd.DataFrame): Le DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# #         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
# #         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
# #                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

# #     Returns:
# #         pd.DataFrame: Le DataFrame original avec les données interpolées,
# #                       la colonne 'Is_Daylight' calculée, la durée du jour, et un DatetimeIndex.
# #     """
# #     df_processed = df.copy()

# #     # Assurez-vous que l'index est bien DatetimeIndex et nettoyez les NaT
# #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# #         raise TypeError("Le DataFrame d'entrée pour l'interpolation DOIT avoir un DatetimeIndex.")
    
# #     initial_rows = len(df_processed)
# #     df_processed = df_processed[df_processed.index.notna()]
# #     if len(df_processed) == 0:
# #         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
# #     if initial_rows - len(df_processed) > 0:
# #         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")
    
# #     print(f"DEBUG (interpolation): Type de l'index du DataFrame initial: {type(df_processed.index)}")
# #     print(f"DEBUG (interpolation): Premières 5 valeurs de l'index après nettoyage des NaT: {df_processed.index[:5].tolist() if not df_processed.empty else 'DataFrame vide'}")

# #     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
# #     if not all(col in df_gps.columns for col in required_gps_cols):
# #         raise ValueError(
# #             f"df_gps doit contenir les colonnes {required_gps_cols}. "
# #             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
# #         )

# #     if not df_gps['Station'].is_unique:
# #         print("Avertissement: La colonne 'Station' dans df_gps contient des noms de station dupliqués.")
# #         print("Ceci peut entraîner des comportements inattendus ou des stations non reconnues.")
# #         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
# #         print(f"Suppression de {len(df_gps) - len(df_gps_unique)} doublons dans df_gps (en gardant la première occurrence).")
# #     else:
# #         df_gps_unique = df_gps.copy()

# #     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

# #     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
# #     for col in numerical_cols:
# #         if col in df_processed.columns:
# #             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

# #     df_processed_parts = []

# #     for station_name, group in df_processed.groupby('Station'):
# #         group_copy = group.copy()
# #         print(f"DEBUG (interpolation/groupby): Début du traitement du groupe '{station_name}'.")
        
# #         # Standardize group_copy.index to UTC first
# #         # This block ensures the index is UTC-aware before proceeding
# #         if group_copy.index.tz is None:
# #             group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
# #         elif group_copy.index.tz != pytz.utc: # If it's already tz-aware but not UTC, convert to UTC
# #             group_copy.index = group_copy.index.tz_convert('UTC')
# #         print(f"DEBUG (interpolation/groupby): Index Datetime pour '{station_name}' STANDARDISÉ à UTC. Dtype: {group_copy.index.dtype}")
        
# #         # S'assurer que l'index n'a pas de NaT après localisation
# #         group_copy = group_copy[group_copy.index.notna()]
# #         if group_copy.empty:
# #             warnings.warn(f"Le groupe '{station_name}' est vide après nettoyage de l'index Datetime. Il sera ignoré.")
# #             continue


# #         # --- Calculs Astral en utilisant le fuseau horaire local ---
# #         apply_fixed_daylight = True
# #         gps_data = gps_info_dict.get(station_name)
# #         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
# #             lat = gps_data['Lat']
# #             long = gps_data['Long']
# #             timezone_str = gps_data['Timezone']

# #             try:
# #                 local_tz = pytz.timezone(timezone_str)
# #                 # Create a local timezone-aware version of the index for Astral calculations
# #                 index_for_astral_local = group_copy.index.tz_convert(local_tz)

# #                 daily_sun_info = {}
# #                 # Get unique dates from the local-time index (these are timezone-aware Timestamps)
# #                 # Utilisation de .drop_duplicates() au lieu de .unique() pour garantir un objet Pandas
# #                 unique_dates_ts_local = index_for_astral_local.normalize().drop_duplicates()

# #                 # Ensure unique_dates is not empty before processing
# #                 if unique_dates_ts_local.empty: # Utilisez .empty pour les objets Pandas
# #                     raise ValueError("No unique dates found for Astral calculation.")
                
# #                 for ts_local_aware in unique_dates_ts_local: # Iterate over timezone-aware Timestamps
# #                     loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
                    
# #                     # Convert the timezone-aware Timestamp to a naive datetime.date object
# #                     # This is what Astral's sun.sun function expects for its 'date' parameter
# #                     # This conversion helps avoid potential re-localization warnings from Astral/pytz.
# #                     naive_date_for_astral = ts_local_aware.to_pydatetime().date()
                    
# #                     # Pass the NAIVE date object to Astral. Astral's observer handles the timezone internally.
# #                     s = sun.sun(loc.observer, date=naive_date_for_astral) 
# #                     daily_sun_info[naive_date_for_astral] = {
# #                         'sunrise': s['sunrise'],
# #                         'sunset': s['sunset']
# #                     }

# #                 # Explicitly create a list of naive dates for the DataFrame index
# #                 naive_unique_dates_for_index = [ts.date() for ts in unique_dates_ts_local] # Use .date() here as keys are naive
# #                 temp_df_sun_index = pd.Index(naive_unique_dates_for_index, name='Date_Local_Naive')
# #                 temp_df_sun = pd.DataFrame(index=temp_df_sun_index)
                
# #                 # NOUVEAUX DÉBOGAGES pour comprendre le type juste avant l'opération
# #                 print(f"DEBUG (astral_calc): unique_dates_ts_local type: {type(unique_dates_ts_local)}")
# #                 print(f"DEBUG (astral_calc): naive_unique_dates_for_index type: {type(naive_unique_dates_for_index)}")
# #                 print(f"DEBUG (astral_calc): temp_df_sun_index type: {type(temp_df_sun_index)}")
# #                 if not temp_df_sun.empty:
# #                     print(f"DEBUG (astral_calc): First element of temp_df_sun.index: {temp_df_sun.index[0]}")
# #                     print(f"DEBUG (astral_calc): Type of first element of temp_df_sun.index: {type(temp_df_sun.index[0])}")

# #                 # Correction: Utilisation de la compréhension de liste pour éviter le problème de .map()
# #                 temp_df_sun['sunrise_time_local'] = [daily_sun_info.get(date, {}).get('sunrise') for date in temp_df_sun.index]
# #                 temp_df_sun['sunset_time_local'] = [daily_sun_info.get(date, {}).get('sunset') for date in temp_df_sun.index]

# #                 # Merge with group_copy (which has UTC index)
# #                 # To merge, create a normalized local date column (naive) in group_copy
# #                 group_copy_reset = group_copy.reset_index()
# #                 group_copy_reset['Date_Local_Naive'] = group_copy_reset['Datetime'].dt.tz_convert(local_tz).dt.date

# #                 group_copy_reset = pd.merge(group_copy_reset, temp_df_sun, on='Date_Local_Naive', how='left')

# #                 # Convert merged local times back to UTC for comparison with original 'Datetime' (which is UTC)
# #                 group_copy_reset['sunrise_time_utc'] = group_copy_reset['sunrise_time_local'].dt.tz_convert('UTC')
# #                 group_copy_reset['sunset_time_utc'] = group_copy_reset['sunset_time_local'].dt.tz_convert('UTC')

# #                 group_copy_reset.loc[:, 'Is_Daylight'] = (group_copy_reset['Datetime'] >= group_copy_reset['sunrise_time_utc']) & \
# #                                                           (group_copy_reset['Datetime'] < group_copy_reset['sunset_time_utc'])

# #                 daylight_timedelta_local = group_copy_reset['sunset_time_local'] - group_copy_reset['sunrise_time_local']
                
# #                 # Correction ici pour formater la durée en HH:MM:SS
# #                 def format_timedelta_to_hms(td):
# #                     if pd.isna(td):
# #                         return np.nan
# #                     total_seconds = int(td.total_seconds())
# #                     hours = total_seconds // 3600
# #                     minutes = (total_seconds % 3600) // 60
# #                     seconds = total_seconds % 60
# #                     return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

# #                 group_copy_reset.loc[:, 'Daylight_Duration'] = daylight_timedelta_local.apply(format_timedelta_to_hms)


# #                 group_copy = group_copy_reset.set_index('Datetime')
# #                 group_copy = group_copy.drop(columns=['Date_Local_Naive', 'sunrise_time_local', 'sunset_time_local', 'sunrise_time_utc', 'sunset_time_utc'], errors='ignore')

# #                 print(f"Lever et coucher du soleil calculés pour {station_name}.")
# #                 apply_fixed_daylight = False

# #             except Exception as e:
# #                 print(f"Erreur lors du calcul du lever/coucher du soleil avec Astral pour {station_name}: {e}.")
# #                 traceback.print_exc() # Print full traceback for this specific error
# #                 warnings.warn(f"Calcul Astral impossible pour '{station_name}'. Utilisation de l'indicateur jour/nuit fixe.")
# #                 apply_fixed_daylight = True
# #         else:
# #             print(f"Avertissement: Coordonnées ou Fuseau horaire manquants/invalides pour le site '{station_name}' dans df_gps. Utilisation de l'indicateur jour/nuit fixe.")
# #             apply_fixed_daylight = True

# #         if apply_fixed_daylight:
# #             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
# #             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00" # Règle fixe pour la durée
# #             print(f"Utilisation de l'indicateur jour/nuit fixe (7h-18h) pour {station_name}.")

# #         df_processed_parts.append(group_copy)

# #     if not df_processed_parts:
# #         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

# #     # Concaténation de toutes les parties. Puisque chaque partie a un DatetimeIndex,
# #     # nous pouvons concaténer directement sans ignore_index=True
# #     df_final = pd.concat(df_processed_parts)
# #     # Assurez-vous que l'index est trié et qu'il est bien nommé 'Datetime'
# #     df_final = pd.concat(df_processed_parts).sort_index()
# #     df_final.index.name = 'Datetime' 
# #     print(f"DEBUG (interpolation/concat): Index du DataFrame final après concaténation et tri: {type(df_final.index)}")
# #     print(f"DEBUG (interpolation/concat): Colonnes du DataFrame final après concaténation: {df_final.columns.tolist()}")


# #     # Nettoyage des colonnes temporaires (elles ne sont plus nécessaires ou n'existent plus)
# #     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date']
# #     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

# #     # Gestion intelligente de Rain_mm : Appel de la fonction modifiée
# #     df_final = create_rain_mm(df_final)

# #     # Note : Si create_rain_mm ne crée pas la colonne, elle ne sera pas présente ici.
# #     # Les autres parties du code devront gérer l'absence de 'Rain_mm'.

# #     # Interpolation standard et bornage pour les variables numériques
# #     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
# #                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
# #                       'Solar_R_W/m^2', 'Wind_Dir_Deg']

# #     for var in standard_vars:
# #         if var in df_final.columns: # Vérifier si la colonne existe avant de la traiter
# #             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
# #             if var in limits:
# #                 min_val = limits[var]['min']
# #                 max_val = limits[var]['max']
# #                 initial_nan_count = df_final[var].isna().sum()
# #                 if min_val is not None:
# #                     df_final.loc[df_final[var] < min_val, var] = np.nan
# #                 if max_val is not None:
# #                     df_final.loc[df_final[var] > max_val, var] = np.nan
                
# #                 new_nan_count = df_final[var].isna().sum()
# #                 if new_nan_count > initial_nan_count:
# #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
# #             print(f"DEBUG (interpolation/variable): Interpolation de '{var}'. Type de l'index de df_final: {type(df_final.index)}")
            
# #             # Interpolation seulement si l'index est bien DatetimeIndex
# #             if isinstance(df_final.index, pd.DatetimeIndex):
# #                 df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
# #             else:
# #                 print(f"Avertissement (interpolation/variable): L'index n'est pas un DatetimeIndex pour l'interpolation de '{var}'. Utilisation de la méthode 'linear'.")
# #                 df_final[var] = df_final[var].interpolate(method='linear', limit_direction='both')
# #             df_final[var] = df_final[var].bfill().ffill()

# #     # Interpolation CONDITIONNELLE de la radiation solaire
# #     if 'Solar_R_W/m^2' in df_final.columns: # Vérifier si la colonne existe
# #         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

# #         if 'Solar_R_W/m^2' in limits:
# #             min_val = limits['Solar_R_W/m^2']['min']
# #             max_val = limits['Solar_R_W/m^2']['max']
# #             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
# #             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
# #             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
# #                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

# #         if 'Is_Daylight' in df_final.columns:
# #             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

# #             if 'Rain_mm' in df_final.columns: # Vérifier l'existence de 'Rain_mm'
# #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
# #             else:
# #                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
# #                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
# #             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

# #             print(f"DEBUG (interpolation/solaire): Interpolation de 'Solar_R_W/m^2' (conditionnel). Type de l'index de df_final: {type(df_final.index)}")

# #             is_day = df_final['Is_Daylight']
# #             if isinstance(df_final.index, pd.DatetimeIndex):
# #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
# #             else:
# #                 print(f"Avertissement (interpolation/solaire): L'index n'est pas un DatetimeIndex pour l'interpolation de 'Solar_R_W/m^2'. Utilisation de la méthode 'linear'.")
# #                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both')

# #             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

# #             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
# #             warnings.warn("Radiation solaire interpolée avec succès.")
# #         else:
# #             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
# #             if isinstance(df_final.index, pd.DatetimeIndex):
# #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()
# #             else:
# #                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both').bfill().ffill()

# #     warnings.warn("Vérification des valeurs manquantes après interpolation:")
# #     missing_after_interp = df_final.isna().sum()
# #     columns_with_missing = missing_after_interp[missing_after_interp > 0]
# #     if not columns_with_missing.empty:
# #         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
# #     else:
# #         warnings.warn("Aucune valeur manquante après l'interpolation.")

# #     return df_final


# # def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
# #     """
# #     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
# #     de coordonnées UTM vers latitude/longitude WGS84.

# #     Args:
# #         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

# #     Returns:
# #         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
# #     """
# #     df_copy = df.copy()

# #     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
# #     if not all(col in df_copy.columns for col in required_utm_cols):
# #         raise ValueError(
# #             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
# #         )

# #     def convert_row(row):
# #         try:
# #             zone = int(row['zone'])
# #             hemisphere = str(row['hemisphere']).upper()
# #             is_northern = hemisphere == 'N'

# #             proj_utm = CRS.from_proj4(
# #                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
# #             )
# #             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

# #             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
# #             lon, lat = transformer.transform(row['Easting'], row['Northing'])
# #             return pd.Series({'Long': lon, 'Lat': lat})
# #         except Exception as e:
# #             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
# #             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

# #     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
# #     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

# #     return df_copy

# # def _load_and_prepare_gps_data() -> pd.DataFrame:
# #     """
# #     Charge les fichiers de coordonnées des stations depuis Google Drive,
# #     les prétraite (suppression/ajout de colonnes/lignes, renommage),
# #     convertit les coordonnées UTM en GPS pour Dano et Dassari,
# #     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

# #     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
# #     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

# #     Returns:
# #         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
# #                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
# #     """
# #     print("Début de la préparation des données de coordonnées des stations...")
# #     data_dir = 'data'
# #     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

# #     files_info = [
# #         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
# #         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
# #         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
# #     ]

# #     loaded_dfs = []

# #     for file_info in files_info:
# #         output_file_path = os.path.join(data_dir, file_info['name'])
        
# #         if not os.path.exists(output_file_path):
# #             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
# #             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
# #             print(f"Téléchargement de {file_info['bassin']} terminé.")
# #         else:
# #             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
# #         loaded_dfs.append(pd.read_excel(output_file_path))

# #     vea_sissili_bassin = loaded_dfs[0]
# #     dano_bassin = loaded_dfs[1]
# #     dassari_bassin = loaded_dfs[2]

# #     # Prétraitement des DataFrames (votre code original)
# #     print("Début du prétraitement des données de stations...")
    
# #     # Vea Sissili
# #     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
# #     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
# #     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

# #     # Dassari
# #     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
# #     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
# #     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

# #     # Dano et Dassari : Renommage et ajout des colonnes UTM
# #     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
# #     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

# #     dano_bassin['zone'] = 30
# #     dano_bassin['hemisphere'] = 'N'
# #     dassari_bassin['zone'] = 31
# #     dassari_bassin['hemisphere'] = 'N'

# #     # Application de la fonction de conversion UTM vers GPS
# #     dano_bassin = convert_utm_df_to_gps(dano_bassin)
# #     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

# #     # Ajout des fuseaux horaires
# #     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
# #     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
# #     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

# #     # Fusion de tous les bassins
# #     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

# #     # Renommer 'Name' en 'Station'
# #     bassins = bassins.rename(columns={'Name': 'Station'})

# #     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
# #     initial_rows = len(bassins)
# #     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
# #     if len(bassins) < initial_rows:
# #         print(f"Attention: {initial_rows - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
# #     # 5. Sauvegarde du DataFrame final en JSON
# #     output_json_path = os.path.join(data_dir, "station_coordinates.json")
# #     # Utiliser to_json avec orient='records' pour un format plus lisible et facile à charger
# #     bassins.to_json(output_json_path, orient='records', indent=4)
# #     print(f"\nPréparation des données terminée. Coordonnées des stations sauvegardées dans '{output_json_path}'.")
# #     print("Vous pouvez maintenant lancer votre application Flask.")

# #     return bassins # Retourner le DataFrame des données GPS


# # def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
# #     """
# #     Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
# #     Conserve la première occurrence en cas de doublon.

# #     Args:
# #         df (pd.DataFrame): Le DataFrame d'entrée.

# #     Returns:
# #         pd.DataFrame: Le DataFrame sans doublons.
# #     """
# #     if 'Station' in df.columns and 'Datetime' in df.columns:
# #         initial_rows = len(df)
# #         df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
# #         if len(df_cleaned) < initial_rows:
# #             warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
# #         return df_cleaned
# #     else:
# #         warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
# #         return df

# # def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
# #     """
# #     Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

# #     Args:
# #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
# #         limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

# #     Returns:
# #         pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
# #     """
# #     df_processed = df.copy()

# #     if not isinstance(df_processed.index, pd.DatetimeIndex):
# #         warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
# #         try:
# #             df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
# #             df_processed = df_processed[df_processed.index.notna()]
# #             if df_processed.empty:
# #                 raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
# #         except Exception as e:
# #             raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

# #     for var, vals in limits.items():
# #         if var in df_processed.columns:
# #             min_val = vals.get('min')
# #             max_val = vals.get('max')
# #             if min_val is not None or max_val is not None:
# #                 initial_nan_count = df_processed[var].isna().sum()
# #                 if min_val is not None:
# #                     df_processed.loc[df_processed[var] < min_val, var] = np.nan
# #                 if max_val is not None:
# #                     df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
# #                 new_nan_count = df_processed[var].isna().sum()
# #                 if new_nan_count > initial_nan_count:
# #                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
# #     return df_processed

# # def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
# #     """
# #     Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
# #     Retourne l'objet Figure Plotly.
# #     """
# #     if not isinstance(df.index, pd.DatetimeIndex):
# #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

# #     filtered_df = df[df['Station'] == station].copy()
# #     if filtered_df.empty:
# #         return go.Figure() # Retourne une figure vide si pas de données

# #     if periode == 'Journalière':
# #         resampled_df = filtered_df[variable].resample('D').mean()
# #     elif periode == 'Hebdomadaire':
# #         resampled_df = filtered_df[variable].resample('W').mean()
# #     elif periode == 'Mensuelle':
# #         resampled_df = filtered_df[variable].resample('M').mean()
# #     elif periode == 'Annuelle':
# #         resampled_df = filtered_df[variable].resample('Y').mean()
# #     else: # Données Brutes
# #         resampled_df = filtered_df[variable]

# #     resampled_df = resampled_df.dropna()

# #     if resampled_df.empty:
# #         return go.Figure() # Retourne une figure vide si pas de données après resample/dropna

# #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# #     color = colors.get(station, '#1f77b4')

# #     fig = go.Figure()
# #     fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# #                              mode='lines', name=f'{variable_meta["Nom"]} - {station}',
# #                              line=dict(color=color)))

# #     fig.update_layout(
# #         title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
# #         xaxis_title="Date",
# #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# #         hovermode="x unified",
# #         template="plotly_white" # Utiliser un template Plotly plus clair
# #     )
# #     return fig

# # def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
# #     """
# #     Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
# #     Retourne l'objet Figure Plotly.
# #     """
# #     if not isinstance(df.index, pd.DatetimeIndex):
# #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

# #     fig = go.Figure()
    
# #     all_stations = df['Station'].unique()
# #     if len(all_stations) < 2:
# #         warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
# #         return go.Figure() # Retourne une figure vide si moins de 2 stations

# #     for station in all_stations:
# #         filtered_df = df[df['Station'] == station].copy()
# #         if filtered_df.empty:
# #             continue

# #         if periode == 'Journalière':
# #             resampled_df = filtered_df[variable].resample('D').mean()
# #         elif periode == 'Hebdomadaire':
# #             resampled_df = filtered_df[variable].resample('W').mean()
# #         elif periode == 'Mensuelle':
# #             resampled_df = filtered_df[variable].resample('M').mean()
# #         elif periode == 'Annuelle':
# #             resampled_df = filtered_df[variable].resample('Y').mean()
# #         else: # Données Brutes
# #             resampled_df = filtered_df[variable]

# #         resampled_df = resampled_df.dropna()
# #         if resampled_df.empty:
# #             continue
        
# #         color = colors.get(station, '#1f77b4')
# #         fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
# #                                  mode='lines', name=station,
# #                                  line=dict(color=color)))

# #     if not fig.data:
# #         return go.Figure() # Retourne une figure vide si aucune trace n'a été ajoutée

# #     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
# #     fig.update_layout(
# #         title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
# #         xaxis_title="Date",
# #         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
# #         hovermode="x unified",
# #         legend_title="Variables",
# #         template="plotly_white"
# #     )
# #     return fig


# # def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> go.Figure:
# #     """
# #     Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
# #     Retourne l'objet Figure Plotly.
# #     """
# #     if not isinstance(df.index, pd.DatetimeIndex):
# #         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

# #     filtered_df = df[df['Station'] == station].copy()
# #     if filtered_df.empty:
# #         return go.Figure() # Retourne une figure vide si pas de données

# #     numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight', 'Daylight_Duration']]

# #     if not numerical_vars:
# #         warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
# #         return go.Figure() # Retourne une figure vide si pas de variables numériques

# #     normalized_df = filtered_df[numerical_vars].copy()
# #     for col in normalized_df.columns:
# #         min_val = normalized_df[col].min()
# #         max_val = normalized_df[col].max()
# #         if max_val != min_val:
# #             normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
# #         else:
# #             normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

# #     normalized_df = normalized_df.dropna(how='all')

# #     if normalized_df.empty:
# #         return go.Figure() # Retourne une figure vide si pas de données après normalisation/dropna
    
# #     fig = go.Figure()
# #     for var in normalized_df.columns:
# #         var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
# #         color = colors.get(var, None)

# #         fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
# #                                  mode='lines', name=var_meta['Nom'],
# #                                  line=dict(color=color)))

# #     fig.update_layout(
# #         title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
# #         xaxis_title="Date",
# #         yaxis_title="Valeur Normalisée (0-1)",
# #         hovermode="x unified",
# #         legend_title="Variables",
# #         template="plotly_white"
# #     )
# #     return fig

# # def calculate_daily_summary_table(df: pd.DataFrame) -> pd.DataFrame:
# #     """
# #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# #     groupées par station. Cette fonction renvoie un DataFrame de statistiques, non un graphique.
# #     C'est la fonction qui remplace l'ancienne 'daily_stats'.

# #     Args:
# #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# #     Returns:
# #         pd.DataFrame: DataFrame avec les statistiques journalières.
# #     """
# #     df_copy = df.copy()

# #     # Assurez-vous que 'Datetime' est une colonne et de type datetime
# #     if isinstance(df_copy.index, pd.DatetimeIndex):
# #         df_copy = df_copy.reset_index()

# #     df_copy['Datetime'] = pd.to_datetime(df_copy['Datetime'], errors='coerce')
# #     df_copy = df_copy.dropna(subset=['Datetime', 'Station'])

# #     if df_copy.empty:
# #         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans calculate_daily_summary_table.")
# #         return pd.DataFrame()

# #     if 'Is_Daylight' not in df_copy.columns:
# #         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
# #         df_copy['Is_Daylight'] = (df_copy['Datetime'].dt.hour >= 7) & (df_copy['Datetime'].dt.hour <= 18)

# #     numerical_cols = [col for col in df_copy.columns if pd.api.types.is_numeric_dtype(df_copy[col]) and col not in ['Station', 'Datetime', 'Is_Daylight', 'Daylight_Duration']]
    
# #     if not numerical_cols:
# #         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
# #         return pd.DataFrame()

# #     # Calcul des statistiques de base par jour
# #     daily_aggregated_df = df_copy.groupby(['Station', df_copy['Datetime'].dt.date]).agg({
# #         col: ['mean', 'min', 'max'] for col in numerical_cols if METADATA_VARIABLES.get(col, {}).get('is_rain') == False
# #     })

# #     # Renommage des colonnes agrégées pour les non-pluies
# #     daily_aggregated_df.columns = ['_'.join(col).strip() for col in daily_aggregated_df.columns.values]


# #     # Traitement spécifique pour la pluie (Rain_mm)
# #     if 'Rain_mm' in df_copy.columns and METADATA_VARIABLES.get('Rain_mm', {}).get('is_rain'):
# #         df_daily_rain = df_copy.groupby(['Station', df_copy['Datetime'].dt.date])['Rain_mm'].sum().reset_index()
# #         df_daily_rain = df_daily_rain.rename(columns={'Rain_mm': 'Rain_mm_sum'})

# #         # Fusionner les statistiques de pluie avec les autres
# #         if not daily_aggregated_df.empty:
# #             daily_aggregated_df = daily_aggregated_df.reset_index()
# #             daily_stats_df = pd.merge(daily_aggregated_df, df_daily_rain, on=['Station', 'Datetime'], how='left')
# #             daily_stats_df = daily_stats_df.rename(columns={'Datetime': 'Date'})
# #         else:
# #             daily_stats_df = df_daily_rain.rename(columns={'Datetime': 'Date'})
# #     else:
# #         daily_stats_df = daily_aggregated_df.reset_index().rename(columns={'Datetime': 'Date'})


# #     # Calcul des statistiques de saison et de sécheresse pour la pluie
# #     if 'Rain_mm' in df_copy.columns and METADATA_VARIABLES.get('Rain_mm', {}).get('is_rain'):
# #         df_daily_rain_raw = df_copy.groupby(['Station', pd.Grouper(key='Datetime', freq='D')])['Rain_mm'].sum().reset_index()
        
# #         RAIN_SEASON_GAP_THRESHOLD = pd.Timedelta(days=60)
# #         season_stats = []
# #         for station_name, station_df_rain in df_daily_rain_raw.groupby('Station'):
# #             station_df_rain = station_df_rain.set_index('Datetime').sort_index()
# #             rain_events = station_df_rain[station_df_rain['Rain_mm'] > 0].index

# #             if rain_events.empty:
# #                 season_stats.append({'Station': station_name, 'Moyenne_Saison_Pluvieuse': np.nan, 'Debut_Saison_Pluvieuse': pd.NaT, 'Fin_Saison_Pluvieuse': pd.NaT, 'Duree_Saison_Pluvieuse_Jours': np.nan})
# #                 continue
            
# #             block_ids = (rain_events.to_series().diff() > RAIN_SEASON_GAP_THRESHOLD).cumsum()
# #             valid_blocks = {}
# #             for block_id, rain_dates_in_block in rain_events.to_series().groupby(block_ids):
# #                 if not rain_dates_in_block.empty:
# #                     block_start = rain_dates_in_block.min()
# #                     block_end = rain_dates_in_block.max()
# #                     full_block_df = station_df_rain.loc[block_start:block_end]
# #                     valid_blocks[block_id] = full_block_df

# #             if not valid_blocks:
# #                 season_stats.append({'Station': station_name, 'Moyenne_Saison_Pluvieuse': np.nan, 'Debut_Saison_Pluvieuse': pd.NaT, 'Fin_Saison_Pluvieuse': pd.NaT, 'Duree_Saison_Pluvieuse_Jours': np.nan})
# #                 continue

# #             main_block_id = max(valid_blocks, key=lambda k: (valid_blocks[k].index.max() - valid_blocks[k].index.min()).days)
# #             main_season_df = valid_blocks[main_block_id]

# #             debut_saison = main_season_df.index.min()
# #             fin_saison = main_season_df.index.max()
# #             total_days = (fin_saison - debut_saison).days + 1
# #             moyenne_saison = main_season_df['Rain_mm'].sum() / total_days if total_days > 0 else 0

# #             season_stats.append({
# #                 'Station': station_name,
# #                 'Moyenne_Saison_Pluvieuse': moyenne_saison,
# #                 'Debut_Saison_Pluvieuse': debut_saison,
# #                 'Fin_Saison_Pluvieuse': fin_saison,
# #                 'Duree_Saison_Pluvieuse_Jours': total_days
# #             })
# #         df_season_stats = pd.DataFrame(season_stats)
        
# #         # Merge season stats into main daily_stats_df
# #         if not df_season_stats.empty:
# #             daily_stats_df = pd.merge(daily_stats_df, df_season_stats, on='Station', how='left')


# #     # Calcul des statistiques globales (Max, Min, Moyenne, Médiane, etc.) par station
# #     # pour les variables numériques, à inclure dans le tableau récapitulatif.
# #     final_stats_per_station = pd.DataFrame()
# #     for station_name in df_copy['Station'].unique():
# #         station_df = df_copy[df_copy['Station'] == station_name].copy()
# #         station_summary = {'Station': station_name}

# #         for var in numerical_cols:
# #             if var in station_df.columns and pd.api.types.is_numeric_dtype(station_df[var]):
# #                 # Filtrage pour la radiation solaire (seulement pendant le jour)
# #                 if var == 'Solar_R_W/m^2':
# #                     var_data = station_df.loc[station_df['Is_Daylight'], var].dropna()
# #                 else:
# #                     var_data = station_df[var].dropna()
                
# #                 if not var_data.empty:
# #                     station_summary[f'{var}_Maximum'] = var_data.max()
# #                     station_summary[f'{var}_Minimum'] = var_data.min()
# #                     station_summary[f'{var}_Moyenne'] = var_data.mean()
# #                     station_summary[f'{var}_Mediane'] = var_data.median()
                    
# #                     # Pour Rain_mm uniquement
# #                     if var == 'Rain_mm':
# #                         station_summary[f'{var}_Cumul_Annuel'] = station_df['Rain_mm'].sum()
# #                         # Moyenne des jours pluvieux (seulement les jours où il a plu)
# #                         rainy_days_data = station_df[station_df['Rain_mm'] > 0]['Rain_mm'].dropna()
# #                         station_summary[f'{var}_Moyenne_Jours_Pluvieux'] = rainy_days_data.mean() if not rainy_days_data.empty else np.nan

# #                         # Ajouter la durée de la saison pluvieuse et de sécheresse du df_season_stats
# #                         if 'Duree_Saison_Pluvieuse_Jours' in daily_stats_df.columns:
# #                             s_data = daily_stats_df[daily_stats_df['Station'] == station_name]
# #                             if not s_data.empty:
# #                                 station_summary[f'{var}_Duree_Saison_Pluvieuse_Jours'] = s_data['Duree_Saison_Pluvieuse_Jours'].iloc[0]
# #                                 # Placeholder pour sécheresse si non calculé ailleurs
# #                                 station_summary[f'{var}_Duree_Secheresse_Definie_Jours'] = np.nan # Vous devrez calculer ceci plus tard

# #         final_stats_per_station = pd.concat([final_stats_per_station, pd.DataFrame([station_summary])], ignore_index=True)
        
# #     return final_stats_per_station # Retourne le DataFrame de statistiques agrégées par station


# # def generate_variable_summary_plots_for_web(df: pd.DataFrame, station: str, variable: str, metadata: dict, palette: dict) -> plt.Figure:
# #     """
# #     Génère un graphique Matplotlib/Seaborn pour les statistiques agrégées d'une variable spécifique
# #     pour une station donnée, en utilisant la logique fournie par l'utilisateur pour 'daily_stats'.

# #     Args:
# #         df (pd.DataFrame): Le DataFrame global de données traitées avec DatetimeIndex et colonne 'Station'.
# #         station (str): Le nom de la station à visualiser.
# #         variable (str): La variable à visualiser (e.g., 'Air_Temp_Deg_C', 'Rain_mm').
# #         metadata (dict): Dictionnaire de métadonnées pour les variables (Nom, Unite, is_rain).
# #         palette (dict): Dictionnaire de couleurs pour les différentes métriques statistiques.

# #     Returns:
# #         plt.Figure: Un objet Figure Matplotlib contenant tous les sous-graphiques pour la variable sélectionnée.
# #     """
# #     df_station = df[df['Station'] == station].copy()

# #     if df_station.empty:
# #         fig, ax = plt.subplots(figsize=(10, 6))
# #         ax.text(0.5, 0.5, f"Aucune donnée pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# #         ax.axis('off')
# #         return fig

# #     # S'assurer que Datetime est un index datetime correct pour les données de la station
# #     if isinstance(df_station.index, pd.DatetimeIndex):
# #         df_station = df_station.reset_index() # Réinitialise l'index pour un accès facile aux colonnes
    
# #     df_station['Datetime'] = pd.to_datetime(df_station['Datetime'], errors='coerce')
# #     df_station = df_station.dropna(subset=['Datetime', 'Station'])
# #     df_station = df_station.set_index('Datetime').sort_index() # Remet l'index pour les opérations de séries temporelles

# #     if df_station.empty:
# #         fig, ax = plt.subplots(figsize=(10, 6))
# #         ax.text(0.5, 0.5, f"DataFrame vide après nettoyage des dates pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# #         ax.axis('off')
# #         return fig

# #     # Assurez-vous que 'Is_Daylight' existe (généralement géré par interpolation, mais au cas où)
# #     if 'Is_Daylight' not in df_station.columns:
# #         df_station['Is_Daylight'] = (df_station.index.hour >= 7) & (df_station.index.hour <= 18)


# #     var_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})

# #     # Dictionnaire pour stocker les statistiques pour la variable sélectionnée
# #     stats_for_plot = {}
# #     metrics_to_plot = []
    
# #     # --- LOGIQUE SPÉCIFIQUE À RAIN_MM (transférée de la fonction daily_stats de l'utilisateur) ---
# #     if var_meta.get('is_rain', False) and variable == 'Rain_mm':
# #         # Vérifier si 'Rain_mm' existe avant de procéder aux calculs spécifiques à la pluie
# #         if 'Rain_mm' not in df_station.columns:
# #             fig, ax = plt.subplots(figsize=(10, 6))
# #             ax.text(0.5, 0.5, f"La variable 'Rain_mm' n'est pas disponible pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# #             ax.axis('off')
# #             return fig

# #         df_daily_rain = df_station.groupby(pd.Grouper(freq='D'))['Rain_mm'].sum().reset_index()
# #         df_daily_rain = df_daily_rain.rename(columns={'Rain_mm': 'Rain_mm_sum'})
# #         df_daily_rain['Datetime'] = pd.to_datetime(df_daily_rain['Datetime'])
# #         df_daily_rain = df_daily_rain.set_index('Datetime').sort_index()

# #         RAIN_SEASON_GAP_THRESHOLD = pd.Timedelta(days=60)
# #         rain_events = df_daily_rain[df_daily_rain['Rain_mm_sum'] > 0].index

# #         s_moyenne_saison = np.nan
# #         s_duree_saison = np.nan
# #         s_debut_saison = pd.NaT
# #         s_fin_saison = pd.NaT

# #         if not rain_events.empty:
# #             block_ids = (rain_events.to_series().diff() > RAIN_SEASON_GAP_THRESHOLD).cumsum()
# #             valid_blocks = {}
# #             for block_id, rain_dates_in_block in rain_events.to_series().groupby(block_ids):
# #                 if not rain_dates_in_block.empty:
# #                     block_start = rain_dates_in_block.min()
# #                     block_end = rain_dates_in_block.max()
# #                     full_block_df = df_daily_rain.loc[block_start:block_end]
# #                     valid_blocks[block_id] = full_block_df

# #             if valid_blocks:
# #                 main_block_id = max(valid_blocks, key=lambda k: (valid_blocks[k].index.max() - valid_blocks[k].index.min()).days)
# #                 main_season_df = valid_blocks[main_block_id]

# #                 s_debut_saison = main_season_df.index.min()
# #                 s_fin_saison = main_season_df.index.max()
# #                 total_days_season = (s_fin_saison - s_debut_saison).days + 1
# #                 s_moyenne_saison = main_season_df['Rain_mm_sum'].sum() / total_days_season if total_days_season > 0 else 0
# #                 s_duree_saison = total_days_season

# #         # Logique de détection de la sécheresse
# #         longest_dry_spell = np.nan
# #         debut_secheresse = pd.NaT
# #         fin_secheresse = pd.NaT

# #         full_daily_series_rain = df_daily_rain['Rain_mm_sum'].resample('D').sum().fillna(0)
# #         rainy_days_index = full_daily_series_rain[full_daily_series_rain > 0].index

# #         if not rainy_days_index.empty and pd.notna(s_moyenne_saison) and s_moyenne_saison > 0:
# #             temp_dry_spells = []
# #             for i in range(1, len(rainy_days_index)):
# #                 prev_rain_date = rainy_days_index[i-1]
# #                 current_rain_date = rainy_days_index[i]
# #                 dry_days_between_rains = (current_rain_date - prev_rain_date).days - 1

# #                 if dry_days_between_rains > 0:
# #                     rain_prev_day = full_daily_series_rain.loc[prev_rain_date]
# #                     temp_debut = pd.NaT
# #                     temp_duree = 0
# #                     for j in range(1, dry_days_between_rains + 1):
# #                         current_dry_date = prev_rain_date + timedelta(days=j)
# #                         current_ratio = rain_prev_day / j
# #                         if current_ratio < s_moyenne_saison: # Condition pour la sécheresse définie
# #                             temp_debut = current_dry_date
# #                             temp_duree = (current_rain_date - temp_debut).days
# #                             break
# #                     if pd.notna(temp_debut) and temp_duree > 0:
# #                         temp_dry_spells.append({
# #                             'Duree': temp_duree,
# #                             'Debut': temp_debut,
# #                             'Fin': current_rain_date - timedelta(days=1)
# #                         })
            
# #             if temp_dry_spells:
# #                 df_temp_dry = pd.DataFrame(temp_dry_spells)
# #                 idx_max_dry = df_temp_dry['Duree'].idxmax()
# #                 longest_dry_spell = df_temp_dry.loc[idx_max_dry, 'Duree']
# #                 debut_secheresse = df_temp_dry.loc[idx_max_dry, 'Debut']
# #                 fin_secheresse = df_temp_dry.loc[idx_max_dry, 'Fin']

# #         # Statistiques pour Rain_mm (tirées de la vue consolidée de daily_stats de l'utilisateur)
# #         rain_data_for_stats = df_station[variable].dropna()
        
# #         stats_for_plot['Maximum'] = rain_data_for_stats.max() if not rain_data_for_stats.empty else np.nan
# #         stats_for_plot['Minimum'] = rain_data_for_stats.min() if not rain_data_for_stats.empty else np.nan
# #         stats_for_plot['Mediane'] = rain_data_for_stats.median() if not rain_data_for_stats.empty else np.nan
# #         stats_for_plot['Cumul_Annuel'] = df_station[variable].sum()
        
# #         rainy_days_data = df_station[df_station[variable] > 0][variable].dropna()
# #         stats_for_plot['Moyenne_Jours_Pluvieux'] = rainy_days_data.mean() if not rainy_days_data.empty else np.nan
        
# #         stats_for_plot['Moyenne_Saison_Pluvieuse'] = s_moyenne_saison
# #         stats_for_plot['Duree_Saison_Pluvieuse_Jours'] = s_duree_saison
# #         stats_for_plot['Debut_Saison_Pluvieuse'] = s_debut_saison
# #         stats_for_plot['Fin_Saison_Pluvieuse'] = s_fin_saison
        
# #         stats_for_plot['Duree_Secheresse_Definie_Jours'] = longest_dry_spell
# #         stats_for_plot['Debut_Secheresse_Definie'] = debut_secheresse
# #         stats_for_plot['Fin_Secheresse_Definie'] = fin_secheresse

# #         # Récupérer les dates pour max/min pour Rain_mm
# #         max_date_idx = df_station[variable].idxmax() if not df_station[variable].empty else pd.NaT
# #         min_date_idx = df_station[variable].idxmin() if not df_station[variable].empty else pd.NaT
# #         stats_for_plot['Date_Maximum'] = max_date_idx if pd.notna(max_date_idx) else pd.NaT
# #         stats_for_plot['Date_Minimum'] = min_date_idx if pd.notna(min_date_idx) else pd.NaT
        
# #         metrics_to_plot = [
# #             'Maximum', 'Minimum', 'Cumul_Annuel', 'Mediane',
# #             'Moyenne_Jours_Pluvieux', 'Moyenne_Saison_Pluvieuse',
# #             'Duree_Saison_Pluvieuse_Jours', 'Duree_Secheresse_Definie_Jours'
# #         ]
# #         nrows, ncols = 4, 2
# #         figsize = (18, 16)
        
# #     # --- LOGIQUE POUR LES AUTRES VARIABLES (transférée de la fonction daily_stats de l'utilisateur) ---
# #     else:
# #         current_var_data = df_station[variable].dropna()
# #         if variable == 'Solar_R_W/m^2':
# #             current_var_data = df_station.loc[df_station['Is_Daylight'], variable].dropna()

# #         if current_var_data.empty:
# #             fig, ax = plt.subplots(figsize=(10, 6))
# #             ax.text(0.5, 0.5, f"Aucune donnée valide pour la variable {var_meta['Nom']} à {station}.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# #             ax.axis('off')
# #             return fig

# #         # Statistiques pour les autres variables (de daily_stats de l'utilisateur)
# #         stats_for_plot['Maximum'] = current_var_data.max()
# #         stats_for_plot['Minimum'] = current_var_data.min()
# #         stats_for_plot['Mediane'] = current_var_data.median()
# #         stats_for_plot['Moyenne'] = current_var_data.mean()

# #         # Récupérer les dates pour max/min
# #         max_idx = current_var_data.idxmax() if not current_var_data.empty else pd.NaT
# #         min_idx = current_var_data.idxmin() if not current_var_data.empty else pd.NaT

# #         stats_for_plot['Date_Maximum'] = max_idx if pd.notna(max_idx) else pd.NaT
# #         stats_for_plot['Date_Minimum'] = min_idx if pd.notna(min_idx) else pd.NaT

# #         metrics_to_plot = ['Maximum', 'Minimum', 'Moyenne', 'Mediane']
# #         nrows, ncols = 2, 2
# #         figsize = (18, 12)

# #     # Gérer le cas où stats_for_plot pourrait être vide (ex: toutes les données sont NaN)
# #     if not stats_for_plot:
# #         fig, ax = plt.subplots(figsize=(10, 6))
# #         ax.text(0.5, 0.5, f"Impossible de calculer des statistiques pour la variable '{variable}' à la station '{station}' (données manquantes ou non numériques).", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
# #         ax.axis('off')
# #         return fig

# #     # --- PARTIE COMMUNE DE TRACÉ (adaptée de la fonction daily_stats de l'utilisateur) ---
# #     fig, axes = plt.subplots(nrows, ncols, figsize=figsize)
# #     plt.subplots_adjust(hspace=0.6, wspace=0.4) # Ajustement de l'espacement pour un meilleur ajustement
# #     axes = axes.flatten()

# #     fig.suptitle(f'Statistiques de {var_meta["Nom"]} pour la station {station}', fontsize=16, y=0.98)

# #     for i, metric in enumerate(metrics_to_plot):
# #         ax = axes[i]
# #         value = stats_for_plot.get(metric)
# #         if pd.isna(value):
# #             ax.text(0.5, 0.5, "Données non disponibles", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=12, color='gray')
# #             ax.axis('off')
# #             continue

# #         color = palette.get(metric.replace(' ', '_'), '#cccccc') # Utilisation de la `palette` passée en argument
        
# #         # Créer un DataFrame factice pour sns.barplot, car il attend un DataFrame
# #         plot_data_bar = pd.DataFrame({'Metric': [metric.replace('_', ' ')], 'Value': [value]})
# #         sns.barplot(x='Metric', y='Value', data=plot_data_bar, ax=ax, color=color, edgecolor='none')

# #         # Ajouter les annotations
# #         text = ""
# #         if metric in ['Duree_Saison_Pluvieuse_Jours', 'Duree_Secheresse_Definie_Jours']:
# #             start_date_key = f'Debut_{metric.replace("Jours", "")}'
# #             end_date_key = f'Fin_{metric.replace("Jours", "")}'
# #             start_date = stats_for_plot.get(start_date_key)
# #             end_date = stats_for_plot.get(end_date_key)
# #             date_info = ""
# #             if pd.notna(start_date) and pd.notna(end_date):
# #                 date_info = f"\ndu {start_date.strftime('%d/%m/%Y')} au {end_date.strftime('%d/%m/%Y')}"
# #             text = f"{int(value)} j{date_info}"
# #         elif metric in ['Maximum', 'Minimum', 'Cumul_Annuel', 'Moyenne_Jours_Pluvieux', 'Moyenne_Saison_Pluvieuse', 'Mediane', 'Moyenne']:
# #             unit = var_meta['Unite']
# #             date_str = ''
# #             # Vérifier les clés de date pour max/min si elles existent dans stats_for_plot
# #             if (metric == 'Maximum' and 'Date_Maximum' in stats_for_plot and pd.notna(stats_for_plot['Date_Maximum'])):
# #                 date_str = f"\n({stats_for_plot['Date_Maximum'].strftime('%d/%m/%Y')})"
# #             elif (metric == 'Minimum' and 'Date_Minimum' in stats_for_plot and pd.notna(stats_for_plot['Date_Minimum'])):
# #                 date_str = f"\n({stats_for_plot['Date_Minimum'].strftime('%d/%m/%Y')})"
            
# #             text = f"{value:.1f} {unit}{date_str}"
# #         else:
# #             text = f"{value:.1f} {var_meta['Unite']}"

# #         # Positionne le texte sur le graphique.
# #         ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
# #                 text, ha='center', va='bottom', fontsize=9, color='black',
# #                 bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))
        
# #         ax.set_title(f"{var_meta['Nom']} {metric.replace('_', ' ')}", fontsize=11)
# #         ax.set_xlabel("")
# #         ax.set_ylabel(f"Valeur ({var_meta['Unite']})", fontsize=10)
# #         ax.tick_params(axis='x', rotation=0)
# #         ax.set_xticklabels([])

# #     # Si moins de graphiques que de sous-graphiques, désactiver les axes inutilisés
# #     for j in range(i + 1, len(axes)):
# #         fig.delaxes(axes[j])

# #     plt.tight_layout(rect=[0, 0, 1, 0.96])

# #     return fig


# # def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
# #     """
# #     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
# #     groupées par station.

# #     Args:
# #         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

# #     Returns:
# #         pd.DataFrame: DataFrame avec les statistiques journalières.
# #     """
# #     df = df.copy()

# #     # Assurez-vous que 'Datetime' est une colonne et de type datetime
# #     if isinstance(df.index, pd.DatetimeIndex):
# #         df = df.reset_index()

# #     df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')
# #     df = df.dropna(subset=['Datetime', 'Station'])

# #     if df.empty:
# #         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans daily_stats.")
# #         return pd.DataFrame()

# #     if 'Is_Daylight' not in df.columns:
# #         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
# #         df['Is_Daylight'] = (df['Datetime'].dt.hour >= 7) & (df['Datetime'].dt.hour <= 18)

# #     numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Datetime', 'Is_Daylight']]
    
# #     if not numerical_cols:
# #         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
# #         return pd.DataFrame()

# #     agg_funcs = {col: ['mean', 'min', 'max'] for col in numerical_cols}
    
# #     if 'Rain_mm' in numerical_cols:
# #         agg_funcs['Rain_mm'].append('sum')

# #     daily_stats_df = df.groupby(['Station', df['Datetime'].dt.date]).agg(agg_funcs)

# #     daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

# #     daily_stats_df = daily_stats_df.rename_axis(index={'Datetime': 'Date'})
# #     daily_stats_df = daily_stats_df.reset_index()

# #     return daily_stats_df


# import pandas as pd
# from pyproj import CRS, Transformer
# import pytz
# from astral.location import LocationInfo
# from astral import sun
# import numpy as np
# import warnings
# import os
# import gdown # Importation du module gdown pour le téléchargement
# import plotly.graph_objects as go # Importation pour gérer les objets Figure Plotly
# import matplotlib.pyplot as plt # NOUVEAUX IMPORTS pour Matplotlib
# import seaborn as sns
# import traceback # Importation de traceback pour les messages d'erreur détaillés
# import math # Ajouté pour math.ceil
# from datetime import timedelta # Ajouté pour timedelta

# # Importation des configurations et métadonnées depuis config.py
# from config import METADATA_VARIABLES, PALETTE_DEFAUT, DATA_LIMITS

# # --- NOUVELLES Configurations pour les stations Dano ---
# STATIONS_DANO_TAMBIRI_1 = ['Tambiri 1']
# COLONNES_SELECT_TAMBIRI_1 = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'AirTC_Avg', 'RH', 'WS_ms_S_WVT', 'WindDir_D1_WVT', 'Rain_mm_Tot', 'BP_mbar_Avg', 'Station']
# COLONNES_RENNOMAGE_SELECT_TAMBIRI_1 = {
#     'AirTC_Avg': 'Air_Temp_Deg_C',
#     'RH': 'Rel_H_%',
#     'WS_ms_S_WVT': 'Wind_Sp_m/sec',
#     'WindDir_D1_WVT': 'Wind_Dir_Deg',
#     'Rain_mm_Tot': 'Rain_mm'
# }

# # --- NOUVELLES Configurations pour les stations Dassari ---
# STATIONS_DASSARI_OURIYORI_1 = ['Ouriyori 1']
# COLONNES_SUP_OURIYORI_1 = [
#     'TIMESTAMP', 'RECORD', 'WSDiag', 'Intensity_RT_Avg', 'Acc_RT_NRT_Tot',
#     'Pluvio_Status', 'BP_mbar_Avg', 'SR01Up_Avg', 'SR01Dn_Avg', 'IR01Up_Avg',
#     'IR01Dn_Avg', 'NR01TC_Avg', 'IR01UpCo_Avg', 'IR01DnCo_Avg',
#     'Acc_NRT_Tot', 'Acc_totNRT', 'Bucket_RT_Avg', 'Bucket_NRT',
#     'Temp_load_cell_Avg', 'Heater_Status'
# ]
# RENAMED_COLUMNS_OURIYORI_1 = {
#     'Rain_mm_Tot': 'Rain_mm',
#     'AirTC_Avg': 'Air_Temp_Deg_C',
#     'RH': 'Rel_H_%',
#     'SlrW_Avg': 'Solar_R_W/m^2',
#     'WS_ms_S_WVT': 'Wind_Sp_m/sec',
#     'WindDir_D1_WVT': 'Wind_Dir_Deg'
# }

# # --- Configurations existantes pour les stations Vea Sissili ---
# stations_vea_a_9_variables = ['Oualem', 'Nebou', 'Nabugubelle', 'Manyoro', 'Gwosi', 'Doninga','Bongo Soe']

# colonnes_a_renommer_stations_vea_a_9_variables = {
#     'Rain_mm_Tot': 'Rain_mm',
#     'AirTC_Avg': 'Air_Temp_Deg_C',
#     'RH': 'Rel_H_%',
#     'SlrW_Avg': 'Solar_R_W/m^2',
#     'WS_ms_S_WVT': 'Wind_Sp_m/sec',
#     'WindDir_D1_WVT': 'Wind_Dir_Deg',
#     'Date': 'Datetime' # Renommer 'Date' en 'Datetime' pour compatibilité
# }

# colonnes_a_sup_stations_vea_a_9_variables = ['SlrkJ_Tot', 'WS_ms_Avg', 'WindDir', 'Rain_01_mm_Tot', 'Rain_02_mm_Tot']

# colonnes_a_renommer_aniabisi = {
#     'Rain_mm_Tot': 'Rain_mm',
#     'AirTC_Avg': 'Air_Temp_Deg_C',
#     'RH': 'Rel_H_%',
#     'SlrW_Avg': 'Solar_R_W/m^2',
#     'WS_ms_S_WVT': 'Wind_Sp_m/sec',
#     'WindDir_D1_WVT': 'Wind_Dir_Deg',
#     'Date': 'Datetime' # Renommer 'Date' en 'Datetime' pour compatibilité
# }

# colonnes_a_sup_aniabisi = ['Intensity_RT_Avg', 'Acc_NRT_Tot', 'Acc_RT_NRT_Tot', 'SR01Up_Avg', 'SR01Dn_Avg', 'IR01Up_Avg', 'IR01Dn_Avg','IR01UpCo_Avg', 'IR01DnCo_Avg']

# colonnes_a_renommer_atampisi = {
#     'Rain_01_mm_Tot': 'Rain_01_mm',
#     'Rain_02_mm_Tot': 'Rain_02_mm',
#     'AirTC_Avg': 'Air_Temp_Deg_C',
#     'RH': 'Rel_H_%',
#     'SlrW_Avg': 'Solar_R_W/m^2',
#     'WS_ms_Avg': 'Wind_Sp_m/sec',
#     'WindDir': 'Wind_Dir_Deg',
#     'Date': 'Datetime' # Renommer 'Date' en 'Datetime' pour compatibilité
# }


# # Fonction utilitaire pour créer Rain_mm si nécessaire
# def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
#     """
#     Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
#     Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
#     Si aucune des colonnes sources n'est présente, 'Rain_mm' n'est pas créée.
#     """
#     df_copy = df.copy()
#     if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
#         df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
#         warnings.warn("Colonne Rain_mm créée en fusionnant Rain_01_mm et Rain_02_mm.")
#     elif 'Rain_01_mm' in df_copy.columns:
#         df_copy['Rain_mm'] = df_copy['Rain_01_mm']
#         warnings.warn("Colonne Rain_mm créée à partir de Rain_01_mm.")
#     elif 'Rain_02_mm' in df_copy.columns:
#         df_copy['Rain_mm'] = df_copy['Rain_02_mm']
#         warnings.warn("Colonne Rain_mm créée à partir de Rain_02_mm.")
#     else:
#         # Si ni 'Rain_01_mm' ni 'Rain_02_mm' n'existe, ne pas créer 'Rain_mm'
#         warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents. La colonne 'Rain_mm' ne sera pas créée.")
#     return df_copy

# def filter_colonnes(df: pd.DataFrame, renamed_columns: dict, delete_columns: list = None) -> pd.DataFrame:
#     """
#     Traite un DataFrame en renommant les colonnes, en supprimant des colonnes
#     et en extrayant les composantes de date/heure si une colonne 'Date' est présente.
#     Principalement utilisé pour les stations Vea Sissili.

#     Args:
#         df (pd.DataFrame): DataFrame à traiter.
#         renamed_columns (dict): Dictionnaire pour renommer les colonnes.
#         delete_columns (list, optional): Liste des colonnes à supprimer. Defaults to None.

#     Returns:
#         pd.DataFrame: DataFrame nettoyé et enrichi.
#     """
#     df_copy = df.copy()

#     # Supprimer les colonnes spécifiées
#     if delete_columns is not None:
#         df_copy.drop(columns=delete_columns, inplace=True, errors='ignore')

#     # Renommer les colonnes
#     df_copy.rename(columns=renamed_columns, inplace=True, errors='ignore')

#     # La gestion de la colonne 'Date' et l'extraction des composantes sont maintenant
#     # gérées de manière centralisée par `create_datetime` après l'appel à cette fonction.
#     return df_copy


# def selection_colonnes_dano(df: pd.DataFrame, column_select: list, renamed_columns: dict) -> pd.DataFrame:
#     """
#     Sélectionne des colonnes spécifiques et les renomme pour les stations du bassin de Dano
#     (ex: Tambiri 1).

#     Args:
#         df (pd.DataFrame): DataFrame à traiter.
#         column_select (list): Liste des colonnes à sélectionner.
#         renamed_columns (dict): Dictionnaire pour renommer les colonnes.

#     Returns:
#         pd.DataFrame: DataFrame avec les colonnes sélectionnées et renommées.
#     """
#     df_copy = df.copy()
    
#     # Filtrer les colonnes, en ignorant les erreurs si une colonne n'existe pas
#     # Créer une liste des colonnes qui existent réellement dans df_copy
#     existing_columns_to_select = [col for col in column_select if col in df_copy.columns]
#     df_copy = df_copy[existing_columns_to_select]

#     # Renommer les colonnes
#     df_copy.rename(columns=renamed_columns, inplace=True, errors='ignore')

#     return df_copy


# def process_dassari_ouriyori_1(df: pd.DataFrame, columns_to_drop: list, columns_to_rename: dict) -> pd.DataFrame:
#     """
#     Traite un DataFrame spécifique pour la station Ouriyori 1 du bassin Dassari.
#     Gère la colonne 'TIMESTAMP', supprime des colonnes, et renomme d'autres.

#     Args:
#         df (pd.DataFrame): DataFrame à traiter.
#         columns_to_drop (list): Liste des colonnes à supprimer.
#         columns_to_rename (dict): Dictionnaire pour renommer les colonnes.

#     Returns:
#         pd.DataFrame: DataFrame nettoyé et enrichi.
#     """
#     df_copy = df.copy()

#     # Supprimer les lignes d'en-tête secondaires basées sur 'TIMESTAMP'
#     if "TIMESTAMP" in df_copy.columns:
#         initial_rows = len(df_copy)
#         df_copy = df_copy[~df_copy["TIMESTAMP"].astype(str).isin(["TS", "NaN", "nan"])]
#         df_copy.reset_index(drop=True, inplace=True)
#         if len(df_copy) < initial_rows:
#             warnings.warn(f"Suppression de {initial_rows - len(df_copy)} lignes d'en-tête invalides basées sur 'TIMESTAMP'.")
            
#         # Renommer 'TIMESTAMP' en 'Datetime' pour une gestion ultérieure cohérente
#         df_copy.rename(columns={'TIMESTAMP': 'Datetime'}, inplace=True, errors='ignore')
#     else:
#         warnings.warn("Colonne 'TIMESTAMP' manquante pour le traitement Dassari Ouriyori 1. Le pré-traitement pourrait être incomplet.")

#     # Supprimer les colonnes spécifiées
#     df_copy.drop(columns=columns_to_drop, inplace=True, errors='ignore')

#     # Renommer les colonnes
#     df_copy.rename(columns=columns_to_rename, inplace=True, errors='ignore')

#     return df_copy


# def apply_station_specific_preprocessing(df: pd.DataFrame, station_name: str) -> pd.DataFrame:
#     """
#     Applique la logique de pré-traitement spécifique à une station donnée.
#     Cette fonction est le point d'entrée pour les transformations de données brutes
#     spécifiques à chaque type de fichier/station.

#     Args:
#         df (pd.DataFrame): DataFrame d'entrée brut pour une station.
#         station_name (str): Nom de la station.

#     Returns:
#         pd.DataFrame: DataFrame pré-traité.
#     """
#     df_processed = df.copy()
#     print(f"DEBUG (apply_station_specific_preprocessing): Application du pré-traitement pour station: {station_name}")

#     # --- Logique Vea Sissili ---
#     if station_name in stations_vea_a_9_variables:
#         print(f"DEBUG (preprocessing): Appliquer filter_colonnes pour station {station_name} (9 variables).")
#         df_processed = filter_colonnes(df_processed, colonnes_a_renommer_stations_vea_a_9_variables, colonnes_a_sup_stations_vea_a_9_variables)
#     elif station_name == 'Aniabisi':
#         print(f"DEBUG (preprocessing): Appliquer filter_colonnes pour station {station_name} (Aniabisi).")
#         df_processed = filter_colonnes(df_processed, colonnes_a_renommer_aniabisi, colonnes_a_sup_aniabisi)
#     elif station_name == 'Atampisi':
#         print(f"DEBUG (preprocessing): Appliquer filter_colonnes pour station {station_name} (Atampisi).")
#         df_processed = filter_colonnes(df_processed, colonnes_a_renommer_atampisi) # delete_columns is None by default

#     # --- Logique Dano (Tambiri 1) ---
#     elif station_name in STATIONS_DANO_TAMBIRI_1:
#         print(f"DEBUG (preprocessing): Appliquer selection_colonnes_dano pour station {station_name}.")
#         df_processed = selection_colonnes_dano(df_processed, COLONNES_SELECT_TAMBIRI_1, COLONNES_RENNOMAGE_SELECT_TAMBIRI_1)

#     # --- Logique Dassari (Ouriyori 1) ---
#     elif station_name in STATIONS_DASSARI_OURIYORI_1:
#         print(f"DEBUG (preprocessing): Appliquer process_dassari_ouriyori_1 pour station {station_name}.")
#         df_processed = process_dassari_ouriyori_1(df_processed, COLONNES_SUP_OURIYORI_1, RENAMED_COLUMNS_OURIYORI_1)
    
#     else:
#         print(f"DEBUG (preprocessing): Aucune logique de pré-traitement spécifique définie pour la station {station_name}. Le DataFrame sera traité tel quel.")

#     return df_processed


# def create_datetime(df: pd.DataFrame, bassin: str = None) -> pd.DataFrame:
#     """
#     Crée ou valide la colonne 'Datetime' et ses composantes (Year, Month, Day, Hour, Minute, Date)
#     à partir de colonnes existantes ('Datetime' ou 'Date' ou composantes séparées).
#     Cette fonction est robuste pour différents formats.

#     Args:
#         df (pd.DataFrame): DataFrame d'entrée.
#         bassin (str, optional): Nom du bassin (non directement utilisé pour la logique de date ici,
#                                 mais peut être utile pour le débogage ou des logiques futures).

#     Returns:
#         pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
#     """
#     df_copy = df.copy()

#     # Si 'Datetime' est déjà un type datetime et valide, extraire les composantes et retourner
#     if pd.api.types.is_datetime64_any_dtype(df_copy.get('Datetime')) and df_copy['Datetime'].notna().any():
#         df_copy['Year'] = df_copy['Datetime'].dt.year
#         df_copy['Month'] = df_copy['Datetime'].dt.month
#         df_copy['Day'] = df_copy['Datetime'].dt.day
#         df_copy['Hour'] = df_copy['Datetime'].dt.hour
#         df_copy['Minute'] = df_copy['Datetime'].dt.minute
#         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
#              df_copy['Date'] = df_copy['Datetime'].dt.date
#         print("DEBUG (create_datetime): 'Datetime' déjà valide. Composantes extraites.")
#         return df_copy

#     # Si 'Date' existe, tenter de la convertir en 'Datetime'
#     if 'Date' in df_copy.columns:
#         print("DEBUG (create_datetime): Tentative de conversion de 'Date' en 'Datetime'.")
#         try:
#             df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
#             df_copy.dropna(subset=['Datetime'], inplace=True) # Supprimer les lignes avec Datetime invalide
#         except Exception as e:
#             warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime: {e}")
#             df_copy['Datetime'] = pd.NaT # Assigner NaT si conversion échoue
    
#     # Sinon, tenter de créer 'Datetime' à partir de Year, Month, Day, Hour, Minute
#     else:
#         date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
#         existing_date_components = [col for col in date_cols if col in df_copy.columns]
        
#         if not existing_date_components:
#             warnings.warn("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute, ou Date/Datetime) trouvée. 'Datetime' ne sera pas créé.")
#             return df_copy # Retourner le DataFrame original si aucune composante de date

#         print("DEBUG (create_datetime): Tentative de création de 'Datetime' à partir des composantes séparées.")
#         try:
#             # Convertir les colonnes de date/heure en numérique, remplir les NaN avec 0 pour la conversion string
#             for col in date_cols:
#                 if col in df_copy.columns:
#                     df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce').fillna(0).astype(int)

#             # Créer une chaîne de caractères pour la conversion en Datetime
#             df_copy['temp_date_str'] = df_copy.apply(
#                 lambda row: f"{int(row.get('Year', 2000)):04d}-"
#                             f"{int(row.get('Month', 1)):02d}-"
#                             f"{int(row.get('Day', 1)):02d} "
#                             f"{int(row.get('Hour', 0)):02d}:"
#                             f"{int(row.get('Minute', 0)):02d}",
#                 axis=1
#             )
#             df_copy['Datetime'] = pd.to_datetime(df_copy['temp_date_str'], errors='coerce')
#             df_copy.drop(columns=['temp_date_str'], inplace=True, errors='ignore')
#             df_copy.dropna(subset=['Datetime'], inplace=True) # Supprimer les lignes avec Datetime invalide
            
#         except Exception as e:
#             warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
#             df_copy['Datetime'] = pd.NaT # Assigner NaT si conversion échoue
            
#     # Extraction finale des composantes si 'Datetime' a été créé/mis à jour et est valide
#     if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
#         df_copy['Year'] = df_copy['Datetime'].dt.year
#         df_copy['Month'] = df_copy['Datetime'].dt.month
#         df_copy['Day'] = df_copy['Datetime'].dt.day
#         df_copy['Hour'] = df_copy['Datetime'].dt.hour
#         df_copy['Minute'] = df_copy['Datetime'].dt.minute
#         if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
#              df_copy['Date'] = df_copy['Datetime'].dt.date
#         print("DEBUG (create_datetime): 'Datetime' créé/mis à jour et composantes extraites.")
#     else:
#         warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

#     return df_copy

# def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
#     """
#     Effectue toutes les interpolations météorologiques en une seule passe.
#     Cette fonction DOIT recevoir un DataFrame avec un DatetimeIndex.
#     Il doit également contenir une colonne 'Station'.

#     Args:
#         df (pd.DataFrame): Le DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
#         limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
#         df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
#                                (colonnes 'Station', 'Lat', 'Long', 'Timezone').

#     Returns:
#         pd.DataFrame: Le DataFrame original avec les données interpolées,
#                       la colonne 'Is_Daylight' calculée, la durée du jour, et un DatetimeIndex.
#     """
#     df_processed = df.copy()

#     # Assurez-vous que l'index est bien DatetimeIndex et nettoyez les NaT
#     if not isinstance(df_processed.index, pd.DatetimeIndex):
#         raise TypeError("Le DataFrame d'entrée pour l'interpolation DOIT avoir un DatetimeIndex.")
    
#     initial_rows = len(df_processed)
#     df_processed = df_processed[df_processed.index.notna()]
#     if len(df_processed) == 0:
#         raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
#     if initial_rows - len(df_processed) > 0:
#         warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")
    
#     print(f"DEBUG (interpolation): Type de l'index du DataFrame initial: {type(df_processed.index)}")
#     print(f"DEBUG (interpolation): Premières 5 valeurs de l'index après nettoyage des NaT: {df_processed.index[:5].tolist() if not df_processed.empty else 'DataFrame vide'}")

#     required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
#     if not all(col in df_gps.columns for col in required_gps_cols):
#         raise ValueError(
#             f"df_gps doit contenir les colonnes {required_gps_cols}. "
#             f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
#         )

#     if not df_gps['Station'].is_unique:
#         print("Avertissement: La colonne 'Station' dans df_gps contient des noms de station dupliqués.")
#         print("Ceci peut entraîner des comportements inattendus ou des stations non reconnues.")
#         df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
#         print(f"Suppression de {len(df_gps) - len(df_gps_unique)} doublons dans df_gps (en gardant la première occurrence).")
#     else:
#         df_gps_unique = df_gps.copy()

#     gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

#     numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
#                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
#                       'Solar_R_W/m^2', 'Wind_Dir_Deg']
#     for col in numerical_cols:
#         if col in df_processed.columns:
#             df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

#     df_processed_parts = []

#     for station_name, group in df_processed.groupby('Station'):
#         group_copy = group.copy()
#         print(f"DEBUG (interpolation/groupby): Début du traitement du groupe '{station_name}'.")
        
#         # Standardize group_copy.index to UTC first
#         # This block ensures the index is UTC-aware before proceeding
#         if group_copy.index.tz is None:
#             group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
#         elif group_copy.index.tz != pytz.utc: # If it's already tz-aware but not UTC, convert to UTC
#             group_copy.index = group_copy.index.tz_convert('UTC')
#         print(f"DEBUG (interpolation/groupby): Index Datetime pour '{station_name}' STANDARDISÉ à UTC. Dtype: {group_copy.index.dtype}")
        
#         # S'assurer que l'index n'a pas de NaT après localisation
#         group_copy = group_copy[group_copy.index.notna()]
#         if group_copy.empty:
#             warnings.warn(f"Le groupe '{station_name}' est vide après nettoyage de l'index Datetime. Il sera ignoré.")
#             continue


#         # --- Calculs Astral en utilisant le fuseau horaire local ---
#         apply_fixed_daylight = True
#         gps_data = gps_info_dict.get(station_name)
#         if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
#             lat = gps_data['Lat']
#             long = gps_data['Long']
#             timezone_str = gps_data['Timezone']

#             try:
#                 local_tz = pytz.timezone(timezone_str)
#                 # Create a local timezone-aware version of the index for Astral calculations
#                 index_for_astral_local = group_copy.index.tz_convert(local_tz)

#                 daily_sun_info = {}
#                 # Get unique dates from the local-time index (these are timezone-aware Timestamps)
#                 # Utilisation de .drop_duplicates() au lieu de .unique() pour garantir un objet Pandas
#                 unique_dates_ts_local = index_for_astral_local.normalize().drop_duplicates()

#                 # Ensure unique_dates is not empty before processing
#                 if unique_dates_ts_local.empty: # Utilisez .empty pour les objets Pandas
#                     raise ValueError("No unique dates found for Astral calculation.")
                
#                 for ts_local_aware in unique_dates_ts_local: # Iterate over timezone-aware Timestamps
#                     loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
                    
#                     # Convert the timezone-aware Timestamp to a naive datetime.date object
#                     # This is what Astral's sun.sun function expects for its 'date' parameter
#                     # This conversion helps avoid potential re-localization warnings from Astral/pytz.
#                     naive_date_for_astral = ts_local_aware.to_pydatetime().date()
                    
#                     # Pass the NAIVE date object to Astral. Astral's observer handles the timezone internally.
#                     s = sun.sun(loc.observer, date=naive_date_for_astral) 
#                     daily_sun_info[naive_date_for_astral] = {
#                         'sunrise': s['sunrise'],
#                         'sunset': s['sunset']
#                     }

#                 # Explicitly create a list of naive dates for the DataFrame index
#                 naive_unique_dates_for_index = [ts.date() for ts in unique_dates_ts_local] # Use .date() here as keys are naive
#                 temp_df_sun_index = pd.Index(naive_unique_dates_for_index, name='Date_Local_Naive')
#                 temp_df_sun = pd.DataFrame(index=temp_df_sun_index)
                
#                 # NOUVEAUX DÉBOGAGES pour comprendre le type juste avant l'opération
#                 print(f"DEBUG (astral_calc): unique_dates_ts_local type: {type(unique_dates_ts_local)}")
#                 print(f"DEBUG (astral_calc): naive_unique_dates_for_index type: {type(naive_unique_dates_for_index)}")
#                 print(f"DEBUG (astral_calc): temp_df_sun_index type: {type(temp_df_sun_index)}")
#                 if not temp_df_sun.empty:
#                     print(f"DEBUG (astral_calc): First element of temp_df_sun.index: {temp_df_sun.index[0]}")
#                     print(f"DEBUG (astral_calc): Type of first element of temp_df_sun.index: {type(temp_df_sun.index[0])}")

#                 # Correction: Utilisation de la compréhension de liste pour éviter le problème de .map()
#                 temp_df_sun['sunrise_time_local'] = [daily_sun_info.get(date, {}).get('sunrise') for date in temp_df_sun.index]
#                 temp_df_sun['sunset_time_local'] = [daily_sun_info.get(date, {}).get('sunset') for date in temp_df_sun.index]

#                 # Merge with group_copy (which has UTC index)
#                 # To merge, create a normalized local date column (naive) in group_copy
#                 group_copy_reset = group_copy.reset_index()
#                 group_copy_reset['Date_Local_Naive'] = group_copy_reset['Datetime'].dt.tz_convert(local_tz).dt.date

#                 group_copy_reset = pd.merge(group_copy_reset, temp_df_sun, on='Date_Local_Naive', how='left')

#                 # Convert merged local times back to UTC for comparison with original 'Datetime' (which is UTC)
#                 group_copy_reset['sunrise_time_utc'] = group_copy_reset['sunrise_time_local'].dt.tz_convert('UTC')
#                 group_copy_reset['sunset_time_utc'] = group_copy_reset['sunset_time_local'].dt.tz_convert('UTC')

#                 group_copy_reset.loc[:, 'Is_Daylight'] = (group_copy_reset['Datetime'] >= group_copy_reset['sunrise_time_utc']) & \
#                                                           (group_copy_reset['Datetime'] < group_copy_reset['sunset_time_utc'])

#                 daylight_timedelta_local = group_copy_reset['sunset_time_local'] - group_copy_reset['sunrise_time_local']
                
#                 # Correction ici pour formater la durée en HH:MM:SS
#                 def format_timedelta_to_hms(td):
#                     if pd.isna(td):
#                         return np.nan
#                     total_seconds = int(td.total_seconds())
#                     hours = total_seconds // 3600
#                     minutes = (total_seconds % 3600) // 60
#                     seconds = total_seconds % 60
#                     return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

#                 group_copy_reset.loc[:, 'Daylight_Duration'] = daylight_timedelta_local.apply(format_timedelta_to_hms)


#                 group_copy = group_copy_reset.set_index('Datetime')
#                 group_copy = group_copy.drop(columns=['Date_Local_Naive', 'sunrise_time_local', 'sunset_time_local', 'sunrise_time_utc', 'sunset_time_utc'], errors='ignore')

#                 print(f"Lever et coucher du soleil calculés pour {station_name}.")
#                 apply_fixed_daylight = False

#             except Exception as e:
#                 print(f"Erreur lors du calcul du lever/coucher du soleil avec Astral pour {station_name}: {e}.")
#                 traceback.print_exc() # Print full traceback for this specific error
#                 warnings.warn(f"Calcul Astral impossible pour '{station_name}'. Utilisation de l'indicateur jour/nuit fixe.")
#                 apply_fixed_daylight = True
#         else:
#             print(f"Avertissement: Coordonnées ou Fuseau horaire manquants/invalides pour le site '{station_name}' dans df_gps. Utilisation de l'indicateur jour/nuit fixe.")
#             apply_fixed_daylight = True

#         if apply_fixed_daylight:
#             group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
#             group_copy.loc[:, 'Daylight_Duration'] = "11:00:00" # Règle fixe pour la durée
#             print(f"Utilisation de l'indicateur jour/nuit fixe (7h-18h) pour {station_name}.")

#         df_processed_parts.append(group_copy)

#     if not df_processed_parts:
#         raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

#     # Concaténation de toutes les parties. Puisque chaque partie a un DatetimeIndex,
#     # nous pouvons concaténer directement sans ignore_index=True
#     df_final = pd.concat(df_processed_parts)
#     # Assurez-vous que l'index est trié et qu'il est bien nommé 'Datetime'
#     df_final = pd.concat(df_processed_parts).sort_index()
#     df_final.index.name = 'Datetime' 
#     print(f"DEBUG (interpolation/concat): Index du DataFrame final après concaténation et tri: {type(df_final.index)}")
#     print(f"DEBUG (interpolation/concat): Colonnes du DataFrame final après concaténation: {df_final.columns.tolist()}")


#     # Nettoyage des colonnes temporaires (elles ne sont plus nécessaires ou n'existent plus)
#     cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date']
#     df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

#     # Gestion intelligente de Rain_mm : Appel de la fonction modifiée
#     df_final = create_rain_mm(df_final)

#     # Note : Si create_rain_mm ne crée pas la colonne, elle ne sera pas présente ici.
#     # Les autres parties du code devront gérer l'absence de 'Rain_mm'.

#     # Interpolation standard et bornage pour les variables numériques
#     standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
#                       'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
#                       'Solar_R_W/m^2', 'Wind_Dir_Deg']

#     for var in standard_vars:
#         if var in df_final.columns: # Vérifier si la colonne existe avant de la traiter
#             df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
#             if var in limits:
#                 min_val = limits[var]['min']
#                 max_val = limits[var]['max']
#                 initial_nan_count = df_final[var].isna().sum()
#                 if min_val is not None:
#                     df_final.loc[df_final[var] < min_val, var] = np.nan
#                 if max_val is not None:
#                     df_final.loc[df_final[var] > max_val, var] = np.nan
                
#                 new_nan_count = df_final[var].isna().sum()
#                 if new_nan_count > initial_nan_count:
#                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
#             print(f"DEBUG (interpolation/variable): Interpolation de '{var}'. Type de l'index de df_final: {type(df_final.index)}")
            
#             # Interpolation seulement si l'index est bien DatetimeIndex
#             if isinstance(df_final.index, pd.DatetimeIndex):
#                 df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
#             else:
#                 print(f"Avertissement (interpolation/variable): L'index n'est pas un DatetimeIndex pour l'interpolation de '{var}'. Utilisation de la méthode 'linear'.")
#                 df_final[var] = df_final[var].interpolate(method='linear', limit_direction='both')
#             df_final[var] = df_final[var].bfill().ffill()

#     # Interpolation CONDITIONNELLE de la radiation solaire
#     if 'Solar_R_W/m^2' in df_final.columns: # Vérifier si la colonne existe
#         df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

#         if 'Solar_R_W/m^2' in limits:
#             min_val = limits['Solar_R_W/m^2']['min']
#             max_val = limits['Solar_R_W/m^2']['max']
#             initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
#             df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
#             if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
#                 warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

#         if 'Is_Daylight' in df_final.columns:
#             df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

#             if 'Rain_mm' in df_final.columns: # Vérifier l'existence de 'Rain_mm'
#                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
#             else:
#                 cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
#                 warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
#             df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

#             print(f"DEBUG (interpolation/solaire): Interpolation de 'Solar_R_W/m^2' (conditionnel). Type de l'index de df_final: {type(df_final.index)}")

#             is_day = df_final['Is_Daylight']
#             if isinstance(df_final.index, pd.DatetimeIndex):
#                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
#             else:
#                 print(f"Avertissement (interpolation/solaire): L'index n'est pas un DatetimeIndex pour l'interpolation de 'Solar_R_W/m^2'. Utilisation de la méthode 'linear'.")
#                 df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both')

#             df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

#             df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
#             warnings.warn("Radiation solaire interpolée avec succès.")
#         else:
#             warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
#             if isinstance(df_final.index, pd.DatetimeIndex):
#                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()
#             else:
#                  df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both').bfill().ffill()

#     warnings.warn("Vérification des valeurs manquantes après interpolation:")
#     missing_after_interp = df_final.isna().sum()
#     columns_with_missing = missing_after_interp[missing_after_interp > 0]
#     if not columns_with_missing.empty:
#         warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
#     else:
#         warnings.warn("Aucune valeur manquante après l'interpolation.")

#     return df_final


# def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
#     """
#     Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
#     de coordonnées UTM vers latitude/longitude WGS84.

#     Args:
#         df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

#     Returns:
#         pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
#     """
#     df_copy = df.copy()

#     required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
#     if not all(col in df_copy.columns for col in required_utm_cols):
#         raise ValueError(
#             f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
#         )

#     def convert_row(row):
#         try:
#             zone = int(row['zone'])
#             hemisphere = str(row['hemisphere']).upper()
#             is_northern = hemisphere == 'N'

#             proj_utm = CRS.from_proj4(
#                 f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
#             )
#             proj_wgs84 = CRS.from_epsg(4326) # WGS84 Lat/Long

#             transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
#             lon, lat = transformer.transform(row['Easting'], row['Northing'])
#             return pd.Series({'Long': lon, 'Lat': lat})
#         except Exception as e:
#             warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
#             return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

#     df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
#     df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

#     return df_copy

# def _load_and_prepare_gps_data() -> pd.DataFrame:
#     """
#     Charge les fichiers de coordonnées des stations depuis Google Drive,
#     les prétraite (suppression/ajout de colonnes/lignes, renommage),
#     convertit les coordonnées UTM en GPS pour Dano et Dassari,
#     ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.

#     Cette fonction est destinée à être appelée une seule fois au démarrage de l'application.
#     Elle vérifie d'abord si les fichiers existent localement pour éviter des téléchargements répétés.

#     Returns:
#         pd.DataFrame: Un DataFrame consolidé de toutes les stations avec
#                       leurs coordonnées GPS (Lat, Long) et fuseaux horaires.
#     """
#     print("Début de la préparation des données de coordonnées des stations...")
#     data_dir = 'data'
#     os.makedirs(data_dir, exist_ok=True) # Assurez-vous que le dossier 'data' existe

#     files_info = [
#         {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
#         {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
#         {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
#     ]

#     loaded_dfs = []

#     for file_info in files_info:
#         output_file_path = os.path.join(data_dir, file_info['name'])
        
#         if not os.path.exists(output_file_path):
#             print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
#             gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
#             print(f"Téléchargement de {file_info['bassin']} terminé.")
#         else:
#             print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
#         loaded_dfs.append(pd.read_excel(output_file_path))

#     vea_sissili_bassin = loaded_dfs[0]
#     dano_bassin = loaded_dfs[1]
#     dassari_bassin = loaded_dfs[2]

#     # Prétraitement des DataFrames (votre code original)
#     print("Début du prétraitement des données de stations...")
    
#     # Vea Sissili
#     vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
#     new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
#     vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

#     # Dassari
#     dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
#     new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
#     dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

#     # Dano et Dassari : Renommage et ajout des colonnes UTM
#     dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
#     dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

#     dano_bassin['zone'] = 30
#     dano_bassin['hemisphere'] = 'N'
#     dassari_bassin['zone'] = 31
#     dassari_bassin['hemisphere'] = 'N'

#     # Application de la fonction de conversion UTM vers GPS
#     dano_bassin = convert_utm_df_to_gps(dano_bassin)
#     dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

#     # Ajout des fuseaux horaires
#     dano_bassin['Timezone'] = 'Africa/Ouagadougou'
#     dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
#     vea_sissili_bassin['Timezone'] = 'Africa/Accra' # Assurez-vous que c'est le bon fuseau horaire pour Vea Sissili

#     # Fusion de tous les bassins
#     bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

#     # Renommer 'Name' en 'Station'
#     bassins = bassins.rename(columns={'Name': 'Station'})

#     # Nettoyer les lignes avec des valeurs NaN dans Lat/Long/Timezone qui pourraient résulter d'erreurs de conversion
#     initial_rows = len(bassins)
#     bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
#     if len(bassins) < initial_rows:
#         print(f"Attention: {initial_rows - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
#     # 5. Sauvegarde du DataFrame final en JSON
#     output_json_path = os.path.join(data_dir, "station_coordinates.json")
#     # Utiliser to_json avec orient='records' pour un format plus lisible et facile à charger
#     bassins.to_json(output_json_path, orient='records', indent=4)
#     print(f"\nPréparation des données terminée. Coordonnées des stations sauvegardées dans '{output_json_path}'.")
#     print("Vous pouvez maintenant lancer votre application Flask.")

#     return bassins # Retourner le DataFrame des données GPS


# def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
#     """
#     Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
#     Conserve la première occurrence en cas de doublon.

#     Args:
#         df (pd.DataFrame): Le DataFrame d'entrée.

#     Returns:
#         pd.DataFrame: Le DataFrame sans doublons.
#     """
#     if 'Station' in df.columns and 'Datetime' in df.columns:
#         initial_rows = len(df)
#         df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
#         if len(df_cleaned) < initial_rows:
#             warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
#         return df_cleaned
#     else:
#         warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
#         return df

# def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
#     """
#     Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

#     Args:
#         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
#         limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

#     Returns:
#         pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
#     """
#     df_processed = df.copy()

#     if not isinstance(df_processed.index, pd.DatetimeIndex):
#         warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
#         try:
#             df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
#             df_processed = df_processed[df_processed.index.notna()]
#             if df_processed.empty:
#                 raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
#         except Exception as e:
#             raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

#     for var, vals in limits.items():
#         if var in df_processed.columns:
#             min_val = vals.get('min')
#             max_val = vals.get('max')
#             if min_val is not None or max_val is not None:
#                 initial_nan_count = df_processed[var].isna().sum()
#                 if min_val is not None:
#                     df_processed.loc[df_processed[var] < min_val, var] = np.nan
#                 if max_val is not None:
#                     df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
#                 new_nan_count = df_processed[var].isna().sum()
#                 if new_nan_count > initial_nan_count:
#                     warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
#     return df_processed

# def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
#     """
#     Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
#     Retourne l'objet Figure Plotly.
#     """
#     if not isinstance(df.index, pd.DatetimeIndex):
#         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

#     filtered_df = df[df['Station'] == station].copy()
#     if filtered_df.empty:
#         return go.Figure() # Retourne une figure vide si pas de données

#     if periode == 'Journalière':
#         resampled_df = filtered_df[variable].resample('D').mean()
#     elif periode == 'Hebdomadaire':
#         resampled_df = filtered_df[variable].resample('W').mean()
#     elif periode == 'Mensuelle':
#         resampled_df = filtered_df[variable].resample('M').mean()
#     elif periode == 'Annuelle':
#         resampled_df = filtered_df[variable].resample('Y').mean()
#     else: # Données Brutes
#         resampled_df = filtered_df[variable]

#     resampled_df = resampled_df.dropna()

#     if resampled_df.empty:
#         return go.Figure() # Retourne une figure vide si pas de données après resample/dropna

#     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
#     color = colors.get(station, '#1f77b4')

#     fig = go.Figure()
#     fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
#                              mode='lines', name=f'{variable_meta["Nom"]} - {station}',
#                              line=dict(color=color)))

#     fig.update_layout(
#         title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
#         xaxis_title="Date",
#         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
#         hovermode="x unified",
#         template="plotly_white" # Utiliser un template Plotly plus clair
#     )
#     return fig

# def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
#     """
#     Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
#     Retourne l'objet Figure Plotly.
#     """
#     if not isinstance(df.index, pd.DatetimeIndex):
#         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

#     fig = go.Figure()
    
#     all_stations = df['Station'].unique()
#     if len(all_stations) < 2:
#         warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
#         return go.Figure() # Retourne une figure vide si moins de 2 stations

#     for station in all_stations:
#         filtered_df = df[df['Station'] == station].copy()
#         if filtered_df.empty:
#             continue

#         if periode == 'Journalière':
#             resampled_df = filtered_df[variable].resample('D').mean()
#         elif periode == 'Hebdomadaire':
#             resampled_df = filtered_df[variable].resample('W').mean()
#         elif periode == 'Mensuelle':
#             resampled_df = filtered_df[variable].resample('M').mean()
#         elif periode == 'Annuelle':
#             resampled_df = filtered_df[variable].resample('Y').mean()
#         else: # Données Brutes
#             resampled_df = filtered_df[variable]

#         resampled_df = resampled_df.dropna()
#         if resampled_df.empty:
#             continue
        
#         color = colors.get(station, '#1f77b4')
#         fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
#                                  mode='lines', name=station,
#                                  line=dict(color=color)))

#     if not fig.data:
#         return go.Figure() # Retourne une figure vide si aucune trace n'a été ajoutée

#     variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
#     fig.update_layout(
#         title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
#         xaxis_title="Date",
#         yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
#         hovermode="x unified",
#         legend_title="Variables",
#         template="plotly_white"
#     )
#     return fig


# def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> go.Figure:
#     """
#     Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
#     Retourne l'objet Figure Plotly.
#     """
#     if not isinstance(df.index, pd.DatetimeIndex):
#         raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

#     filtered_df = df[df['Station'] == station].copy()
#     if filtered_df.empty:
#         return go.Figure() # Retourne une figure vide si pas de données

#     numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight', 'Daylight_Duration']]

#     if not numerical_vars:
#         warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
#         return go.Figure() # Retourne une figure vide si pas de variables numériques

#     normalized_df = filtered_df[numerical_vars].copy()
#     for col in normalized_df.columns:
#         min_val = normalized_df[col].min()
#         max_val = normalized_df[col].max()
#         if max_val != min_val:
#             normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
#         else:
#             normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

#     normalized_df = normalized_df.dropna(how='all')

#     if normalized_df.empty:
#         return go.Figure() # Retourne une figure vide si pas de données après normalisation/dropna
    
#     fig = go.Figure()
#     for var in normalized_df.columns:
#         var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
#         color = colors.get(var, None)

#         fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
#                                  mode='lines', name=var_meta['Nom'],
#                                  line=dict(color=color)))

#     fig.update_layout(
#         title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
#         xaxis_title="Date",
#         yaxis_title="Valeur Normalisée (0-1)",
#         hovermode="x unified",
#         legend_title="Variables",
#         template="plotly_white"
#     )
#     return fig

# def calculate_daily_summary_table(df: pd.DataFrame) -> pd.DataFrame:
#     """
#     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
#     groupées par station. Cette fonction renvoie un DataFrame de statistiques, non un graphique.
#     C'est la fonction qui remplace l'ancienne 'daily_stats'.

#     Args:
#         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

#     Returns:
#         pd.DataFrame: DataFrame avec les statistiques journalières.
#     """
#     df_copy = df.copy()

#     # Assurez-vous que 'Datetime' est une colonne et de type datetime
#     if isinstance(df_copy.index, pd.DatetimeIndex):
#         df_copy = df_copy.reset_index()

#     df_copy['Datetime'] = pd.to_datetime(df_copy['Datetime'], errors='coerce')
#     df_copy = df_copy.dropna(subset=['Datetime', 'Station'])

#     if df_copy.empty:
#         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans calculate_daily_summary_table.")
#         return pd.DataFrame()

#     if 'Is_Daylight' not in df_copy.columns:
#         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
#         df_copy['Is_Daylight'] = (df_copy['Datetime'].dt.hour >= 7) & (df_copy['Datetime'].dt.hour <= 18)

#     numerical_cols = [col for col in df_copy.columns if pd.api.types.is_numeric_dtype(df_copy[col]) and col not in ['Station', 'Datetime', 'Is_Daylight', 'Daylight_Duration']]
    
#     if not numerical_cols:
#         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
#         return pd.DataFrame()

#     # Calcul des statistiques de base par jour
#     daily_aggregated_df = df_copy.groupby(['Station', df_copy['Datetime'].dt.date]).agg({
#         col: ['mean', 'min', 'max'] for col in numerical_cols if METADATA_VARIABLES.get(col, {}).get('is_rain') == False
#     })

#     # Renommage des colonnes agrégées pour les non-pluies
#     daily_aggregated_df.columns = ['_'.join(col).strip() for col in daily_aggregated_df.columns.values]


#     # Traitement spécifique pour la pluie (Rain_mm)
#     if 'Rain_mm' in df_copy.columns and METADATA_VARIABLES.get('Rain_mm', {}).get('is_rain'):
#         df_daily_rain = df_copy.groupby(['Station', df_copy['Datetime'].dt.date])['Rain_mm'].sum().reset_index()
#         df_daily_rain = df_daily_rain.rename(columns={'Rain_mm': 'Rain_mm_sum'})

#         # Fusionner les statistiques de pluie avec les autres
#         if not daily_aggregated_df.empty:
#             daily_aggregated_df = daily_aggregated_df.reset_index()
#             daily_stats_df = pd.merge(daily_aggregated_df, df_daily_rain, on=['Station', 'Datetime'], how='left')
#             daily_stats_df = daily_stats_df.rename(columns={'Datetime': 'Date'})
#         else:
#             daily_stats_df = df_daily_rain.rename(columns={'Datetime': 'Date'})
#     else:
#         daily_stats_df = daily_aggregated_df.reset_index().rename(columns={'Datetime': 'Date'})


#     # Calcul des statistiques de saison et de sécheresse pour la pluie
#     if 'Rain_mm' in df_copy.columns and METADATA_VARIABLES.get('Rain_mm', {}).get('is_rain'):
#         df_daily_rain_raw = df_copy.groupby(['Station', pd.Grouper(key='Datetime', freq='D')])['Rain_mm'].sum().reset_index()
        
#         RAIN_SEASON_GAP_THRESHOLD = pd.Timedelta(days=60)
#         season_stats = []
#         for station_name, station_df_rain in df_daily_rain_raw.groupby('Station'):
#             station_df_rain = station_df_rain.set_index('Datetime').sort_index()
#             rain_events = station_df_rain[station_df_rain['Rain_mm'] > 0].index

#             if rain_events.empty:
#                 season_stats.append({'Station': station_name, 'Moyenne_Saison_Pluvieuse': np.nan, 'Debut_Saison_Pluvieuse': pd.NaT, 'Fin_Saison_Pluvieuse': pd.NaT, 'Duree_Saison_Pluvieuse_Jours': np.nan})
#                 continue
            
#             block_ids = (rain_events.to_series().diff() > RAIN_SEASON_GAP_THRESHOLD).cumsum()
#             valid_blocks = {}
#             for block_id, rain_dates_in_block in rain_events.to_series().groupby(block_ids):
#                 if not rain_dates_in_block.empty:
#                     block_start = rain_dates_in_block.min()
#                     block_end = rain_dates_in_block.max()
#                     full_block_df = station_df_rain.loc[block_start:block_end]
#                     valid_blocks[block_id] = full_block_df

#             if not valid_blocks:
#                 season_stats.append({'Station': station_name, 'Moyenne_Saison_Pluvieuse': np.nan, 'Debut_Saison_Pluvieuse': pd.NaT, 'Fin_Saison_Pluvieuse': pd.NaT, 'Duree_Saison_Pluvieuse_Jours': np.nan})
#                 continue

#             main_block_id = max(valid_blocks, key=lambda k: (valid_blocks[k].index.max() - valid_blocks[k].index.min()).days)
#             main_season_df = valid_blocks[main_block_id]

#             debut_saison = main_season_df.index.min()
#             fin_saison = main_season_df.index.max()
#             total_days = (fin_saison - debut_saison).days + 1
#             moyenne_saison = main_season_df['Rain_mm'].sum() / total_days if total_days > 0 else 0

#             season_stats.append({
#                 'Station': station_name,
#                 'Moyenne_Saison_Pluvieuse': moyenne_saison,
#                 'Debut_Saison_Pluvieuse': debut_saison,
#                 'Fin_Saison_Pluvieuse': fin_saison,
#                 'Duree_Saison_Pluvieuse_Jours': total_days
#             })
#         df_season_stats = pd.DataFrame(season_stats)
        
#         # Merge season stats into main daily_stats_df
#         if not df_season_stats.empty:
#             daily_stats_df = pd.merge(daily_stats_df, df_season_stats, on='Station', how='left')


#     # Calcul des statistiques globales (Max, Min, Moyenne, Médiane, etc.) par station
#     # pour les variables numériques, à inclure dans le tableau récapitulatif.
#     final_stats_per_station = pd.DataFrame()
#     for station_name in df_copy['Station'].unique():
#         station_df = df_copy[df_copy['Station'] == station_name].copy()
#         station_summary = {'Station': station_name}

#         for var in numerical_cols:
#             if var in station_df.columns and pd.api.types.is_numeric_dtype(station_df[var]):
#                 # Filtrage pour la radiation solaire (seulement pendant le jour)
#                 if var == 'Solar_R_W/m^2':
#                     var_data = station_df.loc[station_df['Is_Daylight'], var].dropna()
#                 else:
#                     var_data = station_df[var].dropna()
                
#                 if not var_data.empty:
#                     station_summary[f'{var}_Maximum'] = var_data.max()
#                     station_summary[f'{var}_Minimum'] = var_data.min()
#                     station_summary[f'{var}_Moyenne'] = var_data.mean()
#                     station_summary[f'{var}_Mediane'] = var_data.median()
                    
#                     # Pour Rain_mm uniquement
#                     if var == 'Rain_mm':
#                         station_summary[f'{var}_Cumul_Annuel'] = station_df['Rain_mm'].sum()
#                         # Moyenne des jours pluvieux (seulement les jours où il a plu)
#                         rainy_days_data = station_df[station_df['Rain_mm'] > 0]['Rain_mm'].dropna()
#                         station_summary[f'{var}_Moyenne_Jours_Pluvieux'] = rainy_days_data.mean() if not rainy_days_data.empty else np.nan

#                         # Ajouter la durée de la saison pluvieuse et de sécheresse du df_season_stats
#                         if 'Duree_Saison_Pluvieuse_Jours' in daily_stats_df.columns:
#                             s_data = daily_stats_df[daily_stats_df['Station'] == station_name]
#                             if not s_data.empty:
#                                 station_summary[f'{var}_Duree_Saison_Pluvieuse_Jours'] = s_data['Duree_Saison_Pluvieuse_Jours'].iloc[0]
#                                 # Placeholder pour sécheresse si non calculé ailleurs
#                                 station_summary[f'{var}_Duree_Secheresse_Definie_Jours'] = np.nan # Vous devrez calculer ceci plus tard

#         final_stats_per_station = pd.concat([final_stats_per_station, pd.DataFrame([station_summary])], ignore_index=True)
        
#     return final_stats_per_station # Retourne le DataFrame de statistiques agrégées par station


# def generate_variable_summary_plots_for_web(df: pd.DataFrame, station: str, variable: str, metadata: dict, palette: dict) -> plt.Figure:
#     """
#     Génère un graphique Matplotlib/Seaborn pour les statistiques agrégées d'une variable spécifique
#     pour une station donnée, en utilisant la logique fournie par l'utilisateur pour 'daily_stats'.

#     Args:
#         df (pd.DataFrame): Le DataFrame global de données traitées avec DatetimeIndex et colonne 'Station'.
#         station (str): Le nom de la station à visualiser.
#         variable (str): La variable à visualiser (e.g., 'Air_Temp_Deg_C', 'Rain_mm').
#         metadata (dict): Dictionnaire de métadonnées pour les variables (Nom, Unite, is_rain).
#         palette (dict): Dictionnaire de couleurs pour les différentes métriques statistiques.

#     Returns:
#         plt.Figure: Un objet Figure Matplotlib contenant tous les sous-graphiques pour la variable sélectionnée.
#     """
#     df_station = df[df['Station'] == station].copy()

#     if df_station.empty:
#         fig, ax = plt.subplots(figsize=(10, 6))
#         ax.text(0.5, 0.5, f"Aucune donnée pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
#         ax.axis('off')
#         return fig

#     # S'assurer que Datetime est un index datetime correct pour les données de la station
#     if isinstance(df_station.index, pd.DatetimeIndex):
#         df_station = df_station.reset_index() # Réinitialise l'index pour un accès facile aux colonnes
    
#     df_station['Datetime'] = pd.to_datetime(df_station['Datetime'], errors='coerce')
#     df_station = df_station.dropna(subset=['Datetime', 'Station'])
#     df_station = df_station.set_index('Datetime').sort_index() # Remet l'index pour les opérations de séries temporelles

#     if df_station.empty:
#         fig, ax = plt.subplots(figsize=(10, 6))
#         ax.text(0.5, 0.5, f"DataFrame vide après nettoyage des dates pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
#         ax.axis('off')
#         return fig

#     # Assurez-vous que 'Is_Daylight' existe (généralement géré par interpolation, mais au cas où)
#     if 'Is_Daylight' not in df_station.columns:
#         df_station['Is_Daylight'] = (df_station.index.hour >= 7) & (df_station.index.hour <= 18)


#     var_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})

#     # Dictionnaire pour stocker les statistiques pour la variable sélectionnée
#     stats_for_plot = {}
#     metrics_to_plot = []
    
#     # --- LOGIQUE SPÉCIFIQUE À RAIN_MM (transférée de la fonction daily_stats de l'utilisateur) ---
#     if var_meta.get('is_rain', False) and variable == 'Rain_mm':
#         # Vérifier si 'Rain_mm' existe avant de procéder aux calculs spécifiques à la pluie
#         if 'Rain_mm' not in df_station.columns:
#             fig, ax = plt.subplots(figsize=(10, 6))
#             ax.text(0.5, 0.5, f"La variable 'Rain_mm' n'est pas disponible pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
#             ax.axis('off')
#             return fig

#         df_daily_rain = df_station.groupby(pd.Grouper(freq='D'))['Rain_mm'].sum().reset_index()
#         df_daily_rain = df_daily_rain.rename(columns={'Rain_mm': 'Rain_mm_sum'})
#         df_daily_rain['Datetime'] = pd.to_datetime(df_daily_rain['Datetime'])
#         df_daily_rain = df_daily_rain.set_index('Datetime').sort_index()

#         RAIN_SEASON_GAP_THRESHOLD = pd.Timedelta(days=60)
#         rain_events = df_daily_rain[df_daily_rain['Rain_mm_sum'] > 0].index

#         s_moyenne_saison = np.nan
#         s_duree_saison = np.nan
#         s_debut_saison = pd.NaT
#         s_fin_saison = pd.NaT

#         if not rain_events.empty:
#             block_ids = (rain_events.to_series().diff() > RAIN_SEASON_GAP_THRESHOLD).cumsum()
#             valid_blocks = {}
#             for block_id, rain_dates_in_block in rain_events.to_series().groupby(block_ids):
#                 if not rain_dates_in_block.empty:
#                     block_start = rain_dates_in_block.min()
#                     block_end = rain_dates_in_block.max()
#                     full_block_df = df_daily_rain.loc[block_start:block_end]
#                     valid_blocks[block_id] = full_block_df

#             if valid_blocks:
#                 main_block_id = max(valid_blocks, key=lambda k: (valid_blocks[k].index.max() - valid_blocks[k].index.min()).days)
#                 main_season_df = valid_blocks[main_block_id]

#                 s_debut_saison = main_season_df.index.min()
#                 s_fin_saison = main_season_df.index.max()
#                 total_days_season = (s_fin_saison - s_debut_saison).days + 1
#                 s_moyenne_saison = main_season_df['Rain_mm_sum'].sum() / total_days_season if total_days_season > 0 else 0
#                 s_duree_saison = total_days_season

#         # Logique de détection de la sécheresse
#         longest_dry_spell = np.nan
#         debut_secheresse = pd.NaT
#         fin_secheresse = pd.NaT

#         full_daily_series_rain = df_daily_rain['Rain_mm_sum'].resample('D').sum().fillna(0)
#         rainy_days_index = full_daily_series_rain[full_daily_series_rain > 0].index

#         if not rainy_days_index.empty and pd.notna(s_moyenne_saison) and s_moyenne_saison > 0:
#             temp_dry_spells = []
#             for i in range(1, len(rainy_days_index)):
#                 prev_rain_date = rainy_days_index[i-1]
#                 current_rain_date = rainy_days_index[i]
#                 dry_days_between_rains = (current_rain_date - prev_rain_date).days - 1

#                 if dry_days_between_rains > 0:
#                     rain_prev_day = full_daily_series_rain.loc[prev_rain_date]
#                     temp_debut = pd.NaT
#                     temp_duree = 0
#                     for j in range(1, dry_days_between_rains + 1):
#                         current_dry_date = prev_rain_date + timedelta(days=j)
#                         current_ratio = rain_prev_day / j
#                         if current_ratio < s_moyenne_saison: # Condition pour la sécheresse définie
#                             temp_debut = current_dry_date
#                             temp_duree = (current_rain_date - temp_debut).days
#                             break
#                     if pd.notna(temp_debut) and temp_duree > 0:
#                         temp_dry_spells.append({
#                             'Duree': temp_duree,
#                             'Debut': temp_debut,
#                             'Fin': current_rain_date - timedelta(days=1)
#                         })
            
#             if temp_dry_spells:
#                 df_temp_dry = pd.DataFrame(temp_dry_spells)
#                 idx_max_dry = df_temp_dry['Duree'].idxmax()
#                 longest_dry_spell = df_temp_dry.loc[idx_max_dry, 'Duree']
#                 debut_secheresse = df_temp_dry.loc[idx_max_dry, 'Debut']
#                 fin_secheresse = df_temp_dry.loc[idx_max_dry, 'Fin']

#         # Statistiques pour Rain_mm (tirées de la vue consolidée de daily_stats de l'utilisateur)
#         rain_data_for_stats = df_station[variable].dropna()
        
#         stats_for_plot['Maximum'] = rain_data_for_stats.max() if not rain_data_for_stats.empty else np.nan
#         stats_for_plot['Minimum'] = rain_data_for_stats.min() if not rain_data_for_stats.empty else np.nan
#         stats_for_plot['Mediane'] = rain_data_for_stats.median() if not rain_data_for_stats.empty else np.nan
#         stats_for_plot['Cumul_Annuel'] = df_station[variable].sum()
        
#         rainy_days_data = df_station[df_station[variable] > 0][variable].dropna()
#         stats_for_plot['Moyenne_Jours_Pluvieux'] = rainy_days_data.mean() if not rainy_days_data.empty else np.nan
        
#         stats_for_plot['Moyenne_Saison_Pluvieuse'] = s_moyenne_saison
#         stats_for_plot['Duree_Saison_Pluvieuse_Jours'] = s_duree_saison
#         stats_for_plot['Debut_Saison_Pluvieuse'] = s_debut_saison
#         stats_for_plot['Fin_Saison_Pluvieuse'] = s_fin_saison
        
#         stats_for_plot['Duree_Secheresse_Definie_Jours'] = longest_dry_spell
#         stats_for_plot['Debut_Secheresse_Definie'] = debut_secheresse
#         stats_for_plot['Fin_Secheresse_Definie'] = fin_secheresse

#         # Récupérer les dates pour max/min pour Rain_mm
#         max_date_idx = df_station[variable].idxmax() if not df_station[variable].empty else pd.NaT
#         min_date_idx = df_station[variable].idxmin() if not df_station[variable].empty else pd.NaT
#         stats_for_plot['Date_Maximum'] = max_date_idx if pd.notna(max_date_idx) else pd.NaT
#         stats_for_plot['Date_Minimum'] = min_date_idx if pd.notna(min_date_idx) else pd.NaT
        
#         metrics_to_plot = [
#             'Maximum', 'Minimum', 'Cumul_Annuel', 'Mediane',
#             'Moyenne_Jours_Pluvieux', 'Moyenne_Saison_Pluvieuse',
#             'Duree_Saison_Pluvieuse_Jours', 'Duree_Secheresse_Definie_Jours'
#         ]
#         nrows, ncols = 4, 2
#         figsize = (18, 16)
        
#     # --- LOGIQUE POUR LES AUTRES VARIABLES (transférée de la fonction daily_stats de l'utilisateur) ---
#     else:
#         current_var_data = df_station[variable].dropna()
#         if variable == 'Solar_R_W/m^2':
#             current_var_data = df_station.loc[df_station['Is_Daylight'], variable].dropna()

#         if current_var_data.empty:
#             fig, ax = plt.subplots(figsize=(10, 6))
#             ax.text(0.5, 0.5, f"Aucune donnée valide pour la variable {var_meta['Nom']} à {station}.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
#             ax.axis('off')
#             return fig

#         # Statistiques pour les autres variables (de daily_stats de l'utilisateur)
#         stats_for_plot['Maximum'] = current_var_data.max()
#         stats_for_plot['Minimum'] = current_var_data.min()
#         stats_for_plot['Mediane'] = current_var_data.median()
#         stats_for_plot['Moyenne'] = current_var_data.mean()

#         # Récupérer les dates pour max/min
#         max_idx = current_var_data.idxmax() if not current_var_data.empty else pd.NaT
#         min_idx = current_var_data.idxmin() if not current_var_data.empty else pd.NaT

#         stats_for_plot['Date_Maximum'] = max_idx if pd.notna(max_idx) else pd.NaT
#         stats_for_plot['Date_Minimum'] = min_idx if pd.notna(min_idx) else pd.NaT

#         metrics_to_plot = ['Maximum', 'Minimum', 'Moyenne', 'Mediane']
#         nrows, ncols = 2, 2
#         figsize = (18, 12)

#     # Gérer le cas où stats_for_plot pourrait être vide (ex: toutes les données sont NaN)
#     if not stats_for_plot:
#         fig, ax = plt.subplots(figsize=(10, 6))
#         ax.text(0.5, 0.5, f"Impossible de calculer des statistiques pour la variable '{variable}' à la station '{station}' (données manquantes ou non numériques).", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
#         ax.axis('off')
#         return fig

#     # --- PARTIE COMMUNE DE TRACÉ (adaptée de la fonction daily_stats de l'utilisateur) ---
#     fig, axes = plt.subplots(nrows, ncols, figsize=figsize)
#     plt.subplots_adjust(hspace=0.6, wspace=0.4) # Ajustement de l'espacement pour un meilleur ajustement
#     axes = axes.flatten()

#     fig.suptitle(f'Statistiques de {var_meta["Nom"]} pour la station {station}', fontsize=16, y=0.98)

#     for i, metric in enumerate(metrics_to_plot):
#         ax = axes[i]
#         value = stats_for_plot.get(metric)
#         if pd.isna(value):
#             ax.text(0.5, 0.5, "Données non disponibles", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=12, color='gray')
#             ax.axis('off')
#             continue

#         color = palette.get(metric.replace(' ', '_'), '#cccccc') # Utilisation de la `palette` passée en argument
        
#         # Créer un DataFrame factice pour sns.barplot, car il attend un DataFrame
#         plot_data_bar = pd.DataFrame({'Metric': [metric.replace('_', ' ')], 'Value': [value]})
#         sns.barplot(x='Metric', y='Value', data=plot_data_bar, ax=ax, color=color, edgecolor='none')

#         # Ajouter les annotations
#         text = ""
#         if metric in ['Duree_Saison_Pluvieuse_Jours', 'Duree_Secheresse_Definie_Jours']:
#             start_date_key = f'Debut_{metric.replace("Jours", "")}'
#             end_date_key = f'Fin_{metric.replace("Jours", "")}'
#             start_date = stats_for_plot.get(start_date_key)
#             end_date = stats_for_plot.get(end_date_key)
#             date_info = ""
#             if pd.notna(start_date) and pd.notna(end_date):
#                 date_info = f"\ndu {start_date.strftime('%d/%m/%Y')} au {end_date.strftime('%d/%m/%Y')}"
#             text = f"{int(value)} j{date_info}"
#         elif metric in ['Maximum', 'Minimum', 'Cumul_Annuel', 'Moyenne_Jours_Pluvieux', 'Moyenne_Saison_Pluvieuse', 'Mediane', 'Moyenne']:
#             unit = var_meta['Unite']
#             date_str = ''
#             # Vérifier les clés de date pour max/min si elles existent dans stats_for_plot
#             if (metric == 'Maximum' and 'Date_Maximum' in stats_for_plot and pd.notna(stats_for_plot['Date_Maximum'])):
#                 date_str = f"\n({stats_for_plot['Date_Maximum'].strftime('%d/%m/%Y')})"
#             elif (metric == 'Minimum' and 'Date_Minimum' in stats_for_plot and pd.notna(stats_for_plot['Date_Minimum'])):
#                 date_str = f"\n({stats_for_plot['Date_Minimum'].strftime('%d/%m/%Y')})"
            
#             text = f"{value:.1f} {unit}{date_str}"
#         else:
#             text = f"{value:.1f} {var_meta['Unite']}"

#         # Positionne le texte sur le graphique.
#         # Attention: 'bar' n'est pas défini ici, c'est le résultat de sns.barplot.
#         # Pour une barre simple, la hauteur est plot_data_bar['Value'][0] et la position x est 0.
#         # On peut simplifier l'annotation ou s'assurer que 'bar' est correctement extrait si on utilise sns.barplot directement.
#         # Pour le moment, on va utiliser des positions génériques pour l'annotation de texte.
#         # ax.text(0.5, value, # Position x au centre de la barre (0.5 pour une seule barre sur l'axe x)
#         #         text, ha='center', va='bottom', fontsize=9, color='black',
#         #         bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))
#         # Correction pour récupérer l'objet bar après le barplot
#         for bar in ax.patches: # itérer sur les patchs (barres) créés par seaborn
#             ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(),
#                     text, ha='center', va='bottom', fontsize=9, color='black',
#                     bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))

#         ax.set_title(f"{var_meta['Nom']} {metric.replace('_', ' ')}", fontsize=11)
#         ax.set_xlabel("")
#         ax.set_ylabel(f"Valeur ({var_meta['Unite']})", fontsize=10)
#         ax.tick_params(axis='x', rotation=0)
#         ax.set_xticklabels([])

#     # Si moins de graphiques que de sous-graphiques, désactiver les axes inutilisés
#     for j in range(i + 1, len(axes)):
#         fig.delaxes(axes[j])

#     plt.tight_layout(rect=[0, 0, 1, 0.96])

#     return fig


# def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
#     """
#     Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
#     groupées par station.

#     Args:
#         df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.

#     Returns:
#         pd.DataFrame: DataFrame avec les statistiques journalières.
#     """
#     df = df.copy()

#     # Assurez-vous que 'Datetime' est une colonne et de type datetime
#     if isinstance(df.index, pd.DatetimeIndex):
#         df = df.reset_index()

#     df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')
#     df = df.dropna(subset=['Datetime', 'Station'])

#     if df.empty:
#         print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans daily_stats.")
#         return pd.DataFrame()

#     if 'Is_Daylight' not in df.columns:
#         warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
#         df['Is_Daylight'] = (df['Datetime'].dt.hour >= 7) & (df['Datetime'].dt.hour <= 18)

#     numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Datetime', 'Is_Daylight']]
    
#     if not numerical_cols:
#         warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
#         return pd.DataFrame()

#     agg_funcs = {col: ['mean', 'min', 'max'] for col in numerical_cols}
    
#     if 'Rain_mm' in numerical_cols:
#         agg_funcs['Rain_mm'].append('sum')

#     daily_stats_df = df.groupby(['Station', df['Datetime'].dt.date]).agg(agg_funcs)

#     daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

#     daily_stats_df = daily_stats_df.rename_axis(index={'Datetime': 'Date'})
#     daily_stats_df = daily_stats_df.reset_index()

#     return daily_stats_df


import pandas as pd
from pyproj import CRS, Transformer
import pytz
from astral.location import LocationInfo
from astral import sun
import numpy as np
import warnings
import os
import gdown
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import seaborn as sns
import traceback
import math
from datetime import timedelta

from config import METADATA_VARIABLES, PALETTE_DEFAUT, DATA_LIMITS

# def apply_station_specific_preprocessing(df: pd.DataFrame, bassin: str, station: str) -> pd.DataFrame:
#     """
#     Prétraite les données d'une station en fonction de son bassin et de son nom.
#     Applique les renommages de colonnes et les sélections spécifiques.
    
#     Args:
#         df (pd.DataFrame): DataFrame brut à traiter
#         bassin (str): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI')
#         station (str): Nom de la station
        
#     Returns:
#         pd.DataFrame: DataFrame prétraité avec les colonnes standardisées
#     """
#     df_copy = df.copy()
    
#     # Traitement pour le bassin Dano
#     if bassin.upper() == 'DANO':
#         if station == 'Tambiri 1':
#             colonnes_select = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'AirTC_Avg', 'RH', 
#                              'WS_ms_S_WVT', 'WindDir_D1_WVT', 'Rain_mm_Tot', 'BP_mbar_Avg', 'Station']
#             colonnes_renommage = {
#                 'AirTC_Avg': 'Air_Temp_Deg_C', 
#                 'RH': 'Rel_H_%', 
#                 'WS_ms_S_WVT': 'Wind_Sp_m/sec', 
#                 'WindDir_D1_WVT': 'Wind_Dir_Deg', 
#                 'Rain_mm_Tot': 'Rain_mm'
#             }
#             df_copy = df_copy[colonnes_select]
#             df_copy.rename(columns=colonnes_renommage, inplace=True, errors='ignore')
    
#     # Traitement pour le bassin Dassari
#     elif bassin.upper() == 'DASSARI':
#         if station == 'Ouriyori 1':
#             colonnes_sup = ['TIMESTAMP', 'RECORD', 'WSDiag', 'Intensity_RT_Avg', 'Acc_RT_NRT_Tot', 
#                           'Pluvio_Status', 'BP_mbar_Avg', 'SR01Up_Avg', 'SR01Dn_Avg', 'IR01Up_Avg', 
#                           'IR01Dn_Avg', 'NR01TC_Avg', 'IR01UpCo_Avg', 'IR01DnCo_Avg',
#                           'Acc_NRT_Tot', 'Acc_totNRT', 'Bucket_RT_Avg', 'Bucket_NRT',
#                           'Temp_load_cell_Avg', 'Heater_Status']
            
#             colonnes_renommage = {
#                 'Rain_mm_Tot': 'Rain_mm',
#                 'AirTC_Avg': 'Air_Temp_Deg_C',
#                 'RH': 'Rel_H_%',
#                 'SlrW_Avg': 'Solar_R_W/m^2',
#                 'WS_ms_S_WVT': 'Wind_Sp_m/sec',
#                 'WindDir_D1_WVT': 'Wind_Dir_Deg'
#             }
            
#             if 'TIMESTAMP' in df_copy.columns:
#                 df_copy["TIMESTAMP"] = pd.to_datetime(df_copy["TIMESTAMP"], errors="coerce")
#                 df_copy.dropna(subset=["TIMESTAMP"], inplace=True)
                
#                 df_copy["Year"] = df_copy["TIMESTAMP"].dt.year
#                 df_copy["Month"] = df_copy["TIMESTAMP"].dt.month
#                 df_copy["Day"] = df_copy["TIMESTAMP"].dt.day
#                 df_copy["Hour"] = df_copy["TIMESTAMP"].dt.hour
#                 df_copy["Minute"] = df_copy["TIMESTAMP"].dt.minute
            
#             df_copy.drop(columns=colonnes_sup, inplace=True, errors='ignore')
#             df_copy.rename(columns=colonnes_renommage, inplace=True, errors='ignore')
    
#     # Traitement pour le bassin Vea Sissili
#     elif bassin.upper() == 'VEA_SISSILI':
#         stations_vea_a_9_variables = ['Oualem', 'Nebou', 'Nabugubelle', 'Manyoro', 'Gwosi', 'Doninga', 'Bongo Soe']
        
#         if station in stations_vea_a_9_variables:
#             colonnes_renommage = {
#                 'Rain_mm_Tot': 'Rain_mm',
#                 'AirTC_Avg': 'Air_Temp_Deg_C',
#                 'RH': 'Rel_H_%',
#                 'SlrW_Avg': 'Solar_R_W/m^2',
#                 'WS_ms_S_WVT': 'Wind_Sp_m/sec',
#                 'WindDir_D1_WVT': 'Wind_Dir_Deg',
#                 'Date': 'Datetime'
#             }
#             colonnes_sup = ['SlrkJ_Tot', 'WS_ms_Avg', 'WindDir', 'Rain_01_mm_Tot', 'Rain_02_mm_Tot']
            
#         elif station == 'Aniabisi':
#             colonnes_renommage = {
#                 'Rain_mm_Tot': 'Rain_mm',
#                 'AirTC_Avg': 'Air_Temp_Deg_C',
#                 'RH': 'Rel_H_%',
#                 'SlrW_Avg': 'Solar_R_W/m^2',
#                 'WS_ms_S_WVT': 'Wind_Sp_m/sec',
#                 'WindDir_D1_WVT': 'Wind_Dir_Deg',
#                 'Date': 'Datetime'
#             }
#             colonnes_sup = ['Intensity_RT_Avg', 'Acc_NRT_Tot', 'Acc_RT_NRT_Tot', 'SR01Up_Avg', 
#                           'SR01Dn_Avg', 'IR01Up_Avg', 'IR01Dn_Avg', 'IR01UpCo_Avg', 'IR01DnCo_Avg']
            
#         elif station == 'Atampisi':
#             colonnes_renommage = {
#                 'Rain_01_mm_Tot': 'Rain_01_mm',
#                 'Rain_02_mm_Tot': 'Rain_02_mm',
#                 'AirTC_Avg': 'Air_Temp_Deg_C',
#                 'RH': 'Rel_H_%',
#                 'SlrW_Avg': 'Solar_R_W/m^2',
#                 'WS_ms_Avg': 'Wind_Sp_m/sec',
#                 'WindDir': 'Wind_Dir_Deg',
#                 'Date': 'Datetime'
#             }
#             colonnes_sup = []
        
#         if 'Date' in df_copy.columns:
#             df_copy["Date"] = pd.to_datetime(df_copy["Date"], errors="coerce")
#             df_copy.dropna(subset=["Date"], inplace=True)
            
#             df_copy["Year"] = df_copy["Date"].dt.year
#             df_copy["Month"] = df_copy["Date"].dt.month
#             df_copy["Day"] = df_copy["Date"].dt.day
#             df_copy["Hour"] = df_copy["Date"].dt.hour
#             df_copy["Minute"] = df_copy["Date"].dt.minute
        
#         if colonnes_sup:
#             df_copy.drop(columns=colonnes_sup, inplace=True, errors='ignore')
        
#         df_copy.rename(columns=colonnes_renommage, inplace=True, errors='ignore')
    
#     return df_copy


def apply_station_specific_preprocessing(df: pd.DataFrame, station: str) -> pd.DataFrame:
    """
    Prétraite les données d'une station spécifique en fonction de son nom.
    Applique les renommages de colonnes et les sélections spécifiques.
    
    Args:
        df (pd.DataFrame): DataFrame brut à traiter
        station (str): Nom de la station
        
    Returns:
        pd.DataFrame: DataFrame prétraité avec les colonnes standardisées
    """
    # Dictionnaire de mapping des stations aux bassins
    STATION_TO_BASIN = {
        # Stations Dano
        'Tambiri 1': 'DANO',
        # Stations Dassari
        'Ouriyori 1': 'DASSARI',
        # Stations Vea Sissili
        'Oualem': 'VEA_SISSILI',
        'Nebou': 'VEA_SISSILI',
        'Nabugubelle': 'VEA_SISSILI',
        'Manyoro': 'VEA_SISSILI',
        'Gwosi': 'VEA_SISSILI',
        'Doninga': 'VEA_SISSILI',
        'Bongo Soe': 'VEA_SISSILI',
        'Aniabisi': 'VEA_SISSILI',
        'Atampisi': 'VEA_SISSILI'
    }
    
    # Déterminer le bassin à partir de la station
    bassin = STATION_TO_BASIN.get(station)
    
    if not bassin:
        warnings.warn(f"Station {station} non reconnue dans aucun bassin. Prétraitement standard appliqué.")
        return df
    
    df_copy = df.copy()
    
    # Traitement pour le bassin Dano
    if bassin == 'DANO':
        if station == 'Tambiri 1':
            colonnes_select = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'AirTC_Avg', 'RH', 
                             'WS_ms_S_WVT', 'WindDir_D1_WVT', 'Rain_mm_Tot', 'BP_mbar_Avg', 'Station']
            colonnes_renommage = {
                'AirTC_Avg': 'Air_Temp_Deg_C', 
                'RH': 'Rel_H_%', 
                'WS_ms_S_WVT': 'Wind_Sp_m/sec', 
                'WindDir_D1_WVT': 'Wind_Dir_Deg', 
                'Rain_mm_Tot': 'Rain_mm'
            }
            df_copy = df_copy[colonnes_select]
            df_copy.rename(columns=colonnes_renommage, inplace=True, errors='ignore')
    
    # Traitement pour le bassin Dassari
    elif bassin == 'DASSARI':
        if station == 'Ouriyori 1':
            colonnes_sup = ['TIMESTAMP', 'RECORD', 'WSDiag', 'Intensity_RT_Avg', 'Acc_RT_NRT_Tot', 
                          'Pluvio_Status', 'BP_mbar_Avg', 'SR01Up_Avg', 'SR01Dn_Avg', 'IR01Up_Avg', 
                          'IR01Dn_Avg', 'NR01TC_Avg', 'IR01UpCo_Avg', 'IR01DnCo_Avg',
                          'Acc_NRT_Tot', 'Acc_totNRT', 'Bucket_RT_Avg', 'Bucket_NRT',
                          'Temp_load_cell_Avg', 'Heater_Status']
            
            colonnes_renommage = {
                'Rain_mm_Tot': 'Rain_mm',
                'AirTC_Avg': 'Air_Temp_Deg_C',
                'RH': 'Rel_H_%',
                'SlrW_Avg': 'Solar_R_W/m^2',
                'WS_ms_S_WVT': 'Wind_Sp_m/sec',
                'WindDir_D1_WVT': 'Wind_Dir_Deg'
            }
            
            if 'TIMESTAMP' in df_copy.columns:
                df_copy["TIMESTAMP"] = pd.to_datetime(df_copy["TIMESTAMP"], errors="coerce")
                df_copy.dropna(subset=["TIMESTAMP"], inplace=True)
                
                df_copy["Year"] = df_copy["TIMESTAMP"].dt.year
                df_copy["Month"] = df_copy["TIMESTAMP"].dt.month
                df_copy["Day"] = df_copy["TIMESTAMP"].dt.day
                df_copy["Hour"] = df_copy["TIMESTAMP"].dt.hour
                df_copy["Minute"] = df_copy["TIMESTAMP"].dt.minute
            
            df_copy.drop(columns=colonnes_sup, inplace=True, errors='ignore')
            df_copy.rename(columns=colonnes_renommage, inplace=True, errors='ignore')
    
    # Traitement pour le bassin Vea Sissili
    elif bassin == 'VEA_SISSILI':
        stations_vea_a_9_variables = ['Oualem', 'Nebou', 'Nabugubelle', 'Manyoro', 'Gwosi', 'Doninga', 'Bongo Soe']
        
        if station in stations_vea_a_9_variables:
            colonnes_renommage = {
                'Rain_mm_Tot': 'Rain_mm',
                'AirTC_Avg': 'Air_Temp_Deg_C',
                'RH': 'Rel_H_%',
                'SlrW_Avg': 'Solar_R_W/m^2',
                'WS_ms_S_WVT': 'Wind_Sp_m/sec',
                'WindDir_D1_WVT': 'Wind_Dir_Deg',
                'Date': 'Datetime'
            }
            colonnes_sup = ['SlrkJ_Tot', 'WS_ms_Avg', 'WindDir', 'Rain_01_mm_Tot', 'Rain_02_mm_Tot']
            
        elif station == 'Aniabisi':
            colonnes_renommage = {
                'Rain_mm_Tot': 'Rain_mm',
                'AirTC_Avg': 'Air_Temp_Deg_C',
                'RH': 'Rel_H_%',
                'SlrW_Avg': 'Solar_R_W/m^2',
                'WS_ms_S_WVT': 'Wind_Sp_m/sec',
                'WindDir_D1_WVT': 'Wind_Dir_Deg',
                'Date': 'Datetime'
            }
            colonnes_sup = ['Intensity_RT_Avg', 'Acc_NRT_Tot', 'Acc_RT_NRT_Tot', 'SR01Up_Avg', 
                          'SR01Dn_Avg', 'IR01Up_Avg', 'IR01Dn_Avg', 'IR01UpCo_Avg', 'IR01DnCo_Avg']
            
        elif station == 'Atampisi':
            colonnes_renommage = {
                'Rain_01_mm_Tot': 'Rain_01_mm',
                'Rain_02_mm_Tot': 'Rain_02_mm',
                'AirTC_Avg': 'Air_Temp_Deg_C',
                'RH': 'Rel_H_%',
                'SlrW_Avg': 'Solar_R_W/m^2',
                'WS_ms_Avg': 'Wind_Sp_m/sec',
                'WindDir': 'Wind_Dir_Deg',
                'Date': 'Datetime'
            }
            colonnes_sup = []
        
        if 'Date' in df_copy.columns:
            df_copy["Date"] = pd.to_datetime(df_copy["Date"], errors="coerce")
            df_copy.dropna(subset=["Date"], inplace=True)
            
            df_copy["Year"] = df_copy["Date"].dt.year
            df_copy["Month"] = df_copy["Date"].dt.month
            df_copy["Day"] = df_copy["Date"].dt.day
            df_copy["Hour"] = df_copy["Date"].dt.hour
            df_copy["Minute"] = df_copy["Date"].dt.minute
        
        if colonnes_sup:
            df_copy.drop(columns=colonnes_sup, inplace=True, errors='ignore')
        
        df_copy.rename(columns=colonnes_renommage, inplace=True, errors='ignore')
    
    return df_copy

def create_rain_mm(df: pd.DataFrame) -> pd.DataFrame:
    """
    Crée la colonne 'Rain_mm' en fusionnant 'Rain_01_mm' et 'Rain_02_mm'.
    Utilise 'Rain_01_mm' par défaut, puis 'Rain_02_mm' si 'Rain_01_mm' est NaN.
    """
    df_copy = df.copy()
    if 'Rain_01_mm' in df_copy.columns and 'Rain_02_mm' in df_copy.columns:
        df_copy['Rain_mm'] = df_copy['Rain_01_mm'].fillna(df_copy['Rain_02_mm'])
    elif 'Rain_01_mm' in df_copy.columns:
        df_copy['Rain_mm'] = df_copy['Rain_01_mm']
    elif 'Rain_02_mm' in df_copy.columns:
        df_copy['Rain_mm'] = df_copy['Rain_02_mm']
    else:
        df_copy['Rain_mm'] = np.nan
        warnings.warn("Ni 'Rain_01_mm' ni 'Rain_02_mm' ne sont présents pour créer 'Rain_mm'. 'Rain_mm' est rempli de NaN.")
    return df_copy

def create_datetime(df: pd.DataFrame, bassin: str = None, station: str = None) -> pd.DataFrame:
    """
    Crée la colonne 'Datetime' à partir de colonnes séparées (Year, Month, Day, Hour, Minute)
    ou à partir d'une colonne 'Date' pour le bassin VEA_SISSILI.

    Args:
        df (pd.DataFrame): DataFrame d'entrée.
        bassin (str, optional): Nom du bassin ('DANO', 'DASSARI', 'VEA_SISSILI').
        station (str, optional): Nom de la station pour un traitement spécifique.

    Returns:
        pd.DataFrame: DataFrame avec la colonne 'Datetime' et ses composantes, si possible.
    """
    df_copy = df.copy()
    
    if bassin and station:
        df_copy = apply_station_specific_preprocessing(df_copy, bassin, station)

    if 'Date' in df_copy.columns and (bassin == 'VEA_SISSILI' or not any(col in df_copy.columns for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'])):
        try:
            df_copy['Datetime'] = pd.to_datetime(df_copy['Date'], errors='coerce')
        except Exception as e:
            warnings.warn(f"Impossible de convertir la colonne 'Date' en Datetime pour le bassin {bassin}: {e}")
            df_copy['Datetime'] = pd.NaT
    else:
        date_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute']
        
        for col in date_cols:
            if col in df_copy.columns:
                df_copy[col] = pd.to_numeric(df_copy[col], errors='coerce')

        try:
            existing_date_components = [col for col in ['Year', 'Month', 'Day', 'Hour', 'Minute'] if col in df_copy.columns]
            
            if not existing_date_components:
                raise ValueError("Aucune colonne de composantes de date/heure (Year, Month, Day, Hour, Minute) trouvée.")

            date_strings = df_copy.apply(
                lambda row: f"{int(row.get('Year', 2000))}-"
                           f"{int(row.get('Month', 1)):02d}-"
                           f"{int(row.get('Day', 1)):02d} "
                           f"{int(row.get('Hour', 0)):02d}:"
                           f"{int(row.get('Minute', 0)):02d}",
                axis=1
            )
            df_copy['Datetime'] = pd.to_datetime(date_strings, errors='coerce')
            
        except Exception as e:
            warnings.warn(f"Impossible de créer Datetime à partir des colonnes séparées. Erreur: {e}. Colonnes présentes: {df_copy.columns.tolist()}")
            df_copy['Datetime'] = pd.NaT
            
    if 'Datetime' in df_copy.columns and df_copy['Datetime'].notna().any():
        df_copy['Year'] = df_copy['Datetime'].dt.year
        df_copy['Month'] = df_copy['Datetime'].dt.month
        df_copy['Day'] = df_copy['Datetime'].dt.day
        df_copy['Hour'] = df_copy['Datetime'].dt.hour
        df_copy['Minute'] = df_copy['Datetime'].dt.minute
        if 'Date' not in df_copy.columns or not pd.api.types.is_datetime64_any_dtype(df_copy['Date']):
             df_copy['Date'] = df_copy['Datetime'].dt.date
    else:
        warnings.warn("La colonne 'Datetime' est vide ou n'existe pas après la tentative de création. Composantes de date/heure non extraites.")

    return df_copy

def interpolation(df: pd.DataFrame, limits: dict, df_gps: pd.DataFrame) -> pd.DataFrame:
    """
    Effectue toutes les interpolations météorologiques en une seule passe.
    Cette fonction DOIT recevoir un DataFrame avec un DatetimeIndex.
    Il doit également contenir une colonne 'Station'.

    Args:
        df (pd.DataFrame): Le DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
        limits (dict): Dictionnaire définissant les limites de valeurs pour chaque variable.
        df_gps (pd.DataFrame): Le DataFrame contenant les informations de station
                               (colonnes 'Station', 'Lat', 'Long', 'Timezone').

    Returns:
        pd.DataFrame: Le DataFrame original avec les données interpolées,
                      la colonne 'Is_Daylight' calculée, la durée du jour, et un DatetimeIndex.
    """
    df_processed = df.copy()

    if not isinstance(df_processed.index, pd.DatetimeIndex):
        raise TypeError("Le DataFrame d'entrée pour l'interpolation DOIT avoir un DatetimeIndex.")
    
    initial_rows = len(df_processed)
    df_processed = df_processed[df_processed.index.notna()]
    if len(df_processed) == 0:
        raise ValueError("Après nettoyage des index temporels manquants, le DataFrame est vide. Impossible de procéder à l'interpolation.")
    if initial_rows - len(df_processed) > 0:
        warnings.warn(f"Suppression de {initial_rows - len(df_processed)} lignes avec index Datetime manquant ou invalide dans l'interpolation.")
    
    print(f"DEBUG (interpolation): Type de l'index du DataFrame initial: {type(df_processed.index)}")
    print(f"DEBUG (interpolation): Premières 5 valeurs de l'index après nettoyage des NaT: {df_processed.index[:5].tolist() if not df_processed.empty else 'DataFrame vide'}")

    required_gps_cols = ['Station', 'Lat', 'Long', 'Timezone']
    if not all(col in df_gps.columns for col in required_gps_cols):
        raise ValueError(
            f"df_gps doit contenir les colonnes {required_gps_cols}. "
            f"Colonnes actuelles dans df_gps : {df_gps.columns.tolist()}"
        )

    if not df_gps['Station'].is_unique:
        print("Avertissement: La colonne 'Station' dans df_gps contient des noms de station dupliqués.")
        print("Ceci peut entraîner des comportements inattendus ou des stations non reconnues.")
        df_gps_unique = df_gps.drop_duplicates(subset=['Station'], keep='first').copy()
        print(f"Suppression de {len(df_gps) - len(df_gps_unique)} doublons dans df_gps (en gardant la première occurrence).")
    else:
        df_gps_unique = df_gps.copy()

    gps_info_dict = df_gps_unique.set_index('Station')[['Lat', 'Long', 'Timezone']].to_dict('index')

    numerical_cols = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
                      'Solar_R_W/m^2', 'Wind_Dir_Deg']
    for col in numerical_cols:
        if col in df_processed.columns:
            df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')

    df_processed_parts = []

    for station_name, group in df_processed.groupby('Station'):
        group_copy = group.copy()
        print(f"DEBUG (interpolation/groupby): Début du traitement du groupe '{station_name}'.")
        
        if group_copy.index.tz is None:
            group_copy.index = group_copy.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')
        elif group_copy.index.tz != pytz.utc:
            group_copy.index = group_copy.index.tz_convert('UTC')
        print(f"DEBUG (interpolation/groupby): Index Datetime pour '{station_name}' STANDARDISÉ à UTC. Dtype: {group_copy.index.dtype}")
        
        group_copy = group_copy[group_copy.index.notna()]
        if group_copy.empty:
            warnings.warn(f"Le groupe '{station_name}' est vide après nettoyage de l'index Datetime. Il sera ignoré.")
            continue

        apply_fixed_daylight = True
        gps_data = gps_info_dict.get(station_name)
        if gps_data and pd.notna(gps_data.get('Lat')) and pd.notna(gps_data.get('Long')) and pd.notna(gps_data.get('Timezone')):
            lat = gps_data['Lat']
            long = gps_data['Long']
            timezone_str = gps_data['Timezone']

            try:
                local_tz = pytz.timezone(timezone_str)
                index_for_astral_local = group_copy.index.tz_convert(local_tz)

                daily_sun_info = {}
                unique_dates_ts_local = index_for_astral_local.normalize().drop_duplicates()

                if unique_dates_ts_local.empty:
                    raise ValueError("No unique dates found for Astral calculation.")
                
                for ts_local_aware in unique_dates_ts_local:
                    loc = LocationInfo(station_name, "Site", timezone_str, lat, long)
                    naive_date_for_astral = ts_local_aware.to_pydatetime().date()
                    s = sun.sun(loc.observer, date=naive_date_for_astral) 
                    daily_sun_info[naive_date_for_astral] = {
                        'sunrise': s['sunrise'],
                        'sunset': s['sunset']
                    }

                naive_unique_dates_for_index = [ts.date() for ts in unique_dates_ts_local]
                temp_df_sun_index = pd.Index(naive_unique_dates_for_index, name='Date_Local_Naive')
                temp_df_sun = pd.DataFrame(index=temp_df_sun_index)
                
                print(f"DEBUG (astral_calc): unique_dates_ts_local type: {type(unique_dates_ts_local)}")
                print(f"DEBUG (astral_calc): naive_unique_dates_for_index type: {type(naive_unique_dates_for_index)}")
                print(f"DEBUG (astral_calc): temp_df_sun_index type: {type(temp_df_sun_index)}")
                if not temp_df_sun.empty:
                    print(f"DEBUG (astral_calc): First element of temp_df_sun.index: {temp_df_sun.index[0]}")
                    print(f"DEBUG (astral_calc): Type of first element of temp_df_sun.index: {type(temp_df_sun.index[0])}")

                temp_df_sun['sunrise_time_local'] = [daily_sun_info.get(date, {}).get('sunrise') for date in temp_df_sun.index]
                temp_df_sun['sunset_time_local'] = [daily_sun_info.get(date, {}).get('sunset') for date in temp_df_sun.index]

                group_copy_reset = group_copy.reset_index()
                group_copy_reset['Date_Local_Naive'] = group_copy_reset['Datetime'].dt.tz_convert(local_tz).dt.date

                group_copy_reset = pd.merge(group_copy_reset, temp_df_sun, on='Date_Local_Naive', how='left')

                group_copy_reset['sunrise_time_utc'] = group_copy_reset['sunrise_time_local'].dt.tz_convert('UTC')
                group_copy_reset['sunset_time_utc'] = group_copy_reset['sunset_time_local'].dt.tz_convert('UTC')

                group_copy_reset.loc[:, 'Is_Daylight'] = (group_copy_reset['Datetime'] >= group_copy_reset['sunrise_time_utc']) & \
                                                          (group_copy_reset['Datetime'] < group_copy_reset['sunset_time_utc'])

                daylight_timedelta_local = group_copy_reset['sunset_time_local'] - group_copy_reset['sunrise_time_local']
                
                def format_timedelta_to_hms(td):
                    if pd.isna(td):
                        return np.nan
                    total_seconds = int(td.total_seconds())
                    hours = total_seconds // 3600
                    minutes = (total_seconds % 3600) // 60
                    seconds = total_seconds % 60
                    return f"{hours:02d}:{minutes:02d}:{seconds:02d}"

                group_copy_reset.loc[:, 'Daylight_Duration'] = daylight_timedelta_local.apply(format_timedelta_to_hms)

                group_copy = group_copy_reset.set_index('Datetime')
                group_copy = group_copy.drop(columns=['Date_Local_Naive', 'sunrise_time_local', 'sunset_time_local', 'sunrise_time_utc', 'sunset_time_utc'], errors='ignore')

                print(f"Lever et coucher du soleil calculés pour {station_name}.")
                apply_fixed_daylight = False

            except Exception as e:
                print(f"Erreur lors du calcul du lever/coucher du soleil avec Astral pour {station_name}: {e}.")
                traceback.print_exc()
                warnings.warn(f"Calcul Astral impossible pour '{station_name}'. Utilisation de l'indicateur jour/nuit fixe.")
                apply_fixed_daylight = True
        else:
            print(f"Avertissement: Coordonnées ou Fuseau horaire manquants/invalides pour le site '{station_name}' dans df_gps. Utilisation de l'indicateur jour/nuit fixe.")
            apply_fixed_daylight = True

        if apply_fixed_daylight:
            group_copy.loc[:, 'Is_Daylight'] = (group_copy.index.hour >= 7) & (group_copy.index.hour <= 18)
            group_copy.loc[:, 'Daylight_Duration'] = "11:00:00"
            print(f"Utilisation de l'indicateur jour/nuit fixe (7h-18h) pour {station_name}.")

        df_processed_parts.append(group_copy)

    if not df_processed_parts:
        raise ValueError("Aucune partie de DataFrame n'a pu être traitée après le regroupement par station.")

    df_final = pd.concat(df_processed_parts).sort_index()
    df_final.index.name = 'Datetime' 
    print(f"DEBUG (interpolation/concat): Index du DataFrame final après concaténation et tri: {type(df_final.index)}")
    print(f"DEBUG (interpolation/concat): Colonnes du DataFrame final après concaténation: {df_final.columns.tolist()}")

    cols_to_drop_after_process = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Date']
    df_final = df_final.drop(columns=cols_to_drop_after_process, errors='ignore')

    if 'Rain_mm' not in df_final.columns or df_final['Rain_mm'].isnull().all():
        if 'Rain_01_mm' in df_final.columns and 'Rain_02_mm' in df_final.columns:
            df_final = create_rain_mm(df_final)
            warnings.warn("Colonne Rain_mm créée à partir des deux capteurs.")
        else:
            warnings.warn("Rain_mm manquant et impossible à créer (capteurs pluie incomplets).")
            if 'Rain_mm' not in df_final.columns:
                df_final['Rain_mm'] = np.nan

    standard_vars = ['Air_Temp_Deg_C', 'Rel_H_%', 'BP_mbar_Avg',
                      'Rain_01_mm', 'Rain_02_mm', 'Rain_mm', 'Wind_Sp_m/sec',
                      'Solar_R_W/m^2', 'Wind_Dir_Deg']

    for var in standard_vars:
        if var in df_final.columns:
            df_final[var] = pd.to_numeric(df_final[var], errors='coerce')
            if var in limits:
                min_val = limits[var]['min']
                max_val = limits[var]['max']
                initial_nan_count = df_final[var].isna().sum()
                if min_val is not None:
                    df_final.loc[df_final[var] < min_val, var] = np.nan
                if max_val is not None:
                    df_final.loc[df_final[var] > max_val, var] = np.nan
                
                new_nan_count = df_final[var].isna().sum()
                if new_nan_count > initial_nan_count:
                    warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
            
            print(f"DEBUG (interpolation/variable): Interpolation de '{var}'. Type de l'index de df_final: {type(df_final.index)}")
            
            if isinstance(df_final.index, pd.DatetimeIndex):
                df_final[var] = df_final[var].interpolate(method='time', limit_direction='both')
            else:
                print(f"Avertissement (interpolation/variable): L'index n'est pas un DatetimeIndex pour l'interpolation de '{var}'. Utilisation de la méthode 'linear'.")
                df_final[var] = df_final[var].interpolate(method='linear', limit_direction='both')
            df_final[var] = df_final[var].bfill().ffill()

    if 'Solar_R_W/m^2' in df_final.columns:
        df_final['Solar_R_W/m^2'] = pd.to_numeric(df_final['Solar_R_W/m^2'], errors='coerce')

        if 'Solar_R_W/m^2' in limits:
            min_val = limits['Solar_R_W/m^2']['min']
            max_val = limits['Solar_R_W/m^2']['max']
            initial_nan_count = df_final['Solar_R_W/m^2'].isna().sum()
            df_final.loc[(df_final['Solar_R_W/m^2'] < min_val) | (df_final['Solar_R_W/m^2'] > max_val), 'Solar_R_W/m^2'] = np.nan
            if df_final['Solar_R_W/m^2'].isna().sum() > initial_nan_count:
                warnings.warn(f"Remplacement de {df_final['Solar_R_W/m^2'].isna().sum() - initial_nan_count} valeurs hors limites dans 'Solar_R_W/m^2' par NaN.")

        if 'Is_Daylight' in df_final.columns:
            df_final.loc[~df_final['Is_Daylight'] & (df_final['Solar_R_W/m^2'] > 0), 'Solar_R_W/m^2'] = 0

            if 'Rain_mm' in df_final.columns:
                cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0) & (df_final['Rain_mm'] == 0)
            else:
                cond_suspect_zeros = (df_final['Is_Daylight']) & (df_final['Solar_R_W/m^2'] == 0)
                warnings.warn("Rain_mm manquant. Tous les 0 de radiation solaire pendant le jour sont traités comme suspects.")
            df_final.loc[cond_suspect_zeros, 'Solar_R_W/m^2'] = np.nan

            print(f"DEBUG (interpolation/solaire): Interpolation de 'Solar_R_W/m^2' (conditionnel). Type de l'index de df_final: {type(df_final.index)}")

            is_day = df_final['Is_Daylight']
            if isinstance(df_final.index, pd.DatetimeIndex):
                df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='time', limit_direction='both')
            else:
                print(f"Avertissement (interpolation/solaire): L'index n'est pas un DatetimeIndex pour l'interpolation de 'Solar_R_W/m^2'. Utilisation de la méthode 'linear'.")
                df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both')

            df_final.loc[is_day, 'Solar_R_W/m^2'] = df_final.loc[is_day, 'Solar_R_W/m^2'].bfill().ffill()

            df_final.loc[~is_day & df_final['Solar_R_W/m^2'].isna(), 'Solar_R_W/m^2'] = 0
            warnings.warn("Radiation solaire interpolée avec succès.")
        else:
            warnings.warn("Colonne 'Is_Daylight' manquante. Radiation solaire interpolée standard.")
            if isinstance(df_final.index, pd.DatetimeIndex):
                 df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='time', limit_direction='both').bfill().ffill()
            else:
                 df_final['Solar_R_W/m^2'] = df_final['Solar_R_W/m^2'].interpolate(method='linear', limit_direction='both').bfill().ffill()

    warnings.warn("Vérification des valeurs manquantes après interpolation:")
    missing_after_interp = df_final.isna().sum()
    columns_with_missing = missing_after_interp[missing_after_interp > 0]
    if not columns_with_missing.empty:
        warnings.warn(f"Valeurs manquantes persistantes:\n{columns_with_missing}")
    else:
        warnings.warn("Aucune valeur manquante après l'interpolation.")

    return df_final

def convert_utm_df_to_gps(df: pd.DataFrame) -> pd.DataFrame:
    """
    Convertit un DataFrame contenant des colonnes 'Easting', 'Northing', 'zone', 'hemisphere'
    de coordonnées UTM vers latitude/longitude WGS84.

    Args:
        df (pd.DataFrame): DataFrame d'entrée avec colonnes UTM.

    Returns:
        pd.DataFrame: DataFrame avec les colonnes 'Long' et 'Lat' (GPS) et sans les colonnes UTM.
    """
    df_copy = df.copy()

    required_utm_cols = ['Easting', 'Northing', 'zone', 'hemisphere']
    if not all(col in df_copy.columns for col in required_utm_cols):
        raise ValueError(
            f"Le DataFrame doit contenir les colonnes {required_utm_cols} pour la conversion UTM."
        )

    def convert_row(row):
        try:
            zone = int(row['zone'])
            hemisphere = str(row['hemisphere']).upper()
            is_northern = hemisphere == 'N'

            proj_utm = CRS.from_proj4(
                f"+proj=utm +zone={zone} +datum=WGS84 +units=m +{'north' if is_northern else 'south'}"
            )
            proj_wgs84 = CRS.from_epsg(4326)

            transformer = Transformer.from_crs(proj_utm, proj_wgs84, always_xy=True)
            lon, lat = transformer.transform(row['Easting'], row['Northing'])
            return pd.Series({'Long': lon, 'Lat': lat})
        except Exception as e:
            warnings.warn(f"Erreur lors de la conversion UTM d'une ligne: {e}")
            return pd.Series({'Long': pd.NA, 'Lat': pd.NA})

    df_copy[['Long', 'Lat']] = df_copy.apply(convert_row, axis=1)
    df_copy = df_copy.drop(columns=['Easting', 'Northing', 'hemisphere', 'zone'], errors='ignore')

    return df_copy

def _load_and_prepare_gps_data() -> pd.DataFrame:
    """
    Charge les fichiers de coordonnées des stations depuis Google Drive,
    les prétraite (suppression/ajout de colonnes/lignes, renommage),
    convertit les coordonnées UTM en GPS pour Dano et Dassari,
    ajoute les fuseaux horaires, et fusionne tous les bassins en un seul DataFrame.
    """
    print("Début de la préparation des données de coordonnées des stations...")
    data_dir = 'data'
    os.makedirs(data_dir, exist_ok=True)

    files_info = [
        {'id': '1Iz5L_XkumG390EZvnMgYr3KwDYeesrNz', 'name': "WASCAL Basins Climate Station Coordinates.xlsx", 'bassin': 'Vea Sissili'},
        {'id': '1H8A-sVMtTok6lrD-NFHQxzHBeQ_P7g4z', 'name': "Dano Basins Climate Station Coordinates.xlsx", 'bassin': 'Dano'},
        {'id': '1SOXI0ZvWqpNp6Qwz_BGeWleUtaYMaOBU', 'name': "DASSARI Climate Station Coordinates.xlsx", 'bassin': 'Dassari'}
    ]

    loaded_dfs = []

    for file_info in files_info:
        output_file_path = os.path.join(data_dir, file_info['name'])
        
        if not os.path.exists(output_file_path):
            print(f"Téléchargement de {file_info['bassin']} depuis Google Drive...")
            gdown.download(f'https://drive.google.com/uc?id={file_info["id"]}', output_file_path, quiet=False)
            print(f"Téléchargement de {file_info['bassin']} terminé.")
        else:
            print(f"Chargement de {file_info['bassin']} depuis le cache local: {output_file_path}")
        
        loaded_dfs.append(pd.read_excel(output_file_path))

    vea_sissili_bassin = loaded_dfs[0]
    dano_bassin = loaded_dfs[1]
    dassari_bassin = loaded_dfs[2]

    print("Début du prétraitement des données de stations...")
    
    vea_sissili_bassin = vea_sissili_bassin.drop(columns=['No', 'Location', 'parameters'], errors='ignore')
    new_row_df_vea = pd.DataFrame([{'Name': 'Atampisi', 'Lat': 10.91501, 'Long': -0.82647}])
    vea_sissili_bassin = pd.concat([vea_sissili_bassin, new_row_df_vea], ignore_index=True)

    dassari_bassin = dassari_bassin.drop(columns=['Altitude (en m)'], errors='ignore')
    new_rows_df_dassari = pd.DataFrame([{'Site name': 'Pouri', 'Lat': 1207107, 'Long': 293642}, {'Site name': 'Fandohoun', 'Lat': 1207107, 'Long': 293642}])
    dassari_bassin = pd.concat([dassari_bassin, new_rows_df_dassari], ignore_index=True)

    dano_bassin = dano_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site Name': 'Name'})
    dassari_bassin = dassari_bassin.rename(columns={'Long': 'Easting', 'Lat': 'Northing', 'Site name': 'Name'})

    dano_bassin['zone'] = 30
    dano_bassin['hemisphere'] = 'N'
    dassari_bassin['zone'] = 31
    dassari_bassin['hemisphere'] = 'N'

    dano_bassin = convert_utm_df_to_gps(dano_bassin)
    dassari_bassin = convert_utm_df_to_gps(dassari_bassin)

    dano_bassin['Timezone'] = 'Africa/Ouagadougou'
    dassari_bassin['Timezone'] = 'Africa/Porto-Novo'
    vea_sissili_bassin['Timezone'] = 'Africa/Accra'

    bassins = pd.concat([vea_sissili_bassin, dano_bassin, dassari_bassin], ignore_index=True)

    bassins = bassins.rename(columns={'Name': 'Station'})

    initial_rows = len(bassins)
    bassins = bassins.dropna(subset=['Lat', 'Long', 'Timezone', 'Station'])
    if len(bassins) < initial_rows:
        print(f"Attention: {initial_rows - len(bassins)} lignes avec des coordonnées ou fuseaux horaires manquants ont été supprimées du DataFrame des stations.")
    
    output_json_path = os.path.join(data_dir, "station_coordinates.json")
    bassins.to_json(output_json_path, orient='records', indent=4)
    print(f"\nPréparation des données terminée. Coordonnées des stations sauvegardées dans '{output_json_path}'.")
    print("Vous pouvez maintenant lancer votre application Flask.")

    return bassins

def gestion_doublons(df: pd.DataFrame) -> pd.DataFrame:
    """
    Gère les doublons dans le DataFrame en se basant sur les colonnes 'Station' et 'Datetime'.
    Conserve la première occurrence en cas de doublon.

    Args:
        df (pd.DataFrame): Le DataFrame d'entrée.

    Returns:
        pd.DataFrame: Le DataFrame sans doublons.
    """
    if 'Station' in df.columns and 'Datetime' in df.columns:
        initial_rows = len(df)
        df_cleaned = df.drop_duplicates(subset=['Station', 'Datetime'], keep='first')
        if len(df_cleaned) < initial_rows:
            warnings.warn(f"Suppression de {initial_rows - len(df_cleaned)} doublons basés sur 'Station' et 'Datetime'.")
        return df_cleaned
    else:
        warnings.warn("Colonnes 'Station' ou 'Datetime' manquantes pour la gestion des doublons. Le DataFrame n'a pas été modifié.")
        return df

def traiter_outliers_meteo(df: pd.DataFrame, limits: dict) -> pd.DataFrame:
    """
    Remplace les valeurs aberrantes par NaN pour toutes les variables météorologiques spécifiées.

    Args:
        df (pd.DataFrame): DataFrame d'entrée avec DatetimeIndex et colonne 'Station'.
        limits (dict): Dictionnaire avec les limites min/max pour chaque variable.

    Returns:
        pd.DataFrame: DataFrame avec les valeurs aberrantes remplacées par NaN.
    """
    df_processed = df.copy()

    if not isinstance(df_processed.index, pd.DatetimeIndex):
        warnings.warn("L'index n'est pas un DatetimeIndex dans traiter_outliers_meteo. Tentative de conversion.")
        try:
            df_processed.index = pd.to_datetime(df_processed.index, errors='coerce')
            df_processed = df_processed[df_processed.index.notna()]
            if df_processed.empty:
                raise ValueError("DataFrame vide après nettoyage des dates invalides dans traiter_outliers_meteo.")
        except Exception as e:
            raise TypeError(f"Impossible de garantir un DatetimeIndex pour traiter_outliers_meteo: {e}")

    for var, vals in limits.items():
        if var in df_processed.columns:
            min_val = vals.get('min')
            max_val = vals.get('max')
            if min_val is not None or max_val is not None:
                initial_nan_count = df_processed[var].isna().sum()
                if min_val is not None:
                    df_processed.loc[df_processed[var] < min_val, var] = np.nan
                if max_val is not None:
                    df_processed.loc[df_processed[var] > max_val, var] = np.nan
                
                new_nan_count = df_processed[var].isna().sum()
                if new_nan_count > initial_nan_count:
                    warnings.warn(f"Remplacement de {new_nan_count - initial_nan_count} valeurs hors limites dans '{var}' par NaN.")
    return df_processed

def generer_graphique_par_variable_et_periode(df: pd.DataFrame, station: str, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
    """
    Génère un graphique Plotly de l'évolution d'une variable pour une station sur une période donnée.
    Retourne l'objet Figure Plotly.
    """
    if not isinstance(df.index, pd.DatetimeIndex):
        raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique par variable et période.")

    filtered_df = df[df['Station'] == station].copy()
    if filtered_df.empty:
        return go.Figure()

    if periode == 'Journalière':
        resampled_df = filtered_df[variable].resample('D').mean()
    elif periode == 'Hebdomadaire':
        resampled_df = filtered_df[variable].resample('W').mean()
    elif periode == 'Mensuelle':
        resampled_df = filtered_df[variable].resample('M').mean()
    elif periode == 'Annuelle':
        resampled_df = filtered_df[variable].resample('Y').mean()
    else:
        resampled_df = filtered_df[variable]

    resampled_df = resampled_df.dropna()

    if resampled_df.empty:
        return go.Figure()

    variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
    color = colors.get(station, '#1f77b4')

    fig = go.Figure()
    fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
                             mode='lines', name=f'{variable_meta["Nom"]} - {station}',
                             line=dict(color=color)))

    fig.update_layout(
        title=f"Évolution de {variable_meta['Nom']} ({variable_meta['Unite']}) pour {station} ({periode})",
        xaxis_title="Date",
        yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
        hovermode="x unified",
        template="plotly_white"
    )
    return fig

def generer_graphique_comparatif(df: pd.DataFrame, variable: str, periode: str, colors: dict, metadata: dict) -> go.Figure:
    """
    Génère un graphique Plotly comparatif de l'évolution d'une variable entre toutes les stations.
    Retourne l'objet Figure Plotly.
    """
    if not isinstance(df.index, pd.DatetimeIndex):
        raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique comparatif.")

    fig = go.Figure()
    
    all_stations = df['Station'].unique()
    if len(all_stations) < 2:
        warnings.warn("Moins de 2 stations disponibles pour la comparaison. Le graphique comparatif ne sera pas généré.")
        return go.Figure()

    for station in all_stations:
        filtered_df = df[df['Station'] == station].copy()
        if filtered_df.empty:
            continue

        if periode == 'Journalière':
            resampled_df = filtered_df[variable].resample('D').mean()
        elif periode == 'Hebdomadaire':
            resampled_df = filtered_df[variable].resample('W').mean()
        elif periode == 'Mensuelle':
            resampled_df = filtered_df[variable].resample('M').mean()
        elif periode == 'Annuelle':
            resampled_df = filtered_df[variable].resample('Y').mean()
        else:
            resampled_df = filtered_df[variable]

        resampled_df = resampled_df.dropna()
        if resampled_df.empty:
            continue
        
        color = colors.get(station, '#1f77b4')
        fig.add_trace(go.Scatter(x=resampled_df.index, y=resampled_df.values,
                                 mode='lines', name=station,
                                 line=dict(color=color)))

    if not fig.data:
        return go.Figure()

    variable_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})
    fig.update_layout(
        title=f"Comparaison de {variable_meta['Nom']} ({variable_meta['Unite']}) entre stations ({periode})",
        xaxis_title="Date",
        yaxis_title=f"{variable_meta['Nom']} ({variable_meta['Unite']})",
        hovermode="x unified",
        legend_title="Variables",
        template="plotly_white"
    )
    return fig

def generate_multi_variable_station_plot(df: pd.DataFrame, station: str, colors: dict, metadata: dict) -> go.Figure:
    """
    Génère un graphique Plotly de l'évolution normalisée de plusieurs variables pour une station donnée.
    Retourne l'objet Figure Plotly.
    """
    if not isinstance(df.index, pd.DatetimeIndex):
        raise TypeError("Le DataFrame doit avoir un DatetimeIndex pour générer le graphique multi-variables.")

    filtered_df = df[df['Station'] == station].copy()
    if filtered_df.empty:
        return go.Figure()

    numerical_vars = [col for col in filtered_df.columns if pd.api.types.is_numeric_dtype(filtered_df[col]) and col not in ['Station', 'Is_Daylight', 'Daylight_Duration']]

    if not numerical_vars:
        warnings.warn("Aucune variable numérique trouvée pour la station sélectionnée.")
        return go.Figure()

    normalized_df = filtered_df[numerical_vars].copy()
    for col in normalized_df.columns:
        min_val = normalized_df[col].min()
        max_val = normalized_df[col].max()
        if max_val != min_val:
            normalized_df[col] = (normalized_df[col] - min_val) / (max_val - min_val)
        else:
            normalized_df[col] = 0.5 if pd.notna(min_val) else np.nan

    normalized_df = normalized_df.dropna(how='all')

    if normalized_df.empty:
        return go.Figure()
    
    fig = go.Figure()
    for var in normalized_df.columns:
        var_meta = metadata.get(var, {'Nom': var, 'Unite': ''})
        color = colors.get(var, None)

        fig.add_trace(go.Scatter(x=normalized_df.index, y=normalized_df[var],
                                 mode='lines', name=var_meta['Nom'],
                                 line=dict(color=color)))

    fig.update_layout(
        title=f"Évolution Normalisée des Variables Météorologiques pour la station {station}",
        xaxis_title="Date",
        yaxis_title="Valeur Normalisée (0-1)",
        hovermode="x unified",
        legend_title="Variables",
        template="plotly_white"
    )
    return fig

def calculate_daily_summary_table(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
    groupées par station.
    """
    df_copy = df.copy()

    if isinstance(df_copy.index, pd.DatetimeIndex):
        df_copy = df_copy.reset_index()

    df_copy['Datetime'] = pd.to_datetime(df_copy['Datetime'], errors='coerce')
    df_copy = df_copy.dropna(subset=['Datetime', 'Station'])

    if df_copy.empty:
        print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans calculate_daily_summary_table.")
        return pd.DataFrame()

    if 'Is_Daylight' not in df_copy.columns:
        warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
        df_copy['Is_Daylight'] = (df_copy['Datetime'].dt.hour >= 7) & (df_copy['Datetime'].dt.hour <= 18)

    numerical_cols = [col for col in df_copy.columns if pd.api.types.is_numeric_dtype(df_copy[col]) and col not in ['Station', 'Datetime', 'Is_Daylight', 'Daylight_Duration']]
    
    if not numerical_cols:
        warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
        return pd.DataFrame()

    daily_aggregated_df = df_copy.groupby(['Station', df_copy['Datetime'].dt.date]).agg({
        col: ['mean', 'min', 'max'] for col in numerical_cols if METADATA_VARIABLES.get(col, {}).get('is_rain') == False
    })

    daily_aggregated_df.columns = ['_'.join(col).strip() for col in daily_aggregated_df.columns.values]

    if 'Rain_mm' in df_copy.columns and METADATA_VARIABLES.get('Rain_mm', {}).get('is_rain'):
        df_daily_rain = df_copy.groupby(['Station', df_copy['Datetime'].dt.date])['Rain_mm'].sum().reset_index()
        df_daily_rain = df_daily_rain.rename(columns={'Rain_mm': 'Rain_mm_sum'})

        if not daily_aggregated_df.empty:
            daily_aggregated_df = daily_aggregated_df.reset_index()
            daily_stats_df = pd.merge(daily_aggregated_df, df_daily_rain, on=['Station', 'Datetime'], how='left')
            daily_stats_df = daily_stats_df.rename(columns={'Datetime': 'Date'})
        else:
            daily_stats_df = df_daily_rain.rename(columns={'Datetime': 'Date'})
    else:
        daily_stats_df = daily_aggregated_df.reset_index().rename(columns={'Datetime': 'Date'})

    if 'Rain_mm' in df_copy.columns and METADATA_VARIABLES.get('Rain_mm', {}).get('is_rain'):
        df_daily_rain_raw = df_copy.groupby(['Station', pd.Grouper(key='Datetime', freq='D')])['Rain_mm'].sum().reset_index()
        
        RAIN_SEASON_GAP_THRESHOLD = pd.Timedelta(days=60)
        season_stats = []
        for station_name, station_df_rain in df_daily_rain_raw.groupby('Station'):
            station_df_rain = station_df_rain.set_index('Datetime').sort_index()
            rain_events = station_df_rain[station_df_rain['Rain_mm'] > 0].index

            if rain_events.empty:
                season_stats.append({'Station': station_name, 'Moyenne_Saison_Pluvieuse': np.nan, 'Debut_Saison_Pluvieuse': pd.NaT, 'Fin_Saison_Pluvieuse': pd.NaT, 'Duree_Saison_Pluvieuse_Jours': np.nan})
                continue
            
            block_ids = (rain_events.to_series().diff() > RAIN_SEASON_GAP_THRESHOLD).cumsum()
            valid_blocks = {}
            for block_id, rain_dates_in_block in rain_events.to_series().groupby(block_ids):
                if not rain_dates_in_block.empty:
                    block_start = rain_dates_in_block.min()
                    block_end = rain_dates_in_block.max()
                    full_block_df = station_df_rain.loc[block_start:block_end]
                    valid_blocks[block_id] = full_block_df

            if not valid_blocks:
                season_stats.append({'Station': station_name, 'Moyenne_Saison_Pluvieuse': np.nan, 'Debut_Saison_Pluvieuse': pd.NaT, 'Fin_Saison_Pluvieuse': pd.NaT, 'Duree_Saison_Pluvieuse_Jours': np.nan})
                continue

            main_block_id = max(valid_blocks, key=lambda k: (valid_blocks[k].index.max() - valid_blocks[k].index.min()).days)
            main_season_df = valid_blocks[main_block_id]

            debut_saison = main_season_df.index.min()
            fin_saison = main_season_df.index.max()
            total_days = (fin_saison - debut_saison).days + 1
            moyenne_saison = main_season_df['Rain_mm'].sum() / total_days if total_days > 0 else 0

            season_stats.append({
                'Station': station_name,
                'Moyenne_Saison_Pluvieuse': moyenne_saison,
                'Debut_Saison_Pluvieuse': debut_saison,
                'Fin_Saison_Pluvieuse': fin_saison,
                'Duree_Saison_Pluvieuse_Jours': total_days
            })
        df_season_stats = pd.DataFrame(season_stats)
        
        if not df_season_stats.empty:
            daily_stats_df = pd.merge(daily_stats_df, df_season_stats, on='Station', how='left')

    final_stats_per_station = pd.DataFrame()
    for station_name in df_copy['Station'].unique():
        station_df = df_copy[df_copy['Station'] == station_name].copy()
        station_summary = {'Station': station_name}

        for var in numerical_cols:
            if var in station_df.columns and pd.api.types.is_numeric_dtype(station_df[var]):
                if var == 'Solar_R_W/m^2':
                    var_data = station_df.loc[station_df['Is_Daylight'], var].dropna()
                else:
                    var_data = station_df[var].dropna()
                
                if not var_data.empty:
                    station_summary[f'{var}_Maximum'] = var_data.max()
                    station_summary[f'{var}_Minimum'] = var_data.min()
                    station_summary[f'{var}_Moyenne'] = var_data.mean()
                    station_summary[f'{var}_Mediane'] = var_data.median()
                    
                    if var == 'Rain_mm':
                        station_summary[f'{var}_Cumul_Annuel'] = station_df['Rain_mm'].sum()
                        rainy_days_data = station_df[station_df['Rain_mm'] > 0]['Rain_mm'].dropna()
                        station_summary[f'{var}_Moyenne_Jours_Pluvieux'] = rainy_days_data.mean() if not rainy_days_data.empty else np.nan

                        if 'Duree_Saison_Pluvieuse_Jours' in daily_stats_df.columns:
                            s_data = daily_stats_df[daily_stats_df['Station'] == station_name]
                            if not s_data.empty:
                                station_summary[f'{var}_Duree_Saison_Pluvieuse_Jours'] = s_data['Duree_Saison_Pluvieuse_Jours'].iloc[0]
                                station_summary[f'{var}_Duree_Secheresse_Definie_Jours'] = np.nan

        final_stats_per_station = pd.concat([final_stats_per_station, pd.DataFrame([station_summary])], ignore_index=True)
        
    return final_stats_per_station

def generate_variable_summary_plots_for_web(df: pd.DataFrame, station: str, variable: str, metadata: dict, palette: dict) -> plt.Figure:
    """
    Génère un graphique Matplotlib/Seaborn pour les statistiques agrégées d'une variable spécifique
    pour une station donnée.
    """
    df_station = df[df['Station'] == station].copy()

    if df_station.empty:
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f"Aucune donnée pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
        ax.axis('off')
        return fig

    if isinstance(df_station.index, pd.DatetimeIndex):
        df_station = df_station.reset_index()
    
    df_station['Datetime'] = pd.to_datetime(df_station['Datetime'], errors='coerce')
    df_station = df_station.dropna(subset=['Datetime', 'Station'])
    df_station = df_station.set_index('Datetime').sort_index()

    if df_station.empty:
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f"DataFrame vide après nettoyage des dates pour la station '{station}'.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
        ax.axis('off')
        return fig

    if 'Is_Daylight' not in df_station.columns:
        df_station['Is_Daylight'] = (df_station.index.hour >= 7) & (df_station.index.hour <= 18)

    var_meta = metadata.get(variable, {'Nom': variable, 'Unite': ''})

    stats_for_plot = {}
    metrics_to_plot = []
    
    if var_meta.get('is_rain', False) and variable == 'Rain_mm':
        df_daily_rain = df_station.groupby(pd.Grouper(freq='D'))['Rain_mm'].sum().reset_index()
        df_daily_rain = df_daily_rain.rename(columns={'Rain_mm': 'Rain_mm_sum'})
        df_daily_rain['Datetime'] = pd.to_datetime(df_daily_rain['Datetime'])
        df_daily_rain = df_daily_rain.set_index('Datetime').sort_index()

        RAIN_SEASON_GAP_THRESHOLD = pd.Timedelta(days=60)
        rain_events = df_daily_rain[df_daily_rain['Rain_mm_sum'] > 0].index

        s_moyenne_saison = np.nan
        s_duree_saison = np.nan
        s_debut_saison = pd.NaT
        s_fin_saison = pd.NaT

        if not rain_events.empty:
            block_ids = (rain_events.to_series().diff() > RAIN_SEASON_GAP_THRESHOLD).cumsum()
            valid_blocks = {}
            for block_id, rain_dates_in_block in rain_events.to_series().groupby(block_ids):
                if not rain_dates_in_block.empty:
                    block_start = rain_dates_in_block.min()
                    block_end = rain_dates_in_block.max()
                    full_block_df = df_daily_rain.loc[block_start:block_end]
                    valid_blocks[block_id] = full_block_df

            if valid_blocks:
                main_block_id = max(valid_blocks, key=lambda k: (valid_blocks[k].index.max() - valid_blocks[k].index.min()).days)
                main_season_df = valid_blocks[main_block_id]

                s_debut_saison = main_season_df.index.min()
                s_fin_saison = main_season_df.index.max()
                total_days_season = (s_fin_saison - s_debut_saison).days + 1
                s_moyenne_saison = main_season_df['Rain_mm_sum'].sum() / total_days_season if total_days_season > 0 else 0
                s_duree_saison = total_days_season

        longest_dry_spell = np.nan
        debut_secheresse = pd.NaT
        fin_secheresse = pd.NaT

        full_daily_series_rain = df_daily_rain['Rain_mm_sum'].resample('D').sum().fillna(0)
        rainy_days_index = full_daily_series_rain[full_daily_series_rain > 0].index

        if not rainy_days_index.empty and pd.notna(s_moyenne_saison) and s_moyenne_saison > 0:
            temp_dry_spells = []
            for i in range(1, len(rainy_days_index)):
                prev_rain_date = rainy_days_index[i-1]
                current_rain_date = rainy_days_index[i]
                dry_days_between_rains = (current_rain_date - prev_rain_date).days - 1

                if dry_days_between_rains > 0:
                    rain_prev_day = full_daily_series_rain.loc[prev_rain_date]
                    temp_debut = pd.NaT
                    temp_duree = 0
                    for j in range(1, dry_days_between_rains + 1):
                        current_dry_date = prev_rain_date + timedelta(days=j)
                        current_ratio = rain_prev_day / j
                        if current_ratio < s_moyenne_saison:
                            temp_debut = current_dry_date
                            temp_duree = (current_rain_date - temp_debut).days
                            break
                    if pd.notna(temp_debut) and temp_duree > 0:
                        temp_dry_spells.append({
                            'Duree': temp_duree,
                            'Debut': temp_debut,
                            'Fin': current_rain_date - timedelta(days=1)
                        })
            
            if temp_dry_spells:
                df_temp_dry = pd.DataFrame(temp_dry_spells)
                idx_max_dry = df_temp_dry['Duree'].idxmax()
                longest_dry_spell = df_temp_dry.loc[idx_max_dry, 'Duree']
                debut_secheresse = df_temp_dry.loc[idx_max_dry, 'Debut']
                fin_secheresse = df_temp_dry.loc[idx_max_dry, 'Fin']

        rain_data_for_stats = df_station[variable].dropna()
        
        stats_for_plot['Maximum'] = rain_data_for_stats.max() if not rain_data_for_stats.empty else np.nan
        stats_for_plot['Minimum'] = rain_data_for_stats.min() if not rain_data_for_stats.empty else np.nan
        stats_for_plot['Mediane'] = rain_data_for_stats.median() if not rain_data_for_stats.empty else np.nan
        stats_for_plot['Cumul_Annuel'] = df_station[variable].sum()
        
        rainy_days_data = df_station[df_station[variable] > 0][variable].dropna()
        stats_for_plot['Moyenne_Jours_Pluvieux'] = rainy_days_data.mean() if not rainy_days_data.empty else np.nan
        
        stats_for_plot['Moyenne_Saison_Pluvieuse'] = s_moyenne_saison
        stats_for_plot['Duree_Saison_Pluvieuse_Jours'] = s_duree_saison
        stats_for_plot['Debut_Saison_Pluvieuse'] = s_debut_saison
        stats_for_plot['Fin_Saison_Pluvieuse'] = s_fin_saison
        
        stats_for_plot['Duree_Secheresse_Definie_Jours'] = longest_dry_spell
        stats_for_plot['Debut_Secheresse_Definie'] = debut_secheresse
        stats_for_plot['Fin_Secheresse_Definie'] = fin_secheresse

        max_date_idx = df_station[variable].idxmax() if not df_station[variable].empty else pd.NaT
        min_date_idx = df_station[variable].idxmin() if not df_station[variable].empty else pd.NaT
        stats_for_plot['Date_Maximum'] = max_date_idx if pd.notna(max_date_idx) else pd.NaT
        stats_for_plot['Date_Minimum'] = min_date_idx if pd.notna(min_date_idx) else pd.NaT
        
        metrics_to_plot = [
            'Maximum', 'Minimum', 'Cumul_Annuel', 'Mediane',
            'Moyenne_Jours_Pluvieux', 'Moyenne_Saison_Pluvieuse',
            'Duree_Saison_Pluvieuse_Jours', 'Duree_Secheresse_Definie_Jours'
        ]
        nrows, ncols = 4, 2
        figsize = (18, 16)
        
    else:
        current_var_data = df_station[variable].dropna()
        if variable == 'Solar_R_W/m^2':
            current_var_data = df_station.loc[df_station['Is_Daylight'], variable].dropna()

        if current_var_data.empty:
            fig, ax = plt.subplots(figsize=(10, 6))
            ax.text(0.5, 0.5, f"Aucune donnée valide pour la variable {var_meta['Nom']} à {station}.", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
            ax.axis('off')
            return fig

        stats_for_plot['Maximum'] = current_var_data.max()
        stats_for_plot['Minimum'] = current_var_data.min()
        stats_for_plot['Mediane'] = current_var_data.median()
        stats_for_plot['Moyenne'] = current_var_data.mean()

        max_idx = current_var_data.idxmax() if not current_var_data.empty else pd.NaT
        min_idx = current_var_data.idxmin() if not current_var_data.empty else pd.NaT

        stats_for_plot['Date_Maximum'] = max_idx if pd.notna(max_idx) else pd.NaT
        stats_for_plot['Date_Minimum'] = min_idx if pd.notna(min_idx) else pd.NaT

        metrics_to_plot = ['Maximum', 'Minimum', 'Moyenne', 'Mediane']
        nrows, ncols = 2, 2
        figsize = (18, 12)

    if not stats_for_plot:
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, f"Impossible de calculer des statistiques pour la variable '{variable}' à la station '{station}' (données manquantes ou non numériques).", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=14, color='red')
        ax.axis('off')
        return fig

    fig, axes = plt.subplots(nrows, ncols, figsize=figsize)
    plt.subplots_adjust(hspace=0.6, wspace=0.4)
    axes = axes.flatten()

    fig.suptitle(f'Statistiques de {var_meta["Nom"]} pour la station {station}', fontsize=16, y=0.98)

    for i, metric in enumerate(metrics_to_plot):
        ax = axes[i]
        value = stats_for_plot.get(metric)
        if pd.isna(value):
            ax.text(0.5, 0.5, "Données non disponibles", horizontalalignment='center', verticalalignment='center', transform=ax.transAxes, fontsize=12, color='gray')
            ax.axis('off')
            continue

        color = palette.get(metric.replace(' ', '_'), '#cccccc')
        
        plot_data_bar = pd.DataFrame({'Metric': [metric.replace('_', ' ')], 'Value': [value]})
        sns.barplot(x='Metric', y='Value', data=plot_data_bar, ax=ax, color=color, edgecolor='none')

        text = ""
        if metric in ['Duree_Saison_Pluvieuse_Jours', 'Duree_Secheresse_Definie_Jours']:
            start_date_key = f'Debut_{metric.replace("Jours", "")}'
            end_date_key = f'Fin_{metric.replace("Jours", "")}'
            start_date = stats_for_plot.get(start_date_key)
            end_date = stats_for_plot.get(end_date_key)
            date_info = ""
            if pd.notna(start_date) and pd.notna(end_date):
                date_info = f"\ndu {start_date.strftime('%d/%m/%Y')} au {end_date.strftime('%d/%m/%Y')}"
            text = f"{int(value)} j{date_info}"
        elif metric in ['Maximum', 'Minimum', 'Cumul_Annuel', 'Moyenne_Jours_Pluvieux', 'Moyenne_Saison_Pluvieuse', 'Mediane', 'Moyenne']:
            unit = var_meta['Unite']
            date_str = ''
            if (metric == 'Maximum' and 'Date_Maximum' in stats_for_plot and pd.notna(stats_for_plot['Date_Maximum'])):
                date_str = f"\n({stats_for_plot['Date_Maximum'].strftime('%d/%m/%Y')})"
            elif (metric == 'Minimum' and 'Date_Minimum' in stats_for_plot and pd.notna(stats_for_plot['Date_Minimum'])):
                date_str = f"\n({stats_for_plot['Date_Minimum'].strftime('%d/%m/%Y')})"
            
            text = f"{value:.1f} {unit}{date_str}"
        else:
            text = f"{value:.1f} {var_meta['Unite']}"

        ax.text(0.5, value, text, ha='center', va='bottom', fontsize=9, color='black',
                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', pad=1))
        
        ax.set_title(f"{var_meta['Nom']} {metric.replace('_', ' ')}", fontsize=11)
        ax.set_xlabel("")
        ax.set_ylabel(f"Valeur ({var_meta['Unite']})", fontsize=10)
        ax.tick_params(axis='x', rotation=0)
        ax.set_xticklabels([])

    for j in range(i + 1, len(axes)):
        fig.delaxes(axes[j])

    plt.tight_layout(rect=[0, 0, 1, 0.96])

    return fig

def daily_stats(df: pd.DataFrame) -> pd.DataFrame:
    """
    Calcule les statistiques journalières (moyenne, min, max, somme) pour les variables numériques
    groupées par station.
    """
    df = df.copy()

    if isinstance(df.index, pd.DatetimeIndex):
        df = df.reset_index()

    df['Datetime'] = pd.to_datetime(df['Datetime'], errors='coerce')
    df = df.dropna(subset=['Datetime', 'Station'])

    if df.empty:
        print("Avertissement: Le DataFrame est vide après le nettoyage des dates et stations dans daily_stats.")
        return pd.DataFrame()

    if 'Is_Daylight' not in df.columns:
        warnings.warn("La colonne 'Is_Daylight' est manquante. Calcul en utilisant une règle fixe (7h-18h).")
        df['Is_Daylight'] = (df['Datetime'].dt.hour >= 7) & (df['Datetime'].dt.hour <= 18)

    numerical_cols = [col for col in df.columns if pd.api.types.is_numeric_dtype(df[col]) and col not in ['Station', 'Datetime', 'Is_Daylight']]
    
    if not numerical_cols:
        warnings.warn("Aucune colonne numérique valide trouvée pour le calcul des statistiques journalières.")
        return pd.DataFrame()

    agg_funcs = {col: ['mean', 'min', 'max'] for col in numerical_cols}
    
    if 'Rain_mm' in numerical_cols:
        agg_funcs['Rain_mm'].append('sum')

    daily_stats_df = df.groupby(['Station', df['Datetime'].dt.date]).agg(agg_funcs)

    daily_stats_df.columns = ['_'.join(col).strip() for col in daily_stats_df.columns.values]

    daily_stats_df = daily_stats_df.rename_axis(index={'Datetime': 'Date'})
    daily_stats_df = daily_stats_df.reset_index()

    return daily_stats_df